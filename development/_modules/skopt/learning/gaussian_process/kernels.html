

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>skopt.learning.gaussian_process.kernels &mdash; SMAC 0.10.1.dev documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> SMAC
          

          
            
            <img src="../../../../_static/SMAC3.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.10.1.dev
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage_recomendation.html">Usage Recommendation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../manual.html">Manual</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../basic_usage.html">Basic Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../options.html">Options and file formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tae.html">Target Algorithm Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../psmac.html">Parallel SMAC (pSMAC)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../validation.html">Validation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api.html">API Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.configspace.html">smac.configspace package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.configspace.util.html">smac.configspace.util module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.epm.html">smac.epm package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.base_epm.html">smac.epm.base_epm module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.base_gp.html">smac.epm.base_gp module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.base_imputor.html">smac.epm.base_imputor module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.gaussian_process.html">smac.epm.gaussian_process module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.gaussian_process_mcmc.html">smac.epm.gaussian_process_mcmc module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.gp_base_prior.html">smac.epm.gp_base_prior module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.gp_kernels.html">smac.epm.gp_kernels module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.random_epm.html">smac.epm.random_epm module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.rf_with_instances.html">smac.epm.rf_with_instances module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.rf_with_instances_hpo.html">smac.epm.rf_with_instances_hpo module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.rfr_imputator.html">smac.epm.rfr_imputator module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.epm.uncorrelated_mo_rf_with_instances.html">smac.epm.uncorrelated_mo_rf_with_instances module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.facade.html">smac.facade package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.facade.experimental.html">smac.facade.experimental package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.facade.experimental.epils_facade.html">smac.facade.experimental.epils_facade module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.facade.experimental.hydra_facade.html">smac.facade.experimental.hydra_facade module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.facade.experimental.psmac_facade.html">smac.facade.experimental.psmac_facade module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.facade.func_facade.html">smac.facade.func_facade module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.facade.roar_facade.html">smac.facade.roar_facade module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.facade.smac_ac_facade.html">smac.facade.smac_ac_facade module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.facade.smac_bo_facade.html">smac.facade.smac_bo_facade module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.facade.smac_hpo_facade.html">smac.facade.smac_hpo_facade module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.initial_design.html">smac.initial_design package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.initial_design.default_configuration_design.html">smac.initial_design.default_configuration_design module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.initial_design.factorial_design.html">smac.initial_design.factorial_design module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.initial_design.initial_design.html">smac.initial_design.initial_design module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.initial_design.latin_hypercube_design.html">smac.initial_design.latin_hypercube_design module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.initial_design.random_configuration_design.html">smac.initial_design.random_configuration_design module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.initial_design.sobol_design.html">smac.initial_design.sobol_design module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.intensification.html">smac.intensification package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.intensification.intensification.html">smac.intensification.intensification module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.optimizer.html">smac.optimizer package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.optimizer.acquisition.html">smac.optimizer.acquisition module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.optimizer.ei_optimization.html">smac.optimizer.ei_optimization module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.optimizer.epils.html">smac.optimizer.epils module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.optimizer.objective.html">smac.optimizer.objective module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.optimizer.pSMAC.html">smac.optimizer.pSMAC module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.optimizer.random_configuration_chooser.html">smac.optimizer.random_configuration_chooser module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.optimizer.smbo.html">smac.optimizer.smbo module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.runhistory.html">smac.runhistory package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.runhistory.runhistory.html">smac.runhistory.runhistory module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.runhistory.runhistory2epm.html">smac.runhistory.runhistory2epm module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.scenario.html">smac.scenario package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.scenario.scenario.html">smac.scenario.scenario module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.stats.html">smac.stats package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.stats.stats.html">smac.stats.stats module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.tae.html">smac.tae package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.tae.execute_func.html">smac.tae.execute_func module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.tae.execute_ta_run.html">smac.tae.execute_ta_run module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.tae.execute_ta_run_aclib.html">smac.tae.execute_ta_run_aclib module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.tae.execute_ta_run_hydra.html">smac.tae.execute_ta_run_hydra module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.tae.execute_ta_run_old.html">smac.tae.execute_ta_run_old module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.utils.html">smac.utils package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.io.html">smac.utils.io package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.utils.io.cmd_reader.html">smac.utils.io.cmd_reader module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.utils.io.input_reader.html">smac.utils.io.input_reader module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.utils.io.output_directory.html">smac.utils.io.output_directory module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.utils.io.output_writer.html">smac.utils.io.output_writer module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../apidoc/smac.utils.io.traj_logging.html">smac.utils.io.traj_logging module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.constants.html">smac.utils.constants module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.dependencies.html">smac.utils.dependencies module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.duplicate_filter_logging.html">smac.utils.duplicate_filter_logging module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.logging.html">smac.utils.logging module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.merge_foreign_data.html">smac.utils.merge_foreign_data module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.test_helpers.html">smac.utils.test_helpers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.util_funcs.html">smac.utils.util_funcs module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../apidoc/smac.utils.validate.html">smac.utils.validate module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../apidoc/smac.smac_cli.html">smac.smac_cli module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq.html">F.A.Q.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../contact.html">Contact</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">SMAC</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>skopt.learning.gaussian_process.kernels</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for skopt.learning.gaussian_process.kernels</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">sqrt</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">Kernel</span> <span class="k">as</span> <span class="n">sk_Kernel</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">ConstantKernel</span> <span class="k">as</span> <span class="n">sk_ConstantKernel</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">DotProduct</span> <span class="k">as</span> <span class="n">sk_DotProduct</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">Exponentiation</span> <span class="k">as</span> <span class="n">sk_Exponentiation</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">ExpSineSquared</span> <span class="k">as</span> <span class="n">sk_ExpSineSquared</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">Hyperparameter</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">Matern</span> <span class="k">as</span> <span class="n">sk_Matern</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">NormalizedKernelMixin</span> <span class="k">as</span> <span class="n">sk_NormalizedKernelMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">Product</span> <span class="k">as</span> <span class="n">sk_Product</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">RationalQuadratic</span> <span class="k">as</span> <span class="n">sk_RationalQuadratic</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">RBF</span> <span class="k">as</span> <span class="n">sk_RBF</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">StationaryKernelMixin</span> <span class="k">as</span> <span class="n">sk_StationaryKernelMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">Sum</span> <span class="k">as</span> <span class="n">sk_Sum</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="k">import</span> <span class="n">WhiteKernel</span> <span class="k">as</span> <span class="n">sk_WhiteKernel</span>


<span class="k">class</span> <span class="nc">Kernel</span><span class="p">(</span><span class="n">sk_Kernel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for skopt.gaussian_process kernels.</span>
<span class="sd">    Supports computation of the gradient of the kernel with respect to X</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">Kernel</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">Sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">Kernel</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">Sum</span><span class="p">(</span><span class="n">ConstantKernel</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Sum</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">Kernel</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">Product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">Kernel</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">Product</span><span class="p">(</span><span class="n">ConstantKernel</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Product</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Exponentiation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes gradient of K(x, X_train) with respect to x</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x: array-like, shape=(n_features,)</span>
<span class="sd">            A single test point.</span>

<span class="sd">        Y: array-like, shape=(n_samples, n_features)</span>
<span class="sd">            Training data used to fit the gaussian process.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        gradient_x: array-like, shape=(n_samples, n_features)</span>
<span class="sd">            Gradient of K(x, X_train) with respect to x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<span class="k">class</span> <span class="nc">RBF</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_RBF</span><span class="p">):</span>
<div class="viewcode-block" id="RBF.gradient_x"><a class="viewcode-back" href="../../../../apidoc/smac.epm.gp_kernels.html#smac.epm.gp_kernels.RBF.gradient_x">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="c1"># diff = (x - X) / length_scale</span>
        <span class="c1"># size = (n_train_samples, n_dimensions)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

        <span class="n">length_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">length_scale</span><span class="p">)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">X_train</span>
        <span class="n">diff</span> <span class="o">/=</span> <span class="n">length_scale</span>

        <span class="c1"># e = -exp(0.5 * \sum_{i=1}^d (diff ** 2))</span>
        <span class="c1"># size = (n_train_samples, 1)</span>
        <span class="n">exp_diff_squared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exp_diff_squared</span> <span class="o">*=</span> <span class="o">-</span><span class="mf">0.5</span>
        <span class="n">exp_diff_squared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">exp_diff_squared</span><span class="p">,</span> <span class="n">exp_diff_squared</span><span class="p">)</span>
        <span class="n">exp_diff_squared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">exp_diff_squared</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exp_diff_squared</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># gradient = (e * diff) / length_scale</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">exp_diff_squared</span> <span class="o">*</span> <span class="n">diff</span>
        <span class="n">gradient</span> <span class="o">/=</span> <span class="n">length_scale</span>
        <span class="k">return</span> <span class="n">gradient</span></div>


<span class="k">class</span> <span class="nc">Matern</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_Matern</span><span class="p">):</span>
<div class="viewcode-block" id="Matern.gradient_x"><a class="viewcode-back" href="../../../../apidoc/smac.epm.gp_kernels.html#smac.epm.gp_kernels.Matern.gradient_x">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">length_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">length_scale</span><span class="p">)</span>

        <span class="c1"># diff = (x - X_train) / length_scale</span>
        <span class="c1"># size = (n_train_samples, n_dimensions)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">X_train</span>
        <span class="n">diff</span> <span class="o">/=</span> <span class="n">length_scale</span>

        <span class="c1"># dist_sq = \sum_{i=1}^d (diff ^ 2)</span>
        <span class="c1"># dist = sqrt(dist_sq)</span>
        <span class="c1"># size = (n_train_samples,)</span>
        <span class="n">dist_sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dist_sq</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span> <span class="o">==</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="c1"># e = -np.exp(-dist) / dist</span>
            <span class="c1"># size = (n_train_samples, 1)</span>
            <span class="n">scaled_exp_dist</span> <span class="o">=</span> <span class="o">-</span><span class="n">dist</span>
            <span class="n">scaled_exp_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scaled_exp_dist</span><span class="p">,</span> <span class="n">scaled_exp_dist</span><span class="p">)</span>
            <span class="n">scaled_exp_dist</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

            <span class="c1"># grad = (e * diff) / length_scale</span>
            <span class="c1"># For all i in [0, D) if x_i equals y_i.</span>
            <span class="c1"># 1. e -&gt; -1</span>
            <span class="c1"># 2. (x_i - y_i) / \sum_{j=1}^D (x_i - y_i)**2 approaches 1.</span>
            <span class="c1"># Hence the gradient when for all i in [0, D),</span>
            <span class="c1"># x_i equals y_i is -1 / length_scale[i].</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">!=</span> <span class="mf">0.0</span>
            <span class="n">scaled_exp_dist</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">/=</span> <span class="n">dist</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
            <span class="n">scaled_exp_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">scaled_exp_dist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">gradient</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaled_exp_dist</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
            <span class="n">gradient</span> <span class="o">/=</span> <span class="n">length_scale</span>
            <span class="k">return</span> <span class="n">gradient</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span> <span class="o">==</span> <span class="mf">1.5</span><span class="p">:</span>
            <span class="c1"># grad(fg) = f&#39;g + fg&#39;</span>
            <span class="c1"># where f = 1 + sqrt(3) * euclidean((X - Y) / length_scale)</span>
            <span class="c1"># where g = exp(-sqrt(3) * euclidean((X - Y) / length_scale))</span>
            <span class="n">sqrt_3_dist</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">dist</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">sqrt_3_dist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># When all of x_i equals y_i, f equals 1.0, (1 - f) equals</span>
            <span class="c1"># zero, hence from below</span>
            <span class="c1"># f * g_grad + g * f_grad (where g_grad = -g * f_grad)</span>
            <span class="c1"># -f * g * f_grad + g * f_grad</span>
            <span class="c1"># g * f_grad * (1 - f) equals zero.</span>
            <span class="c1"># sqrt_3_by_dist can be set to any value since diff equals</span>
            <span class="c1"># zero for this corner case.</span>
            <span class="n">sqrt_3_by_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
            <span class="n">nzd</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">!=</span> <span class="mf">0.0</span>
            <span class="n">sqrt_3_by_dist</span><span class="p">[</span><span class="n">nzd</span><span class="p">]</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">dist</span><span class="p">[</span><span class="n">nzd</span><span class="p">]</span>
            <span class="n">dist_expand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">sqrt_3_by_dist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">f_grad</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">/</span> <span class="n">length_scale</span>
            <span class="n">f_grad</span> <span class="o">*=</span> <span class="n">dist_expand</span>

            <span class="n">sqrt_3_dist</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">exp_sqrt_3_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sqrt_3_dist</span><span class="p">,</span> <span class="n">sqrt_3_dist</span><span class="p">)</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">exp_sqrt_3_dist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">g_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">g</span> <span class="o">*</span> <span class="n">f_grad</span>

            <span class="c1"># f * g_grad + g * f_grad (where g_grad = -g * f_grad)</span>
            <span class="n">f</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">f</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="n">g</span> <span class="o">*</span> <span class="n">f_grad</span> <span class="o">*</span> <span class="n">f</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span> <span class="o">==</span> <span class="mf">2.5</span><span class="p">:</span>
            <span class="c1"># grad(fg) = f&#39;g + fg&#39;</span>
            <span class="c1"># where f = (1 + sqrt(5) * euclidean((X - Y) / length_scale) +</span>
            <span class="c1">#            5 / 3 * sqeuclidean((X - Y) / length_scale))</span>
            <span class="c1"># where g = exp(-sqrt(5) * euclidean((X - Y) / length_scale))</span>
            <span class="n">sqrt_5_dist</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">*</span> <span class="n">dist</span>
            <span class="n">f2</span> <span class="o">=</span> <span class="p">(</span><span class="mf">5.0</span> <span class="o">/</span> <span class="mf">3.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">dist_sq</span>
            <span class="n">f2</span> <span class="o">+=</span> <span class="n">sqrt_5_dist</span>
            <span class="n">f2</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">f2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># For i in [0, D) if x_i equals y_i</span>
            <span class="c1"># f = 1 and g = 1</span>
            <span class="c1"># Grad = f&#39;g + fg&#39; = f&#39; + g&#39;</span>
            <span class="c1"># f&#39; = f_1&#39; + f_2&#39;</span>
            <span class="c1"># Also g&#39; = -g * f1&#39;</span>
            <span class="c1"># Grad = f&#39;g - g * f1&#39; * f</span>
            <span class="c1"># Grad = g * (f&#39; - f1&#39; * f)</span>
            <span class="c1"># Grad = f&#39; - f1&#39;</span>
            <span class="c1"># Grad = f2&#39; which equals zero when x = y</span>
            <span class="c1"># Since for this corner case, diff equals zero,</span>
            <span class="c1"># dist can be set to anything.</span>
            <span class="n">nzd_mask</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">!=</span> <span class="mf">0.0</span>
            <span class="n">nzd</span> <span class="o">=</span> <span class="n">dist</span><span class="p">[</span><span class="n">nzd_mask</span><span class="p">]</span>
            <span class="n">dist</span><span class="p">[</span><span class="n">nzd_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">nzd</span><span class="p">,</span> <span class="n">nzd</span><span class="p">)</span>

            <span class="n">dist</span> <span class="o">*=</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">diff</span> <span class="o">/=</span> <span class="n">length_scale</span>
            <span class="n">f1_grad</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">*</span> <span class="n">diff</span>
            <span class="n">f2_grad</span> <span class="o">=</span> <span class="p">(</span><span class="mf">10.0</span> <span class="o">/</span> <span class="mf">3.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>
            <span class="n">f_grad</span> <span class="o">=</span> <span class="n">f1_grad</span> <span class="o">+</span> <span class="n">f2_grad</span>

            <span class="n">sqrt_5_dist</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sqrt_5_dist</span><span class="p">,</span> <span class="n">sqrt_5_dist</span><span class="p">)</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">g_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">g</span> <span class="o">*</span> <span class="n">f1_grad</span>
            <span class="k">return</span> <span class="n">f</span> <span class="o">*</span> <span class="n">g_grad</span> <span class="o">+</span> <span class="n">g</span> <span class="o">*</span> <span class="n">f_grad</span></div>


<span class="k">class</span> <span class="nc">RationalQuadratic</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_RationalQuadratic</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
        <span class="n">length_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_scale</span>

        <span class="c1"># diff = (x - X_train) / length_scale</span>
        <span class="c1"># size = (n_train_samples, n_dimensions)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">X_train</span>
        <span class="n">diff</span> <span class="o">/=</span> <span class="n">length_scale</span>

        <span class="c1"># dist = -(1 + (\sum_{i=1}^d (diff^2) / (2 * alpha)))** (-alpha - 1)</span>
        <span class="c1"># size = (n_train_samples,)</span>
        <span class="n">scaled_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scaled_dist</span> <span class="o">/=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">scaled_dist</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">scaled_dist</span> <span class="o">**=</span> <span class="p">(</span><span class="o">-</span><span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">scaled_dist</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="n">scaled_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">scaled_dist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">diff_by_ls</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">/</span> <span class="n">length_scale</span>
        <span class="k">return</span> <span class="n">scaled_dist</span> <span class="o">*</span> <span class="n">diff_by_ls</span>


<span class="k">class</span> <span class="nc">ExpSineSquared</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_ExpSineSquared</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">length_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_scale</span>
        <span class="n">periodicity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">periodicity</span>

        <span class="n">diff</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">X_train</span>
        <span class="n">sq_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sq_dist</span><span class="p">)</span>

        <span class="n">pi_by_period</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">periodicity</span><span class="p">)</span>
        <span class="n">sine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">pi_by_period</span><span class="p">)</span> <span class="o">/</span> <span class="n">length_scale</span>
        <span class="n">sine_squared</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sine</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">exp_sine_squared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sine_squared</span><span class="p">)</span>

        <span class="n">grad_wrt_exp</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">pi_by_period</span><span class="p">)</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span>

        <span class="c1"># When x_i -&gt; y_i for all i in [0, D), the gradient becomes</span>
        <span class="c1"># zero. See https://github.com/MechCoder/Notebooks/blob/master/ExpSineSquared%20Kernel%20gradient%20computation.ipynb</span>
        <span class="c1"># for a detailed math explanation</span>
        <span class="c1"># grad_wrt_theta can be anything since diff is zero</span>
        <span class="c1"># for this corner case, hence we set to zero.</span>
        <span class="n">grad_wrt_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
        <span class="n">nzd</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">!=</span> <span class="mf">0.0</span>
        <span class="n">grad_wrt_theta</span><span class="p">[</span><span class="n">nzd</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="p">(</span><span class="n">periodicity</span> <span class="o">*</span> <span class="n">dist</span><span class="p">[</span><span class="n">nzd</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">grad_wrt_theta</span> <span class="o">*</span> <span class="n">exp_sine_squared</span> <span class="o">*</span> <span class="n">grad_wrt_exp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>


<span class="k">class</span> <span class="nc">ConstantKernel</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_ConstantKernel</span><span class="p">):</span>

<div class="viewcode-block" id="ConstantKernel.gradient_x"><a class="viewcode-back" href="../../../../apidoc/smac.epm.gp_kernels.html#smac.epm.gp_kernels.ConstantKernel.gradient_x">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">WhiteKernel</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_WhiteKernel</span><span class="p">):</span>

<div class="viewcode-block" id="WhiteKernel.gradient_x"><a class="viewcode-back" href="../../../../apidoc/smac.epm.gp_kernels.html#smac.epm.gp_kernels.WhiteKernel.gradient_x">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Exponentiation</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_Exponentiation</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">expo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exponent</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span>

        <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">kernel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">X_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">expo</span> <span class="o">*</span> <span class="n">K</span> <span class="o">**</span> <span class="p">(</span><span class="n">expo</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">kernel</span><span class="o">.</span><span class="n">gradient_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Sum</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_Sum</span><span class="p">):</span>

<div class="viewcode-block" id="Sum.gradient_x"><a class="viewcode-back" href="../../../../apidoc/smac.epm.gp_kernels.html#smac.epm.gp_kernels.Sum.gradient_x">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k1</span><span class="o">.</span><span class="n">gradient_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span> <span class="o">+</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k2</span><span class="o">.</span><span class="n">gradient_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
        <span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_Product</span><span class="p">):</span>

<div class="viewcode-block" id="Product.gradient_x"><a class="viewcode-back" href="../../../../apidoc/smac.epm.gp_kernels.html#smac.epm.gp_kernels.Product.gradient_x">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">f_ggrad</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k2</span><span class="o">.</span><span class="n">gradient_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">fgrad_g</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k1</span><span class="o">.</span><span class="n">gradient_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">f_ggrad</span> <span class="o">+</span> <span class="n">fgrad_g</span></div>


<span class="k">class</span> <span class="nc">DotProduct</span><span class="p">(</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">sk_DotProduct</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">gradient_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HammingKernel</span><span class="p">(</span><span class="n">sk_StationaryKernelMixin</span><span class="p">,</span> <span class="n">sk_NormalizedKernelMixin</span><span class="p">,</span>
                    <span class="n">Kernel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The HammingKernel is used to handle categorical inputs.</span>

<span class="sd">    ``K(x_1, x_2) = exp(\sum_{j=1}^{d} -ls_j * (I(x_1j != x_2j)))``</span>

<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    * `length_scale` [float, array-like, shape=[n_features,], 1.0 (default)]</span>
<span class="sd">        The length scale of the kernel. If a float, an isotropic kernel is</span>
<span class="sd">        used. If an array, an anisotropic kernel is used where each dimension</span>
<span class="sd">        of l defines the length-scale of the respective feature dimension.</span>

<span class="sd">    * `length_scale_bounds` [array-like, [1e-5, 1e5] (default)]</span>
<span class="sd">        The lower and upper bound on length_scale</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">length_scale_bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_scale</span> <span class="o">=</span> <span class="n">length_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_scale_bounds</span> <span class="o">=</span> <span class="n">length_scale_bounds</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">hyperparameter_length_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">length_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_scale</span>
        <span class="n">anisotropic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iterable</span><span class="p">(</span><span class="n">length_scale</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">length_scale</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">anisotropic</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Hyperparameter</span><span class="p">(</span><span class="s2">&quot;length_scale&quot;</span><span class="p">,</span> <span class="s2">&quot;numeric&quot;</span><span class="p">,</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">length_scale_bounds</span><span class="p">,</span>
                                  <span class="nb">len</span><span class="p">(</span><span class="n">length_scale</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Hyperparameter</span><span class="p">(</span>
            <span class="s2">&quot;length_scale&quot;</span><span class="p">,</span> <span class="s2">&quot;numeric&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_scale_bounds</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eval_gradient</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples_X, n_features)]</span>
<span class="sd">            Left argument of the returned kernel k(X, Y)</span>

<span class="sd">        * `Y` [array-like, shape=(n_samples_Y, n_features) or None(default)]</span>
<span class="sd">            Right argument of the returned kernel k(X, Y). If None, k(X, X)</span>
<span class="sd">            if evaluated instead.</span>

<span class="sd">        * `eval_gradient` [bool, False(default)]</span>
<span class="sd">            Determines whether the gradient with respect to the kernel</span>
<span class="sd">            hyperparameter is determined. Only supported when Y is None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `K` [array-like, shape=(n_samples_X, n_samples_Y)]</span>
<span class="sd">            Kernel k(X, Y)</span>

<span class="sd">        * `K_gradient` [array-like, shape=(n_samples_X, n_samples_X, n_dims)]</span>
<span class="sd">            The gradient of the kernel k(X, X) with respect to the</span>
<span class="sd">            hyperparameter of the kernel. Only returned when eval_gradient</span>
<span class="sd">            is True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">length_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_scale</span>
        <span class="n">anisotropic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iterable</span><span class="p">(</span><span class="n">length_scale</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">length_scale</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">iterable</span><span class="p">(</span><span class="n">length_scale</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">length_scale</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">length_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">length_scale</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">length_scale</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">length_scale</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">length_scale</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">length_scale</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">anisotropic</span> <span class="ow">and</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">length_scale</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected X to have </span><span class="si">%d</span><span class="s2"> features, got </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">length_scale</span><span class="p">)))</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_dim</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">Y_is_None</span> <span class="o">=</span> <span class="n">Y</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">Y_is_None</span><span class="p">:</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">elif</span> <span class="n">eval_gradient</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;gradient can be evaluated only when Y != X&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

        <span class="n">indicator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">Y</span>
        <span class="n">kernel_prod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">length_scale</span> <span class="o">*</span> <span class="n">indicator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

        <span class="c1"># dK / d theta = (dK / dl) * (dl / d theta)</span>
        <span class="c1"># theta = log(l) =&gt; dl / d (theta) = e^theta = l</span>
        <span class="c1"># dK / d theta = l * dK / dl</span>

        <span class="c1"># dK / dL computation</span>
        <span class="k">if</span> <span class="n">anisotropic</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">kernel_prod</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">kernel_prod</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                                   <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">grad</span> <span class="o">*=</span> <span class="n">length_scale</span>
        <span class="k">if</span> <span class="n">eval_gradient</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kernel_prod</span><span class="p">,</span> <span class="n">grad</span>
        <span class="k">return</span> <span class="n">kernel_prod</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015-2019, Marius Lindauer, Matthias Feurer, Katharina Eggensperger, Joshua Marben, Andr Biedenkapp, Aaron Klein, Stefan Falkner and Frank Hutter

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>