
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/3_multi_objective/2_parego.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_3_multi_objective_2_parego.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_3_multi_objective_2_parego.py:


ParEGO
^^^^^^

An example of how to use multi-objective optimization with ParEGO. Both accuracy and run-time are going to be
optimized on the digits dataset using an MLP, and the configurations are shown in a plot, highlighting the best ones in 
a Pareto front. The red cross indicates the best configuration selected by SMAC.

In the optimization, SMAC evaluates the configurations on two different seeds. Therefore, the plot shows the
mean accuracy and run-time of each configuration.

.. GENERATED FROM PYTHON SOURCE LINES 12-178



.. image-sg:: /examples/3_multi_objective/images/sphx_glr_2_parego_001.png
   :alt: Pareto-Front
   :srcset: /examples/3_multi_objective/images/sphx_glr_2_parego_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [WARNING][target_function_runner.py:71] The argument budget is not set by SMAC: Consider removing it from the target function.
    [INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.
    [INFO][abstract_intensifier.py:515] Added config 97ef22 as new incumbent because there are no incumbents yet.
    [INFO][abstract_intensifier.py:598] Config f88a6b is a new incumbent. Total number of incumbents: 2.
    [INFO][abstract_intensifier.py:598] Config d77b40 is a new incumbent. Total number of incumbents: 3.
    [INFO][abstract_intensifier.py:590] Added config f08f42 and rejected config d77b40 as incumbent because it is not better than the incumbents on 2 instances:
    [INFO][configspace.py:175] --- activation: 'logistic' -> 'tanh'
    [INFO][configspace.py:175] --- batch_size: 175 -> 118
    [INFO][configspace.py:175] --- learning_rate_init: 0.004156370184967407 -> 0.0025694050967575367
    [INFO][configspace.py:175] --- n_layer: 2 -> 1
    [INFO][configspace.py:175] --- n_neurons: 90 -> 201
    [INFO][abstract_intensifier.py:590] Added config 1edc1a and rejected config f88a6b as incumbent because it is not better than the incumbents on 2 instances:
    [INFO][configspace.py:175] --- batch_size: 77 -> 187
    [INFO][configspace.py:175] --- learning_rate_init: 0.000534923804864797 -> 0.0009593239301609004
    [INFO][configspace.py:175] --- n_neurons: 146 -> 183
    [INFO][smbo.py:307] Configuration budget is exhausted:
    [INFO][smbo.py:308] --- Remaining wallclock time: -0.8711967468261719
    [INFO][smbo.py:309] --- Remaining cpu time: inf
    [INFO][smbo.py:310] --- Remaining trials: 177
    Validated costs from default config: 
    --- [0.60155834 0.36748278]

    Validated costs from the Pareto front (incumbents):
    --- [0.88924791 0.43870711]
    --- [0.0281074  1.31155753]
    --- [0.04341071 1.02695835]






|

.. code-block:: default

    from __future__ import annotations

    import time
    import warnings

    import matplotlib.pyplot as plt
    import numpy as np
    from ConfigSpace import (
        Categorical,
        Configuration,
        ConfigurationSpace,
        EqualsCondition,
        Float,
        InCondition,
        Integer,
    )
    from sklearn.datasets import load_digits
    from sklearn.model_selection import StratifiedKFold, cross_val_score
    from sklearn.neural_network import MLPClassifier

    from smac import HyperparameterOptimizationFacade as HPOFacade
    from smac import Scenario
    from smac.facade.abstract_facade import AbstractFacade
    from smac.multi_objective.parego import ParEGO

    __copyright__ = "Copyright 2021, AutoML.org Freiburg-Hannover"
    __license__ = "3-clause BSD"


    digits = load_digits()


    class MLP:
        @property
        def configspace(self) -> ConfigurationSpace:
            cs = ConfigurationSpace()

            n_layer = Integer("n_layer", (1, 5), default=1)
            n_neurons = Integer("n_neurons", (8, 256), log=True, default=10)
            activation = Categorical("activation", ["logistic", "tanh", "relu"], default="tanh")
            solver = Categorical("solver", ["lbfgs", "sgd", "adam"], default="adam")
            batch_size = Integer("batch_size", (30, 300), default=200)
            learning_rate = Categorical("learning_rate", ["constant", "invscaling", "adaptive"], default="constant")
            learning_rate_init = Float("learning_rate_init", (0.0001, 1.0), default=0.001, log=True)

            cs.add_hyperparameters([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])

            use_lr = EqualsCondition(child=learning_rate, parent=solver, value="sgd")
            use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=["sgd", "adam"])
            use_batch_size = InCondition(child=batch_size, parent=solver, values=["sgd", "adam"])

            # We can also add multiple conditions on hyperparameters at once:
            cs.add_conditions([use_lr, use_batch_size, use_lr_init])

            return cs

        def train(self, config: Configuration, seed: int = 0, budget: int = 10) -> dict[str, float]:
            lr = config["learning_rate"] if config["learning_rate"] else "constant"
            lr_init = config["learning_rate_init"] if config["learning_rate_init"] else 0.001
            batch_size = config["batch_size"] if config["batch_size"] else 200

            start_time = time.time()

            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")

                classifier = MLPClassifier(
                    hidden_layer_sizes=[config["n_neurons"]] * config["n_layer"],
                    solver=config["solver"],
                    batch_size=batch_size,
                    activation=config["activation"],
                    learning_rate=lr,
                    learning_rate_init=lr_init,
                    max_iter=int(np.ceil(budget)),
                    random_state=seed,
                )

                # Returns the 5-fold cross validation accuracy
                cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent
                score = cross_val_score(classifier, digits.data, digits.target, cv=cv, error_score="raise")

            return {
                "1 - accuracy": 1 - np.mean(score),
                "time": time.time() - start_time,
            }


    def plot_pareto(smac: AbstractFacade, incumbents: list[Configuration]) -> None:
        """Plots configurations from SMAC and highlights the best configurations in a Pareto front."""
        average_costs = []
        average_pareto_costs = []
        for config in smac.runhistory.get_configs():
            # Since we use multiple seeds, we have to average them to get only one cost value pair for each configuration
            average_cost = smac.runhistory.average_cost(config)

            if config in incumbents:
                average_pareto_costs += [average_cost]
            else:
                average_costs += [average_cost]

        # Let's work with a numpy array
        costs = np.vstack(average_costs)
        pareto_costs = np.vstack(average_pareto_costs)
        pareto_costs = pareto_costs[pareto_costs[:, 0].argsort()]  # Sort them

        costs_x, costs_y = costs[:, 0], costs[:, 1]
        pareto_costs_x, pareto_costs_y = pareto_costs[:, 0], pareto_costs[:, 1]

        plt.scatter(costs_x, costs_y, marker="x", label="Configuration")
        plt.scatter(pareto_costs_x, pareto_costs_y, marker="x", c="r", label="Incumbent")
        plt.step(
            [pareto_costs_x[0]] + pareto_costs_x.tolist() + [np.max(costs_x)],  # We add bounds
            [np.max(costs_y)] + pareto_costs_y.tolist() + [np.min(pareto_costs_y)],  # We add bounds
            where="post",
            linestyle=":",
        )

        plt.title("Pareto-Front")
        plt.xlabel(smac.scenario.objectives[0])
        plt.ylabel(smac.scenario.objectives[1])
        plt.legend()
        plt.show()


    if __name__ == "__main__":
        mlp = MLP()
        objectives = ["1 - accuracy", "time"]

        # Define our environment variables
        scenario = Scenario(
            mlp.configspace,
            objectives=objectives,
            walltime_limit=30,  # After 30 seconds, we stop the hyperparameter optimization
            n_trials=200,  # Evaluate max 200 different trials
            n_workers=1,
        )

        # We want to run five random configurations before starting the optimization.
        initial_design = HPOFacade.get_initial_design(scenario, n_configs=5)
        multi_objective_algorithm = ParEGO(scenario)
        intensifier = HPOFacade.get_intensifier(scenario, max_config_calls=2)

        # Create our SMAC object and pass the scenario and the train method
        smac = HPOFacade(
            scenario,
            mlp.train,
            initial_design=initial_design,
            multi_objective_algorithm=multi_objective_algorithm,
            intensifier=intensifier,
            overwrite=True,
        )

        # Let's optimize
        incumbents = smac.optimize()

        # Get cost of default configuration
        default_cost = smac.validate(mlp.configspace.get_default_configuration())
        print(f"Validated costs from default config: \n--- {default_cost}\n")

        print("Validated costs from the Pareto front (incumbents):")
        for incumbent in incumbents:
            cost = smac.validate(incumbent)
            print("---", cost)

        # Let's plot a pareto front
        plot_pareto(smac, incumbents)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  37.455 seconds)


.. _sphx_glr_download_examples_3_multi_objective_2_parego.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 2_parego.py <2_parego.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 2_parego.ipynb <2_parego.ipynb>`
