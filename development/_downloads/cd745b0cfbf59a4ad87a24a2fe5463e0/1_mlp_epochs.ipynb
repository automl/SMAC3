{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-Layer Perceptron Using Multiple Epochs\n\nExample for optimizing a Multi-Layer Perceptron (MLP) using multiple budgets.\nSince we want to take advantage of multi-fidelity, the ``MultiFidelityFacade`` is a good choice. By default,\n``MultiFidelityFacade`` internally runs with [hyperband](https://arxiv.org/abs/1603.06560) as\nintensification, which is a combination of an aggressive racing mechanism and Successive Halving. Crucially, the target \nfunction must accept a budget variable, detailing how much fidelity smac wants to allocate to this\nconfiguration. In this example, we use both ``SuccessiveHalving`` and ``Hyperband`` to compare the results.\n\nMLP is a deep neural network, and therefore, we choose epochs as fidelity type. This implies,\nthat ``budget`` specifies the number of epochs smac wants to allocate. The digits dataset\nis chosen to optimize the average accuracy on 5-fold cross validation.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example uses the ``MultiFidelityFacade`` facade, which is the closest implementation to\n    [BOHB](https://github.com/automl/HpBandSter).</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ConfigSpace import (\n    Categorical,\n    Configuration,\n    ConfigurationSpace,\n    EqualsCondition,\n    Float,\n    InCondition,\n    Integer,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import MultiFidelityFacade as MFFacade\nfrom smac import Scenario\nfrom smac.facade import AbstractFacade\nfrom smac.intensifier.hyperband import Hyperband\nfrom smac.intensifier.successive_halving import SuccessiveHalving\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\ndataset = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self) -> ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types, we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        n_layer = Integer(\"n_layer\", (1, 5), default=1)\n        n_neurons = Integer(\"n_neurons\", (8, 256), log=True, default=10)\n        activation = Categorical(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default=\"tanh\")\n        solver = Categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\")\n        batch_size = Integer(\"batch_size\", (30, 300), default=200)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        learning_rate_init = Float(\"learning_rate_init\", (0.0001, 1.0), default=0.001, log=True)\n\n        # Add all hyperparameters at once:\n        cs.add([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])\n\n        # Adding conditions to restrict the hyperparameter space...\n        # ... since learning rate is only used when solver is 'sgd'.\n        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n        # ... since learning rate initialization will only be accounted for when using 'sgd' or 'adam'.\n        use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n        # ... since batch size will not be considered when optimizer is 'lbfgs'.\n        use_batch_size = InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n        # We can also add multiple conditions on hyperparameters at once:\n        cs.add([use_lr, use_batch_size, use_lr_init])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: int = 25) -> float:\n        # For deactivated parameters (by virtue of the conditions),\n        # the configuration stores None-values.\n        # This is not accepted by the MLP, so we replace them with placeholder values.\n        lr = config.get(\"learning_rate\", \"constant\")\n        lr_init = config.get(\"learning_rate_init\", 0.001)\n        batch_size = config.get(\"batch_size\", 200)\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"solver\"],\n                batch_size=batch_size,\n                activation=config[\"activation\"],\n                learning_rate=lr,\n                learning_rate_init=lr_init,\n                max_iter=int(np.ceil(budget)),\n                random_state=seed,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, dataset.data, dataset.target, cv=cv, error_score=\"raise\")\n\n        return 1 - np.mean(score)\n\n\ndef plot_trajectory(facades: list[AbstractFacade]) -> None:\n    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n    plt.figure()\n    plt.title(\"Trajectory\")\n    plt.xlabel(\"Wallclock time [s]\")\n    plt.ylabel(facades[0].scenario.objectives)\n    plt.ylim(0, 0.4)\n\n    for facade in facades:\n        X, Y = [], []\n        for item in facade.intensifier.trajectory:\n            # Single-objective optimization\n            assert len(item.config_ids) == 1\n            assert len(item.costs) == 1\n\n            y = item.costs[0]\n            x = item.walltime\n\n            X.append(x)\n            Y.append(y)\n\n        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n        plt.scatter(X, Y, marker=\"x\")\n\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n\n    facades: list[AbstractFacade] = []\n    for intensifier_object in [SuccessiveHalving, Hyperband]:\n        # Define our environment variables\n        scenario = Scenario(\n            mlp.configspace,\n            walltime_limit=60,  # After 60 seconds, we stop the hyperparameter optimization\n            n_trials=500,  # Evaluate max 500 different trials\n            min_budget=1,  # Train the MLP using a hyperparameter configuration for at least 5 epochs\n            max_budget=25,  # Train the MLP using a hyperparameter configuration for at most 25 epochs\n            n_workers=8,\n        )\n\n        # We want to run five random configurations before starting the optimization.\n        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n\n        # Create our intensifier\n        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n\n        # Create our SMAC object and pass the scenario and the train method\n        smac = MFFacade(\n            scenario,\n            mlp.train,\n            initial_design=initial_design,\n            intensifier=intensifier,\n            overwrite=True,\n        )\n\n        # Let's optimize\n        incumbent = smac.optimize()\n\n        # Get cost of default configuration\n        default_cost = smac.validate(mlp.configspace.get_default_configuration())\n        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n\n        # Let's calculate the cost of the incumbent\n        incumbent_cost = smac.validate(incumbent)\n        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n\n        facades.append(smac)\n\n    # Let's plot it\n    plot_trajectory(facades)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}