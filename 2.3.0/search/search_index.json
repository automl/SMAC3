{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Home</p>"},{"location":"#smac3-documentation","title":"SMAC3 Documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>SMAC is a tool for algorithm configuration to optimize the parameters of arbitrary algorithms, including hyperparameter optimization of Machine Learning algorithms. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better.</p> <p>SMAC3 is written in Python3 and continuously tested with Python 3.8, 3.9, and 3.10. Its Random Forest is written in C++. In the following, SMAC is representatively mentioned for SMAC3.</p>"},{"location":"#cite-us","title":"Cite Us","text":"<p>If you use SMAC, please cite our JMLR paper:</p> <pre><code>@article{lindauer-jmlr22a,\n       author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and Andr\u00e9 Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and Ren\u00e9 Sass and Frank Hutter},\n       title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},\n       journal = {Journal of Machine Learning Research},\n       year    = {2022},\n       volume  = {23},\n       number  = {54},\n       pages   = {1--9},\n       url     = {http://jmlr.org/papers/v23/21-0888.html}\n}\n</code></pre> <p>For the original idea, we refer to:</p> <pre><code>Hutter, F. and Hoos, H. H. and Leyton-Brown, K.\nSequential Model-Based Optimization for General Algorithm Configuration\nIn: Proceedings of the conference on Learning and Intelligent Optimization (LION 5)\n</code></pre>"},{"location":"#contact","title":"Contact","text":"<p>SMAC3 is developed by AutoML.org. If you want to contribute or found an issue, please visit our GitHub page. Our guidelines for contributing to this package can be found here.</p>"},{"location":"10_experimental/","title":"Experimental","text":"<p>Warning</p> <p>This part is experimental and might not work in each case. If you would like to suggest any changes, please let us know. </p>"},{"location":"10_experimental/#installation-in-windows-via-wsl","title":"Installation in Windows via WSL","text":"<p>SMAC can be installed in a WSL (Windows Subsystem for Linux) under Windows.</p> <p>1) Install WSL under Windows</p> <p>Install WSL under Windows. This SMAC installation workflow was tested with Ubuntu 18.04. For Ubuntu 20.04,  it has been observed that the SMAC installation results in a segmentation fault (core dumped).</p> <p>2) Get Anaconda</p> <p>Download an Anaconda Linux version to drive D under Windows, e.g. D:\\Anaconda3-2023.03-1-Linux-x86_64</p> <p>In the WSL, Windows resources are mounted under /mnt:</p> <pre><code>cd /mnt/d\nbash Anaconda3-2023.03-1-Linux-x86_64\n</code></pre> <p>Enter this command to create the environment variable:</p> <pre><code>export PATH=\"$PATH:/home/${USER}/anaconda3/bin\n</code></pre> <p>Input <code>python</code> to check if the installation was successful.</p> <p>3) Install SMAC</p> <p>Change to your home folder and install the general software there:</p> <pre><code>cd /home/${USER}\nsudo apt-get install software-properties-common\nsudo apt-get update\nsudo apt-get install build-essential swig\nconda install gxx_linux-64 gcc_linux-64 swig\ncurl https://raw.githubusercontent.com/automl/smac3/master/requirements.txt | xargs -n 1 -L 1 pip install\n</code></pre>"},{"location":"10_experimental/#installation-in-pure-windows","title":"Installation in Pure Windows","text":"<p>Please refer to this issue for installation instructions for SMAC3-1.4 and SMAC3-2.x.</p>"},{"location":"1_installation/","title":"Installation","text":""},{"location":"1_installation/#requirements","title":"Requirements","text":"<p>SMAC is written in python3 and therefore requires an environment with python&gt;=3.8. Furthermore, the Random Forest used in SMAC requires SWIG as a build dependency.</p> <p>Info</p> <p>SMAC is tested on Linux and Mac machines with python &gt;=3.8.</p>"},{"location":"1_installation/#setup","title":"SetUp","text":"<p>We recommend using Anaconda to create and activate an environment:</p> <pre><code>conda create -n SMAC python=3.10\nconda activate SMAC\n</code></pre> <p>Now install swig either on the system level e.g. using the following command for Linux: <pre><code>apt-get install swig\n</code></pre></p> <p>Or install swig inside of an already created conda environment using:</p> <pre><code>conda install gxx_linux-64 gcc_linux-64 swig\n</code></pre>"},{"location":"1_installation/#install-smac","title":"Install SMAC","text":"<p>You can install SMAC either using PyPI or Conda-forge.</p>"},{"location":"1_installation/#pypi","title":"PYPI","text":"<p>To install SMAC with PyPI call:</p> <pre><code>pip install smac\n</code></pre> <p>Or alternatively, clone the environment from GitHub directly:</p> <pre><code>git clone https://github.com/automl/SMAC3.git &amp;&amp; cd SMAC3\npip install -e \".[dev]\"\n</code></pre>"},{"location":"1_installation/#conda-forge","title":"Conda-forge","text":"<p>Installing SMAC from the <code>conda-forge</code> channel can be achieved by adding <code>conda-forge</code> to your channels with:</p> <pre><code>conda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre> <p>You must have <code>conda &gt;= 4.9</code> installed. To update conda or check your current conda version, please follow the instructions from the official anaconda documentation. Once the <code>conda-forge</code> channel has been enabled, SMAC can be installed with:</p> <pre><code>conda install smac\n</code></pre> <p>Read SMAC feedstock for more details.</p>"},{"location":"1_installation/#windows-native-or-via-wsl-experimental","title":"Windows (native or via WSL, experimental)","text":"<p>SMAC can be installed under Windows in a WSL (Windows Subsystem for Linux).  You can find an instruction on how to do this here: Experimental. However, this is experimental and might not work in each case.  If you would like to suggest any changes, please let us know. </p>"},{"location":"2_package_overview/","title":"Package Overview","text":"<p>SMAC supports you in determining well-performing hyperparameter configurations for your algorithms. By being a robust and flexible framework for Bayesian Optimization, SMAC can improve performance within a few function evaluations. It offers several entry points and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances.</p>"},{"location":"2_package_overview/#features","title":"Features","text":"<p>SMAC has the following characteristics and capabilities:</p>"},{"location":"2_package_overview/#global-optimizer","title":"Global Optimizer","text":"<p>Bayesian Optimization is used for sample-efficient optimization.</p>"},{"location":"2_package_overview/#optimize-black-box-functions","title":"Optimize Black-Box Functions","text":"<p>Optimization is only aware of input and output. It is agnostic to internals of the function.</p>"},{"location":"2_package_overview/#flexible-hyperparameters","title":"Flexible Hyperparameters","text":"<p>Use categorical, continuous, hierarchical and/or conditional hyperparameters with the well-integrated ConfigurationSpace. SMAC can optimize up to 100 hyperparameters efficiently.</p>"},{"location":"2_package_overview/#any-objectives","title":"Any Objectives","text":"<p>Optimization with any objective (e.g., accuracy, runtime, cross-validation, ...) is possible.</p>"},{"location":"2_package_overview/#multi-objective-optimization","title":"Multi-Objective Optimization","text":"<p>Optimize arbitrary number of objectives using scalarized multi-objective algorithms. Both ParEGO [Know06] and mean aggregation strategies are supported.</p>"},{"location":"2_package_overview/#multi-fidelity-optimization","title":"Multi-Fidelity Optimization","text":"<p>Judge configurations on multiple budgets to discard unsuitable configurations early on. This will result in a massive speed-up, depending on the budgets.</p>"},{"location":"2_package_overview/#instances","title":"Instances","text":"<p>Find well-performing hyperparameter configurations not only for one instance (e.g. dataset) of an algorithm, but for many.</p>"},{"location":"2_package_overview/#command-line-interface","title":"Command-Line Interface","text":"<p>SMAC can not only be executed within a python file but also from the command line. Consequently, not only algorithms in python can be optimized, but implementations in other languages as well.</p> <p>Note</p> <p>Command-line interface has been temporarily disabled in v2.0. Please fall back to v1.4 if you need it.</p>"},{"location":"2_package_overview/#comparison","title":"Comparison","text":"<p>The following table provides an overview of SMAC's capabilities in comparison with other optimization tools.</p> Package Complex Hyperparameter Space Multi-Objective Multi-Fidelity Instances Command-Line Interface Parallelism HyperMapper \u2705 \u2705 \u274c \u274c \u274c \u274c Optuna \u2705 \u2705 \u2705 \u274c \u2705 \u2705 Hyperopt \u2705 \u274c \u274c \u274c \u2705 \u2705 BoTorch \u274c \u2705 \u2705 \u274c \u274c \u2705 OpenBox \u2705 \u2705 \u274c \u274c \u274c \u2705 HpBandSter \u2705 \u274c \u2705 \u274c \u274c \u2705 SMAC \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"3_getting_started/","title":"Getting Started","text":""},{"location":"3_getting_started/#getting-started","title":"Getting Started","text":"<p>SMAC needs four core components (configuration space, target function, scenario and a facade) to run an optimization process, all of which are explained on this page.</p> <p>They interact in the following way:</p> Interaction of SMAC's components"},{"location":"3_getting_started/#configuration-space","title":"Configuration Space","text":"<p>The configuration space defines the search space of the hyperparameters and, therefore, the tunable parameters' legal ranges and default values.</p> <pre><code>from ConfigSpace import ConfigSpace\n\ncs = ConfigurationSpace({\n    \"myfloat\": (0.1, 1.5),                # Uniform Float\n    \"myint\": (2, 10),                     # Uniform Integer\n    \"species\": [\"mouse\", \"cat\", \"dog\"],   # Categorical\n})\n</code></pre> <p>Please see the documentation of ConfigurationSpace for more details.</p>"},{"location":"3_getting_started/#target-function","title":"Target Function","text":"<p>The target function takes a configuration from the configuration space and returns a performance value. For example, you could use a Neural Network to predict on your data and get some validation performance. If, for instance, you would tune the learning rate of the Network's optimizer, every learning rate will change the final validation performance of the network. This is the target function. SMAC tries to find the best performing learning rate by trying different values and evaluating the target function - in an efficient way.</p> <pre><code>    def train(self, config: Configuration, seed: int) -&gt; float:\n        model = MultiLayerPerceptron(learning_rate=config[\"learning_rate\"])\n        model.fit(...)\n        accuracy = model.validate(...)\n\n        return 1 - accuracy  # SMAC always minimizes (the smaller the better)\n</code></pre> <p>Note</p> <p>In general, the arguments of the target function depend on the intensifier. However, in all cases, the first argument must be the configuration (arbitrary argument name is possible here) and a seed. If you specified instances in the scenario, SMAC requires <code>instance</code> as argument additionally. If you use <code>SuccessiveHalving</code> or <code>Hyperband</code> as intensifier but you did not specify instances, SMAC passes <code>budget</code> as argument to the target function. But don't worry: SMAC will tell you if something is missing or if something is not used.</p> <p>Warning</p> <p>SMAC always minimizes the value returned from the target function.</p> <p>Warning</p> <p>SMAC passes either <code>instance</code> or <code>budget</code> to the target function but never both.</p>"},{"location":"3_getting_started/#scenario","title":"Scenario","text":"<p>The Scenario is used to provide environment variables. For example,  if you want to limit the optimization process by a time limit or want to specify where to save the results.</p> <pre><code>from smac import Scenario\n\nscenario = Scenario(\n    configspace=cs,\n    name=\"experiment_name\",\n    output_directory=Path(\"your_output_directory\")\n    walltime_limit=120,  # Limit to two minutes\n    n_trials=500,  # Evaluated max 500 trials\n    n_workers=8,  # Use eight workers\n    ...\n)\n</code></pre> <p>Note</p> <p>If no <code>name</code> is given, a hash of the experiment is used. Running the same experiment again at a later time will result in exactly the same hash. This is important, because the optimization will warmstart on the preexisting evaluations, if not otherwise specified in the Facade.</p>"},{"location":"3_getting_started/#facade","title":"Facade","text":"<p>Warn</p> <p>By default Facades will try to warmstart on preexisting logs. This behavior can be specified using the <code>overwrite</code> parameter.</p> <p>A facade is the entry point to SMAC, which constructs a default optimization  pipeline for you. SMAC offers various facades, which satisfy many common use cases and are crucial to achieving peak performance. The idea behind the facades is to provide a simple interface to all of SMAC's components, which is easy to use and understand and without the need of deep diving into the material. However, experts are invited to change the components to their specific hyperparameter optimization needs. The following table (horizontally scrollable) shows you what is supported and reveals the default components:</p> Black-Box Hyperparameter Optimization Multi-Fidelity Algorithm Configuration Random Hyperband #Parameters low low/medium/high low/medium/high low/medium/high low/medium/high low/medium/high Supports Instances \u274c \u2705 \u2705 \u2705 \u2705 \u2705 Supports Multi-Fidelity \u274c \u274c \u2705 \u2705 \u274c \u2705 Initial Design Sobol Sobol Random Default Default Default Surrogate Model Gaussian Process Random Forest Random Forest Random Forest Not used Not used Acquisition Function Expected Improvement Log Expected Improvement Log Expected Improvement Expected Improvement Not used Not used Acquisition Maximizer Local and Sorted Random Search Local and Sorted Random Search Local and Sorted Random Search Local and Sorted Random Search Not Used Not Used Intensifier Default Default Hyperband Default Default Hyperband Runhistory Encoder Default Log Log Default Default Default Random Design Probability 8.5% 20% 20% 50% Not used Not used <p>Info</p> <p>The multi-fidelity facade is the closest implementation to BOHB.</p> <p>Note</p> <p>We want to emphasize that SMAC is a highly modular optimization framework. The facade accepts many arguments to specify components of the pipeline. Please also note, that in contrast to previous versions, instantiated objects are passed instead of kwargs.</p> <p>The facades can be imported directly from the <code>smac</code> module.</p> <pre><code>from smac import BlackBoxFacade as BBFacade\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import MultiFidelityFacade as MFFacade\nfrom smac import AlgorithmConfigurationFacade as ACFacade\nfrom smac import RandomFacade as RFacade\nfrom smac import HyperbandFacade as HBFacade\n\nsmac = HPOFacade(scenario=scenario, target_function=train)\nsmac = MFFacade(scenario=scenario, target_function=train)\nsmac = ACFacade(scenario=scenario, target_function=train)\nsmac = RFacade(scenario=scenario, target_function=train)\nsmac = HBFacade(scenario=scenario, target_function=train)\n</code></pre>"},{"location":"4_minimal_example/","title":"Minimal Example","text":"<p>The following code optimizes a support vector machine on the iris dataset.</p> <pre><code>from ConfigSpace import Configuration, ConfigurationSpace\n\nimport numpy as np\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\niris = datasets.load_iris()\n\n\ndef train(config: Configuration, seed: int = 0) -&gt; float:\n    classifier = SVC(C=config[\"C\"], random_state=seed)\n    scores = cross_val_score(classifier, iris.data, iris.target, cv=5)\n    return 1 - np.mean(scores)\n\n\nconfigspace = ConfigurationSpace({\"C\": (0.100, 1000.0)})\n\n# Scenario object specifying the optimization environment\nscenario = Scenario(configspace, deterministic=True, n_trials=200)\n\n# Use SMAC to find the best configuration/hyperparameters\nsmac = HyperparameterOptimizationFacade(scenario, train)\nincumbent = smac.optimize()\n</code></pre>"},{"location":"6_references/","title":"References","text":"<ul> <li> <p>[LJDR18] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar;      Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization;      jmlr.org/papers/v18/16-558.html</p> </li> <li> <p>[HSSL22] Carl Hvarfner, Danny Stoll, Artur Souza, Marius Lindauer, Frank Hutter, Luigi Nardi;      \u03c0BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization;      arxiv.org/pdf/2204.11051.pdf</p> </li> <li> <p>[Know06] J. Knowles;      ParEGO: A Hybrid Algorithm with on-Line Landscape Approximation for Expensive Multiobjective Optimization Problems;      www.semanticscholar.org/paper/ParEGO%3A-a-hybrid-algorithm-with-on-line-landscape-Knowles/73b5b196b35fb23e1f908d73b787c2c2942fadb5</p> </li> <li> <p>[SKKS10] N. Srinivas, S. M. Kakade, A. Krause, M. Seeger;      Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design;      arxiv.org/pdf/0912.3995.pdf</p> </li> </ul>"},{"location":"7_glossary/","title":"Glossary","text":"<ul> <li>BB: See <code>Black-Box</code>.</li> <li>BO: See <code>Bayesian Optimization</code>.</li> <li>BOHB: Bayesian optimization and Hyperband.</li> <li>CLI: Command-Line Interface.</li> <li>CV: Cross-Validation.</li> <li>GP: Gaussian Process.</li> <li>GP-MCMC: Gaussian Process with Markov-Chain Monte-Carlo.</li> <li>HB: See <code>Hyperband</code>.</li> <li>HP: Hyperparameter.</li> <li>MF: See <code>Multi-Fidelity</code>.</li> <li>RF: Random Forest.</li> <li>ROAR: See <code>Random Online Adaptive Racing</code>.</li> <li>SMAC: Sequential Model-Based Algorithm Configuration.</li> <li>SMBO: Sequential Mode-Based Optimization.</li> <li>Bayesian Optimization: Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions. A Bayesian optimization weights exploration and exploitation to find the minimum of its objective.</li> <li>Black-Box: Refers to an algorithm being optimized, where only input and output are observable.</li> <li>Budget: Budget is another word for fidelity. Examples are the number of training epochs or the size of the data subset the algorithm is trained on. However, budget can also be used in the context of instances. For example, if you have 100 instances (let's say we optimize across datasets) and you want to run your algorithm on 10 of them, then the budget is 10.</li> <li>Hyperband: Hyperband. A novel bandit-based algorithm for hyperparameter optimization. Hyperband is an extension of successive halving and therefore works with multi-fidelities.</li> <li>Incumbent: The incumbent is the current best known configuration.</li> <li>Instances: Often you want to optimize across different datasets, subsets, or even different transformations (e.g. augmentation). In general, each of these is called an instance. Configurations are evaluated on multiple instances so that a configuration is found which performs superior on all instances instead of only a few.</li> <li>Intensification: A mechanism that governs how many evaluations to perform with each configuration and when to trust a configuration enough to make it the new current best known configuration (the incumbent).</li> <li>Multi-Fidelity: Multi-fidelity refers to running an algorithm on multiple budgets (such as number of epochs or subsets of data) and thereby evaluating the performance prematurely.</li> <li>Multi-Objective: A multi-objective optimization problem is a problem with more than one objective. The goal is to find a solution that is optimal or at least a good compromise in all objectives.</li> <li>Objective: An objective is a metric to evaluate the quality or performance of an algorithm.</li> <li>Random Online Adaptive Racing: Random Online Adaptive Racing. A simple model-free instantiation of the general <code>SMBO</code> framework. It selects configurations uniformly at random and iteratively compares them against the current incumbent using the intensification mechanism. See SMAC extended chapter 3.2 for details.</li> <li>Target Function: Your model, which returns a cost based on the given config, seed, budget, and/or instance.</li> <li>Trial: Trial is a single run of a target function on a combination of configuration, seed, budget and/or instance.</li> </ul>"},{"location":"8_faq/","title":"F.A.Q.","text":""},{"location":"8_faq/#should-i-use-smac2-or-smac3","title":"Should I use SMAC2 or SMAC3?","text":"<p>SMAC3 is a reimplementation of the original SMAC tool (Sequential Model-Based Optimization for General Algorithm Configuration, Hutter et al., 2021). However, the reimplementation slightly differs from the original   SMAC. For comparisons against the original SMAC, we refer to a stable release of SMAC (v2) in Java   which can be found here.   Since SMAC3 is actively maintained, we recommend to use SMAC3 for any AutoML applications.</p>"},{"location":"8_faq/#smac-cannot-be-imported","title":"SMAC cannot be imported.","text":"<p>Try to either run SMAC from SMAC's root directory or try to run the installation first.</p>"},{"location":"8_faq/#pyrfr-raises-cryptic-import-errors","title":"pyrfr raises cryptic import errors.","text":"<p>Ensure that the gcc used to compile the pyrfr is the same as used for linking   during execution. This often happens with Anaconda. See   Installation for a solution.</p>"},{"location":"8_faq/#how-can-i-use-termbohb-andor-hpbandster-with-smac","title":"How can I use :term:<code>BOHB</code> and/or HpBandSter with SMAC?","text":"<p>The facade MultiFidelityFacade is the closest implementation to :term:<code>BOHB</code> and/or HpBandSter.</p>"},{"location":"8_faq/#i-discovered-a-bug-or-smac-does-not-behave-as-expected-where-should-i-report-to","title":"I discovered a bug or SMAC does not behave as expected. Where should I report to?","text":"<p>Open an issue in our issue list on GitHub. Before you report a bug, please make sure that:</p> <ul> <li>Your bug hasn't already been reported in our issue tracker.</li> <li>You are using the latest SMAC3 version.</li> </ul> <p>If you found an issue, please provide us with the following information:</p> <ul> <li>A description of the problem.</li> <li>An example to reproduce the problem.</li> <li>Any information about your setup that could be helpful to resolve the bug (such as installed python packages).</li> <li>Feel free to add a screenshot showing the issue.</li> </ul>"},{"location":"8_faq/#i-want-to-contribute-code-or-discuss-a-new-idea-where-should-i-report-to","title":"I want to contribute code or discuss a new idea. Where should I report to?","text":"<p>SMAC uses the GitHub issue-tracker to also take care   of questions and feedback and is the preferred location for discussing new features and ongoing work. Please also have a look at our   contribution guide.</p>"},{"location":"8_faq/#what-is-the-meaning-of-deterministic","title":"What is the meaning of deterministic?","text":"<p>If the <code>deterministic</code> flag is set to <code>False</code> the target function is assumed to be non-deterministic.   To evaluate a configuration of a non-deterministic algorithm, multiple runs with different seeds will be evaluated   to determine the performance of that configuration on one instance.   Deterministic algorithms don't depend on seeds, thus requiring only one evaluation of a configuration on an instance   to evaluate the performance on that instance. Nevertheless the default seed 0 is still passed to the   target function.</p>"},{"location":"8_faq/#why-does-smac-not-run-on-colabmac-and-crashes-with-the-error-child-process-not-yet-created","title":"Why does SMAC not run on Colab/Mac and crashes with the error \"Child process not yet created\"?","text":"<p>SMAC uses pynisher to enforce time and memory limits on the target function runner. However, pynisher may not always   work on specific setups. To overcome this error, it is recommended to remove limitations to make SMAC run.</p>"},{"location":"9_license/","title":"License","text":"<p>This program is free software: you can redistribute it and/or modify it under the terms of the 3-clause BSD license (please see the LICENSE file). This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. You should have received a copy of the 3-clause BSD license along with this program (see LICENSE file). If not, see BSD-3-Clause license.</p>"},{"location":"advanced_usage/10_continue/","title":"Continue","text":"<p>SMAC can automatically restore states where it left off if a run was interrupted or prematurely finished. To do so,  it reads in old files (derived from scenario's name, output_directory and seed) and obtains the scenario information of the previous run from those to continue the run.</p> <p>The behavior can be controlled by setting the parameter <code>overwrite</code> in the facade to True or False, respectively:</p> <ul> <li>If set to True, SMAC overwrites the run results if a previous run is found that is consistent in the meta data with the current setup.</li> <li> <p>If set to False and a previous run is found that</p> </li> <li> <p>is consistent in the meta data, the run is continued. </p> </li> <li>is not consistent in the meta data, the user is asked for the exact behaviour (overwrite completely or rename old run first).</li> </ul> <p>.. warning::</p> <pre><code>If you changed any code affecting the run's meta data and specified a name, SMAC will ask you whether you still \nwant to overwrite the old run or rename the old run first. If you did not specify a name, SMAC generates a new name \nand the old run is not affected.\n</code></pre> <p>Please have a look at our continue example.</p>"},{"location":"advanced_usage/11_reproducibility/","title":"Reproducibility","text":"<p>Reproducibility can only be ensured if one worker is used and no time (wallclock or CPU time) is involved.</p>"},{"location":"advanced_usage/12_optimizations/","title":"Optimizations","text":"<p>SMAC might run faster or slower depending on the user specifications. In general it applies that the more you know about the underlying target function, the better you can optimize the optimization process.</p> <p>The following list might help you to make the optimization process more efficient:</p> <ul> <li>Intensifier -&gt; <code>max_config_calls</code>: Higher numbers lead to less configurations.</li> <li> <p>ConfigSelector -&gt; <code>retrain_after</code>: The lower the number, the more often the model is retrained. Recommendation:</p> </li> <li> <p>High target function evaluation times: Low <code>retrain_after</code> (e.g., 1).</p> </li> <li> <p>Low target function evaluation times: High <code>retrain_after</code> (e.g., 8).</p> </li> <li> <p>Scenario -&gt; <code>n_workers</code>: The higher the number, the more configurations are evaluated in parallel. Recommendation:</p> </li> <li> <p>High target function evaluation times: As many <code>n_workers</code> as cores.</p> </li> <li>Low target function evaluation times: Only one worker because the communication might take longer than evaluating     on a single thread.   </li> </ul>"},{"location":"advanced_usage/1_components/","title":"Components","text":"<p>In addition to the basic components mentioned in Getting Started, all other components are explained in the following paragraphs to give a better picture of SMAC. These components are all used to guide the optimization process and simple changes can influence the results drastically.</p> <p>Before diving into the components, we shortly want to explain the main Bayesian optimization loop in SMAC. The SMBO receives all instantiated components from the facade and the logic happens here. In general, a while loop is used to ask for the next trial, submit it to the runner, and wait for the runner to  finish the evaluation. Since the runner and the <code>SMBO</code> object are decoupled, the while loop continues and asks for even  more trials (e.g., in case of multi-threading), which also can be submitted to the runner. If all workers are occupied, SMAC will wait until a new worker is available again. Moreover, limitations like wallclock time and remaining  trials are checked in every iteration.</p>"},{"location":"advanced_usage/1_components/#surrogate-model","title":"Surrogate Model","text":"<p>The surrogate model is used to approximate the objective function of configurations. In previous versions, the model was  referred to as the Empirical Performance Model (EPM). Mostly, Bayesian optimization is used/associated with Gaussian processes. However, SMAC also incorporates random forests as surrogate models, which makes it possible to optimize for  higher dimensional and complex spaces.</p> <p>The data used to train the surrogate model is collected by the runhistory encoder (receives data from the runhistory  and transforms it). If budgets are involved, the highest budget which satisfies <code>min_trials</code> (defaults to 1) in smac.main.config_selector is used. If no budgets are used, all observations are used.</p> <p>If you are using instances, it is recommended to use instance features. The model is trained on each instance  associated with its features. Imagine you have two hyperparameters, two instances and no instance features, the model  would be trained on:</p> HP 1 HP 2 Objective Value 0.1 0.8 0.5 0.1 0.8 0.75 505 7 2.4 505 7 1.3 <p>You can see that the same inputs lead to different objective values because of two instances. If you associate each instance with a feature, you would end-up with the following data points:</p> HP 1 HP 2 Instance Feature Objective Value 0.1 0.8 0 0.5 0.1 0.8 1 0.75 505 7 0 2.4 505 7 1 1.3 <p>The steps to receiving data are as follows:</p> <ul> <li>The intensifier requests new configurations via <code>next(self.config_generator)</code>.</li> <li>The config selector collects the data via the runhistory encoder which iterates over the runhistory trials.</li> <li>The runhistory encoder only collects trials which are in <code>considered_states</code> and timeout trials. Also, only the    highest budget is considered if budgets are used. In this step, multi-objective values are scalarized using the    <code>normalize_costs</code> function (uses <code>objective_bounds</code> from the runhistory) and the multi-objective algorithm.    For example, when ParEGO is used, the scalarization would be different in each training.</li> <li>The selected trial objectives are transformed (e.g., log-transformed, depending on the selected    encoder).</li> <li>The hyperparameters might still have inactive values. The model takes care of that after the collected data    are passed to the model.</li> </ul>"},{"location":"advanced_usage/1_components/#acquisition-function","title":"Acquisition Function","text":"<p>Acquisition functions are mathematical techniques that guide how the parameter space should be explored during Bayesian  optimization. They use the predicted mean and predicted variance generated by the surrogate model. </p> <p>The acquisition function is used by the acquisition maximizer (see next section). Otherwise, SMAC provides a bunch of different acquisition functions (Lower Confidence Bound, Expected Improvement, Probability Improvement,  Thompson, integrated acquisition functions and prior acquisition functions). We refer to literature  for more information about acquisition functions.</p> <p>Note</p> <p>The acquisition function calculates the acquisition value for each configuration. However, the configurations are provided by the acquisition maximizer. Therefore, the acquisition maximizer is responsible for receiving the next configurations.</p>"},{"location":"advanced_usage/1_components/#acquisition-maximize","title":"Acquisition Maximize","text":"<p>The acquisition maximizer is a wrapper for the acquisition function. It returns the next configurations. SMAC supports local search, (sorted) random search, local and (sorted) random search, and differential evolution. While local search checks neighbours of the best configurations, random search makes sure to explore the configuration space. When using sorted random search, random configurations are sorted by the value of the acquisition function.</p> <p>Warning</p> <p>Pay attention to the number of challengers: If you experience RAM issues or long computational times in the acquisition function, you might lower the number of challengers.</p> <p>The acquisition maximizer also incorporates the Random Design. Please see the ChallengerList for more information.</p>"},{"location":"advanced_usage/1_components/#initial-design","title":"Initial Design","text":"<p>The surrogate model needs data to be trained. Therefore, the initial design is used to generate the initial data points. We provide random, latin hypercube, sobol, factorial and default initial designs. The default initial design uses the default configuration from the configuration space and with the factorial initial design, we generate corner points of the configuration space. The sobol sequences are an example of quasi-random low-discrepancy sequences and the latin hypercube design is a statistical method for generating a near-random sample of parameter values from a multidimensional distribution.</p> <p>The initial design configurations are yielded by the config selector first. Moreover, the config selector keeps track of which configurations already have been returned to make sure a configuration is not returned twice.</p> <p></p>"},{"location":"advanced_usage/1_components/#random-design_1","title":"Random Design","text":"<p>The random design is used in the acquisition maximizer to tell whether the next configuration should be random or sampled from the acquisition function. For example, if we use a random design with a probability of  50%, we have a 50% chance to sample a random configuration and a 50% chance to sample a configuration from the acquisition function (although the acquisition function includes exploration and exploitation trade-off already).  This design makes sure that the optimization process is not stuck in a local optimum and we  are guaranteed to find the best configuration over time.</p> <p>In addition to simple probability random design, we also provide annealing and modulus random design.</p>"},{"location":"advanced_usage/1_components/#intensifier","title":"Intensifier","text":"<p>The intensifier compares different configurations based on evaluated :term:<code>trial&lt;Trial&gt;</code> so far. It decides which configuration should be <code>intensified</code> or, in other words, if a configuration is worth to spend more time on (e.g., evaluate another seed pair, evaluate on another instance, or evaluate on a higher budget).</p> <p>Warning</p> <p>Always pay attention to <code>max_config_calls</code> or <code>n_seeds</code>: If this argument is set high, the intensifier might  spend a lot of time on a single configuration.</p> <p>Depending on the components and arguments, the intensifier tells you which seeds, budgets, and/or instances are used throughout the optimization process. You can use the methods <code>uses_seeds</code>, <code>uses_budgets</code>, and  <code>uses_instances</code> (directly callable via the facade) to (sanity-)check whether the intensifier uses these arguments.</p> <p>Another important fact is that the intensifier keeps track of the current incumbent (a.k.a. the best configuration  found so far). In case of multi-objective, multiple incumbents could be found.</p> <p>All intensifiers support multi-objective, multi-fidelity, and multi-threading:</p> <ul> <li>Multi-Objective: Keeping track of multiple incumbents at once.</li> <li>Multi-Fidelity: Incorporating instances or budgets.</li> <li>Multi-Threading: Intensifier are implemented as generators so that calling <code>next</code> on the intensifier can be   repeated as often as needed. Intensifier are not required to receive results as the results are directly taken from   the runhistory.</li> </ul> <p>Note</p> <p>All intensifiers are working on the runhistory and recognize previous logged trials (e.g., if the user already evaluated something beforehand). Previous configurations (in the best case, also complete trials) are added to the  queue/tracker again so that they are integrated into the intensification process.</p> <p>That means continuing a run as well as incorporating user inputs are natively supported.</p>"},{"location":"advanced_usage/1_components/#configuration-selector","title":"Configuration Selector","text":"<p>The configuration selector uses the initial design, surrogate model, acquisition maximizer/function, runhistory, runhistory encoder, and random design to select the next configuration. The configuration selector is directly used by the intensifier and is called everytime a new configuration is requested. </p> <p>The idea behind the configuration selector is straight forward:</p> <ul> <li>Yield the initial design configurations.</li> <li>Train the surrogate model with the data from the runhistory encoder.</li> <li>Get the next <code>retrain_after</code> configurations from the acquisition function/maximizer and yield them.</li> <li>After all <code>retrain_after</code> configurations were yield, go back to step 2.</li> </ul> <p>Note</p> <p>The configuration selector is a generator and yields configurations. Therefore, the current state of the  selector is saved and when the intensifier calls <code>next</code>, the selector continues there where it stopped.</p> <p>Note</p> <p>Everytime the surrogate model is trained, the multi-objective algorithm is updated via  <code>update_on_iteration_start</code>.</p>"},{"location":"advanced_usage/1_components/#multi-objective-algorithm","title":"Multi-Objective Algorithm","text":"<p>The multi-objective algorithm is used to scalarize multi-objective values. The multi-objective algorithm  gets normalized objective values passed and returns a single value. The resulting value (called by the  runhistory encoder) is then used to train the surrogate model.</p> <p>Warning</p> <p>Depending on the multi-objective algorithm, the values for the runhistory encoder might differ each time  the surrogate model is trained. Let's take ParEGO for example: Everytime a new configuration is sampled (see ConfigSelector), the objective weights are updated. Therefore, the scalarized values are different and the acquisition maximizer might return completely different configurations.</p>"},{"location":"advanced_usage/1_components/#runhistory","title":"RunHistory","text":"<p>The runhistory holds all (un-)evaluated trials of the optimization run. You can use the runhistory to  get (running) configs, (running) trials, trials of a specific config, and more. The runhistory encoder iterates over the runhistory to receive data for the surrogate model. The following  code shows how to iterate over the runhistory:</p> <pre><code>smac = HPOFacade(...)\n\n# Iterate over all trials\nfor trial_info, trial_value in smac.runhistory.items():\n    # Trial info\n    config = trial_info.config\n    instance = trial_info.instance\n    budget = trial_info.budget\n    seed = trial_info.seed\n\n    # Trial value\n    cost = trial_value.cost\n    time = trial_value.time\n    status = trial_value.status\n    starttime = trial_value.starttime\n    endtime = trial_value.endtime\n    additional_info = trial_value.additional_info\n\n# Iterate over all configs\nfor config in smac.runhistory.get_configs():\n    # Get the cost of all trials of this config\n    average_cost = smac.runhistory.average_cost(config)\n</code></pre> <p>Warning</p> <p>The intensifier uses a callback to update the incumbent everytime a new trial is added to the runhistory.</p>"},{"location":"advanced_usage/1_components/#runhistory-encoder","title":"RunHistory Encoder","text":"<p>The runhistory encoder is used to encode the runhistory data into a format that can be used by the surrogate model. Only trials with the status <code>considered_states</code> and timeout trials are considered. Multi-objective values are  scalarized using the <code>normalize_costs</code> function (uses <code>objective_bounds</code> from the runhistory). Afterwards, the  normalized value is processed by the multi-objective algorithm. </p>"},{"location":"advanced_usage/1_components/#callback","title":"Callback","text":"<p>Callbacks provide the ability to easily execute code before, inside, and after the Bayesian optimization loop. To add a callback, you have to inherit from <code>smac.Callback</code> and overwrite the methods (if needed). Afterwards, you can pass the callbacks to any facade. </p> <pre><code>from smac import MultiFidelityFacade, Callback\n\n\nclass CustomCallback(Callback):\n    def on_start(self, smbo: SMBO) -&gt; None:\n        pass\n\n    def on_end(self, smbo: SMBO) -&gt; None:\n        pass\n\n    def on_iteration_start(self, smbo: SMBO) -&gt; None:\n        pass\n\n    def on_iteration_end(self, smbo: SMBO, info: RunInfo, value: RunValue) -&gt; bool | None:\n        # We just do a simple printing here\n        print(info, value)\n\n\nsmac = MultiFidelityFacade(\n    ...\n    callbacks=[CustomCallback()]\n)\nsmac.optimize()\n</code></pre>"},{"location":"advanced_usage/2_multi_fidelity/","title":"Multi-Fidelity Optimization","text":"<p>Multi-fidelity refers to running an algorithm on multiple budgets (such as number of epochs or subsets of data) and thereby evaluating the performance prematurely. You can run a multi-fidelity optimization when using Successive Halving or  Hyperband. <code>Hyperband</code> is the default intensifier in the  multi-fidelity facade and requires the arguments  <code>min_budget</code> and <code>max_budget</code> in the scenario if no instances are used.</p> <p>In general, multi-fidelity works for both real-valued and instance budgets. In the real-valued case, the budget is directly passed to the target function. In the instance case, the budget is not passed to the  target function but <code>min_budget</code> and <code>max_budget</code> are used internally to determine the number of instances of  each stage. That's also the reason why <code>min_budget</code> and <code>max_budget</code> are not required when using instances:  The <code>max_budget</code> is simply the max number of instances, whereas the <code>min_budget</code> is simply 1.</p> <p>Warning</p> <p><code>smac.main.config_selector.ConfigSelector</code> contains the <code>min_trials</code> parameter. This parameter determines how many samples are required to train the surrogate model. If budgets are involved, the highest budgets  are checked first. For example, if min_trials is three, but we find only two trials in the runhistory for the highest budget, we will use trials of a lower budget instead.</p> <p>Please have a look into our multi-fidelity examples to see how to use multi-fidelity optimization in real-world applications.</p>"},{"location":"advanced_usage/3_multi_objective/","title":"Multi-Objective Optimization","text":"<p>Often we do not only want to optimize just a single objective, but multiple instead. SMAC offers a multi-objective  optimization interface to do exactly that. Right now, the algorithm used for this is a mean aggregation strategy or  ParEGO [Know06]. In both cases, multiple objectives are aggregated into a single scalar objective, which is then  optimized by SMAC. However, the run history still keeps the original objectives.</p> <p>The basic recipe is as follows:</p> <ul> <li>Specify the objectives in the scenario object as list. For example, <code>Scenario(objectives=[\"obj1\", \"obj2\"])</code>.</li> <li>Make sure that your target function returns a cost dictionary containing the objective names as keys   and the objective values as values, e.g. <code>{'obj1': 0.3, 'obj2': 200}</code>. Alternatively, you can simply   return a list, e.g., <code>[0.3, 200]</code>.</li> <li>Now you can optionally pass a custom multi-objective algorithm class to the SMAC   facade (via <code>multi_objective_algorithm</code>). In all facades, a mean aggregation strategy is used as the    multi-objective algorithm default.</li> </ul> <p>Warning</p> <p>The multi-objective algorithm influences which configurations are sampled next. More specifically,  since only one surrogate model is trained, multiple objectives have to be scalarized into a single objective. This scalarized value is used to train the surrogate model, which is used by the acquisition function/maximizer to sample the next configurations.  </p> <p>You receive the incumbents (points on the Pareto front) after the optimization process directly. Alternatively, you can  use the method <code>get_incumbents</code> in the intensifier.</p> <pre><code>  smac = ...\n  incumbents = smac.optimize()\n\n  # Or you use the intensifier\n  incumbents = smac.intensifier.get_incumbents()\n</code></pre> <p>We show an example of how to use multi-objective with plots in our examples.</p>"},{"location":"advanced_usage/4_instances/","title":"Optimization across Instances","text":"<p>Often you want to optimize the cost across different datasets, subsets, or even different augmentations. For this purpose, you can use instances.</p> <p>To work with instances, you need to add your pre-defined instance names to the scenario object. In the following example, we want to use five different subsets, identified by its id:</p> <pre><code>instances = [\"d0\", \"d1\", \"d2\", \"d3\", \"d4\"]\nscenario = Scenario(\n  ...\n  \"instances\": instances,\n  ...\n)\n</code></pre> <p>Additionally to the instances, there is the option to define <code>instance_features</code>. Those instance features are used to expand the internal X matrix and thus play a role in training the underlying surrogate model. For example, if I  want to add the number of samples and the mean of each subset, I can do as follows:</p> <pre><code>  instance_features = {\n    \"d0\": [121, 0.6],\n    \"d1\": [140, 0.65],\n    \"d2\": [99, 0.45],\n    \"d3\": [102, 0.59],\n    \"d4\": [132, 0.48],\n  }\n\n  scenario = Scenario(\n    ...\n    instances=instances,\n    instance_features=instance_features,\n    ...\n  )\n</code></pre>"},{"location":"advanced_usage/5.1_warmstarting/","title":"Warmstarting SMAC","text":"<p>With the ask and tell interface, we can support warmstarting SMAC. We can communicate rich information about the previous trials to SMAC using <code>TrialInfo</code> and <code>TrialValue</code> instances.</p> <p>We can communicate using the following objects:</p> <pre><code>class TrialValue:\n    \"\"\"Values of a trial.\n\n    Parameters\n    ----------\n    cost : float | list[float]\n    time : float, defaults to 0.0\n    status : StatusType, defaults to StatusType.SUCCESS\n    starttime : float, defaults to 0.0\n    endtime : float, defaults to 0.0\n    additional_info : dict[str, Any], defaults to {}\n    \"\"\"\n\nclass TrialInfo:\n    \"\"\"Information about a trial.\n\n    Parameters\n    ----------\n    config : Configuration\n    instance : str | None, defaults to None\n    seed : int | None, defaults to None\n    budget : float | None, defaults to None\n    \"\"\"\n</code></pre>"},{"location":"advanced_usage/5.1_warmstarting/#usage-example","title":"Usage Example","text":"<p>See <code>examples/1_basics/8_warmstart.py</code>.</p> <pre><code>from __future__ import annotations\n\nfrom smac.scenario import Scenario\nfrom smac.facade import HyperparameterOptimizationFacade\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom smac.runhistory.dataclasses import TrialValue, TrialInfo\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def evaluate(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"The 2-dimensional Rosenbrock function as a toy model.\n        The Rosenbrock function is well know in the optimization community and\n        often serves as a toy problem. It can be defined for arbitrary\n        dimensions. The minimium is always at x_i = 1 with a function value of\n        zero. All input parameters are continuous. The search domain for\n        all x's is the interval [-5, 10].\n        \"\"\"\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nif __name__ == \"__main__\":\n    SEED = 12345\n    task = Rosenbrock2D()\n\n    # Previous evaluations\n    # X vectors need to be connected to the configuration space\n    configurations = [\n        Configuration(task.configspace, {'x0':1, 'x1':2}),\n        Configuration(task.configspace, {'x0':-1, 'x1':3}),\n        Configuration(task.configspace, {'x0':5, 'x1':5}),\n    ]\n    costs = [task.evaluate(c, seed=SEED) for c in configurations]\n\n    # Define optimization problem and budget\n    scenario = Scenario(task.configspace, deterministic=False, n_trials=30)\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(scenario, max_config_calls=1)\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        task.evaluate,\n        intensifier=intensifier,\n        overwrite=True,\n\n        # Modify the initial design to use our custom initial design\n        initial_design=HyperparameterOptimizationFacade.get_initial_design(\n            scenario, \n            n_configs=0,  # Do not use the default initial design\n            additional_configs=configurations  # Use the configurations previously evaluated as initial design\n                                            # This only passes the configurations but not the cost!\n                                            # So in order to actually use the custom, pre-evaluated initial design\n                                            # we need to tell those trials, like below.\n        )\n    )\n\n    # Convert previously evaluated configurations into TrialInfo and TrialValue instances to pass to SMAC\n    trial_infos = [TrialInfo(config=c, seed=SEED) for c in configurations]\n    trial_values = [TrialValue(cost=c) for c in costs]\n\n    # Warmstart SMAC with the trial information and values\n    for info, value in zip(trial_infos, trial_values):\n        smac.tell(info, value)\n\n    # Optimize as usual\n    smac.optimize()\n</code></pre> <p>For more details on ask and tell consult <code>advanced_usage/5_ask_and_tell</code>.</p>"},{"location":"advanced_usage/5_ask_and_tell/","title":"Ask-and-Tell Interface","text":"<p>SMAC provides an ask-and-tell interface in v2.0, giving the user the opportunity to ask for the next trial  and report the results of the trial. </p> <p>Warning</p> <p>When specifying <code>n_trials</code> in the scenario and trials have been registered by the user, SMAC will  count the users trials as well. However, the wallclock time will first start when calling <code>optimize</code>.</p> <p>Warning</p> <p>It might be the case that not all user-provided trials can be considered. Take Successive Halving, for example,  when specifying the min and max budget, intermediate budgets are calculated. If the user provided trials with different budgets, they, obviously, can not be considered. However, all user-provided configurations will flow  into the intensification process.</p> <p>Please have a look at our ask-and-tell example.</p>"},{"location":"advanced_usage/6_commandline/","title":"Command-Line Interface","text":"<p>The command-line interface enables the user to run target functions which are non-python code.  The passed and further called script (using <code>Popen</code>) needs to return a standard output which is then interpreted  to perform the optimization process. </p> <p>Note</p> <p>In SMAC v2.0, SMAC can not be called from the command-line directly. Instead, the user should use the python  interface to call SMAC. The command-line interface is still available in SMAC v1.4.</p>"},{"location":"advanced_usage/6_commandline/#call-of-the-target-function","title":"Call of the Target Function","text":"<p>The following example shows how the script is called:</p> <pre><code>filename --instance=test --instance_features=test --seed=0 --hyperparameter1=5323\n</code></pre> <p>However, as for the python target function, the arguments like instance or budget are depending on which components are used. The hyperparameters are depending on the configuration space. The variable <code>filename</code> could be  something like <code>./path/to/your/script.sh</code>.</p> <p>We recommend using the following code to receive the arguments in a bash script. Please note that the user is not limited to bash scripts but can also use executables, python scripts or anything else.</p> <p>Note</p> <p>Since the script is called wih the filename only, make sure to mark the type of the file (e.g., <code>#!/bin/bash</code>  or <code>#!/usr/bin/env python</code>).</p> <p>Warning</p> <p>Everytime an instance is passed, also an instance feature in form of a comma-separated list (no spaces) of floats is passed. If no instance feature for the instance is given, an empty list is passed.</p> <pre><code>#!/bin/bash\n\n# Set arguments first\nfor argument in \"$@\"\ndo\n    key=$(echo $argument | cut -f1 -d=)\n    value=$(echo $argument | cut -f2 -d=)   \n\n    if [[ $key == *\"--\"* ]]; then\n        v=\"${key/--/}\"\n        declare $v=\"${value}\" \n    fi\ndone\n\necho $instance\necho $hyperparameter1\n</code></pre>"},{"location":"advanced_usage/6_commandline/#return-of-the-target-function","title":"Return of the Target Function","text":"<p>The script must return an stdout (echo or print) in the following form (white-spaces are ignored):</p> <pre><code>cost=0.5; runtime=0.01; status=SUCCESS; additional_info=test (single-objective)\ncost=0.5, 0.4; runtime=0.01; status=SUCCESS; additional_info=test (multi-objective)\n</code></pre> <p>All arguments are optional except cost and are separated by a semicolon. The string of the status must match one of the values from <code>StatusType</code>.</p>"},{"location":"advanced_usage/6_commandline/#start-the-optimization","title":"Start the Optimization","text":"<p>The optimization will be started by the normal python interface. The only difference is that you need to pass a string as target function instead of a python function.</p> <p>!! warning</p> <pre><code>Your script needs to have rights to be executed (e.g., update the rights with ``chmod``).\n</code></pre> <pre><code>...\nsmac = BlackBoxFacade(scenario, target_function=\"./path/to/your/script.sh\")\nincumbent = smac.optimize()\n...\n</code></pre>"},{"location":"advanced_usage/7_stopping_criteria/","title":"Stopping Criteria","text":"<p>In addition to the standard stopping criteria like number of trials or wallclock time, SMAC also provides  more advanced criteria.</p>"},{"location":"advanced_usage/7_stopping_criteria/#termination-cost-threshold","title":"Termination Cost Threshold","text":"<p>SMAC can stop the optimization process after a user-defined cost was reached. In each iteration, the average cost  (using <code>average_cost</code> from the run history) from the incumbent is compared to the termination cost threshold. If one of the objective costs is below its associated termination cost threshold, the optimization process is stopped. Note, since the <code>average_cost</code> method is used, all instance-seed-budget trials of the incumbent are considered so far. In other words, the process can be stopped even if the incumbent has not been evaluated on all instances, on the  highest fidelity, or on all seeds.</p> <pre><code>scenario = Scenario(\n    ...\n    objectives=[\"accuracy\", \"runtime\"],\n    termination_cost_threshold=[0.1, np.inf]\n    ...\n)\n</code></pre> <p>In the code above, the optimization process is stopped if the average accuracy of the incumbent is below 0.1. The  runtime is ignored completely as it is set to infinity. Note here again that SMAC minimizes the objective values.</p>"},{"location":"advanced_usage/7_stopping_criteria/#automatically-stopping","title":"Automatically Stopping","text":"<p>Coming soon. \ud83d\ude0a</p>"},{"location":"advanced_usage/8_logging/","title":"Logging","text":"<p>Logging is a crucial part of the optimization, which should be customizable by the user. This page gives you the overview how to customize the logging experience with SMAC.</p>"},{"location":"advanced_usage/8_logging/#level","title":"Level","text":"<p>The easiest way to change the logging behaviour is to change the level of the global logger. SMAC does this for you if you specify the <code>logging_level</code> in any facade.</p> <pre><code>smac = Facade(\n    ...\n    logging_level=20,\n    ...\n)\n</code></pre> <p>The table shows you the specific levels:</p> Name Level 0 SHOW ALL 10 DEBUG 20 INFO 30 WARNING 40 ERROR 50 CRITICAL"},{"location":"advanced_usage/8_logging/#standard-logging-files","title":"Standard Logging Files","text":"<p>By default, SMAC generates several files to document the optimization process. These files are stored in the directory structure <code>./output_directory/name/seed</code>, where name is replaced by a hash if no name is explicitly provided. This behavior can be customized through the Scenario configuration, as shown in the example below: <pre><code>Scenario(\n    configspace = some_configspace,\n    name = 'experiment_name',\n    output_directory = Path('some_directory'),\n    ...\n)\n</code></pre> Notably, if an output already exists at <code>./some_directory/experiment_name/seed</code>, the behavior is determined by the overwrite parameter in the facade's settings. This parameter specifies whether to continue the previous run (default) or start a new run.</p> <p>The output is split into four different log files, and a copy of the utilized Configuration Space of the ConfigSpace library.</p>"},{"location":"advanced_usage/8_logging/#intensifierjson","title":"intensifier.json","text":"<p>The intensification is logged in <code>intensifier.json</code> and has the following structure:</p> <pre><code>{\n  \"incumbent_ids\": [\n    65\n  ],\n  \"rejected_config_ids\": [\n    1,\n  ],\n  \"incumbents_changed\": 2,\n  \"trajectory\": [\n    {\n      \"config_ids\": [\n        1\n      ],\n      \"costs\": [\n        0.45706284046173096\n      ],\n      \"trial\": 1,\n      \"walltime\": 0.029736042022705078\n    },\n    #...\n  ],\n  \"state\": {\n    \"tracker\": {},\n    \"next_bracket\": 0\n  }\n}\n</code></pre>"},{"location":"advanced_usage/8_logging/#optimizationjson","title":"optimization.json","text":"<p>The optimization process is portrayed in <code>optimization.json</code> with the following structure</p> <pre><code>{\n  \"used_walltime\": 184.87366724014282,\n  \"used_target_function_walltime\": 20.229533672332764,\n  \"last_update\": 1732703596.5609574,\n  \"finished\": false\n}\n</code></pre>"},{"location":"advanced_usage/8_logging/#runhistoryjson","title":"runhistory.json","text":"<p>The runhistory.json in split into four parts. <code>stats</code>, <code>data</code>, <code>configs</code>, and <code>config_origins</code>. <code>stats</code> contains overall broad stats on the different evaluated configurations: <pre><code>  \"stats\": {\n    \"submitted\": 73,\n    \"finished\": 73,\n    \"running\": 0\n  },\n</code></pre></p> <p><code>data</code> contains a list of entries, one for each configuration. <pre><code>  \"data\": [\n    {\n      \"config_id\": 1,\n      \"instance\": null,\n      \"seed\": 209652396,\n      \"budget\": 2.7777777777777777,\n      \"cost\": 2147483647.0,\n      \"time\": 0.0,\n      \"cpu_time\": 0.0,\n      \"status\": 0,\n      \"starttime\": 0.0,\n      \"endtime\": 0.0,\n      \"additional_info\": {}\n    },\n    ...\n  ]\n</code></pre></p> <p><code>configs</code> is a human-readable dictionary of configurations, where the keys are the one-based <code>config_id</code>. It is important to note that in <code>runhistory.json</code>, the indexing is zero-based. <pre><code>  \"configs\": {\n    \"1\": {\n      \"x\": -2.3312147893012\n    },\n</code></pre></p> <p>Lastly, <code>config_origins</code> specifies the source of a configuration, indicating whether it stems from the initial design or results from the maximization of an acquisition function. <pre><code>  \"config_origins\": {\n    \"1\": \"Initial Design: Sobol\",\n    ...\n  }\n</code></pre></p>"},{"location":"advanced_usage/8_logging/#scenariojson","title":"scenario.json","text":"<p>The \u00b4scenario.json\u00b4 file contains the overall state of the Scenario logged to a json file.</p>"},{"location":"advanced_usage/8_logging/#custom-file","title":"Custom File","text":"<p>Sometimes, the user wants to disable or highlight specify modules. You can do that by passing a custom yaml file to the facade instead.</p> <pre><code>smac = Facade(\n    ...\n    logging_level=\"path/to/your/logging.yaml\",\n    ...\n)\n</code></pre> <p>The following file shows you how to display only error messages from the intensifier  but keep the level of everything else on INFO:</p> <pre><code>version: 1\ndisable_existing_loggers: false\nformatters:\n    simple:\n        format: '[%(levelname)s][%(filename)s:%(lineno)d] %(message)s'\nhandlers:\n    console:\n        class: logging.StreamHandler\n        level: INFO\n        formatter: simple\n        stream: ext://sys.stdout\nloggers:\n    smac.intensifier:\n        level: ERROR\n        handlers: [console]\nroot:\n    level: INFO\n    handlers: [console]\n</code></pre>"},{"location":"advanced_usage/9_parallelism/","title":"Parallelism","text":"<p>SMAC supports multiple workers natively via Dask. Just specify <code>n_workers</code> in the scenario and you are ready to go. </p> <p>Note</p> <p>Please keep in mind that additional workers are only used to evaluate trials. The main thread still orchestrates the optimization process, including training the surrogate model.</p> <p>Warning</p> <p>Using high number of workers when the target function evaluation is fast might be counterproductive due to the  overhead of communcation. Consider using only one worker in this case.</p> <p>Warning</p> <p>When using multiple workers, SMAC is not reproducible anymore.</p>"},{"location":"advanced_usage/9_parallelism/#running-on-a-cluster","title":"Running on a Cluster","text":"<p>You can also pass a custom dask client, e.g. to run on a slurm cluster. See our parallelism example.</p> <p>Warning</p> <p>On some clusters you cannot spawn new jobs when running a SLURMCluster inside a job instead of on the login node. No obvious errors might be raised but it can hang silently.</p> <p>Warning</p> <p>Sometimes you need to modify your launch command which can be done with <code>SLURMCluster.job_class.submit_command</code>.    </p> <pre><code>cluster.job_cls.submit_command = submit_command\ncluster.job_cls.cancel_command = cancel_command\n</code></pre>"},{"location":"api/smac/constants/","title":"Constants","text":""},{"location":"api/smac/constants/#smac.constants","title":"smac.constants","text":"<p>Constants used in SMAC, e.g. maximum number of cutoffs, very small number, etc.</p>"},{"location":"api/smac/scenario/","title":"Scenario","text":""},{"location":"api/smac/scenario/#smac.scenario","title":"smac.scenario","text":""},{"location":"api/smac/scenario/#smac.scenario.Scenario","title":"Scenario  <code>dataclass</code>","text":"<pre><code>Scenario(\n    configspace: ConfigurationSpace,\n    name: str | None = None,\n    output_directory: Path = Path(\"smac3_output\"),\n    deterministic: bool = False,\n    objectives: str | list[str] = \"cost\",\n    crash_cost: float | list[float] = inf,\n    termination_cost_threshold: float | list[float] = inf,\n    walltime_limit: float = inf,\n    cputime_limit: float = inf,\n    trial_walltime_limit: float | None = None,\n    trial_memory_limit: int | None = None,\n    n_trials: int = 100,\n    use_default_config: bool = False,\n    instances: list[str] | None = None,\n    instance_features: dict[str, list[float]] | None = None,\n    min_budget: float | int | None = None,\n    max_budget: float | int | None = None,\n    seed: int = 0,\n    n_workers: int = 1,\n)\n</code></pre> <p>The scenario manages environment variables and therefore gives context in which frame the optimization is performed.</p>"},{"location":"api/smac/scenario/#smac.scenario.Scenario--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace     The configuration space from which to sample the configurations. name : str | None, defaults to None     The name of the run. If no name is passed, SMAC generates a hash from the meta data.     Specify this argument to identify your run easily. output_directory : Path, defaults to Path(\"smac3_output\")     The directory in which to save the output. The files are saved in <code>./output_directory/name/seed</code>. deterministic : bool, defaults to False     If deterministic is set to true, only one seed is passed to the target function.     Otherwise, multiple seeds (if n_seeds of the intensifier is greater than 1) are passed     to the target function to ensure generalization. objectives : str | list[str] | None, defaults to \"cost\"     The objective(s) to optimize. This argument is required for multi-objective optimization. crash_cost : float | list[float], defaults to np.inf     Defines the cost for a failed trial. In case of multi-objective, each objective can be associated with     a different cost. termination_cost_threshold : float | list[float], defaults to np.inf     Defines a cost threshold when the optimization should stop. In case of multi-objective, each objective must be     associated with a cost. The optimization stops when all objectives crossed the threshold. walltime_limit : float, defaults to np.inf     The maximum time in seconds that SMAC is allowed to run. cputime_limit : float, defaults to np.inf     The maximum CPU time in seconds that SMAC is allowed to run. trial_walltime_limit : float | None, defaults to None     The maximum time in seconds that a trial is allowed to run. If not specified,     no constraints are enforced. Otherwise, the process will be spawned by pynisher. trial_memory_limit : int | None, defaults to None     The maximum memory in MB that a trial is allowed to use. If not specified,     no constraints are enforced. Otherwise, the process will be spawned by pynisher. n_trials : int, defaults to 100     The maximum number of trials (combination of configuration, seed, budget, and instance, depending on the task)     to run. use_default_config: bool, defaults to False.     If True, the configspace's default configuration is evaluated in the initial design.     For historic benchmark reasons, this is False by default.     Notice, that this will result in n_configs + 1 for the initial design. Respecting n_trials,     this will result in one fewer evaluated configuration in the optimization. instances : list[str] | None, defaults to None     Names of the instances to use. If None, no instances are used.     Instances could be dataset names, seeds, subsets, etc. instance_features : dict[str, list[float]] | None, defaults to None     Instances can be associated with features. For example, meta data of the dataset (mean, var, ...) can be     incorporated which are then further used to expand the training data of the surrogate model. min_budget : float | int | None, defaults to None     The minimum budget (epochs, subset size, number of instances, ...) that is used for the optimization.     Use this argument if you use multi-fidelity or instance optimization. max_budget : float | int | None, defaults to None     The maximum budget (epochs, subset size, number of instances, ...) that is used for the optimization.     Use this argument if you use multi-fidelity or instance optimization. seed : int, defaults to 0     The seed is used to make results reproducible. If seed is -1, SMAC will generate a random seed. n_workers : int, defaults to 1     The number of workers to use for parallelization. If <code>n_workers</code> is greather than 1, SMAC will use     Dask to parallelize the optimization.</p>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the SMAC run.</p>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.meta--note","title":"Note","text":"<p>Meta data are set when the facade is initialized.</p>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Checks whether the config is valid.</p> Source code in <code>smac/scenario.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Checks whether the config is valid.\"\"\"\n    # Use random seed if seed is -1\n    if self.seed == -1:\n        seed = random.randint(0, 999999)\n        object.__setattr__(self, \"seed\", seed)\n\n    # Transform instances to string if they are not\n    if self.instances is not None:\n        instances = [str(instance) for instance in self.instances]\n        object.__setattr__(self, \"instances\", instances)\n\n    # Transform instance features to string if they are not\n    if self.instance_features is not None:\n        instance_features = {str(instance): features for instance, features in self.instance_features.items()}\n        object.__setattr__(self, \"instance_features\", instance_features)\n\n    # Change directory wrt name and seed\n    self._change_output_directory()\n\n    # Set empty meta\n    object.__setattr__(self, \"_meta\", {})\n</code></pre>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.count_instance_features","title":"count_instance_features","text":"<pre><code>count_instance_features() -&gt; int\n</code></pre> <p>Counts the number of instance features.</p> Source code in <code>smac/scenario.py</code> <pre><code>def count_instance_features(self) -&gt; int:\n    \"\"\"Counts the number of instance features.\"\"\"\n    # Check whether key of instance features exist\n    n_features = 0\n    if self.instance_features is not None:\n        for k, v in self.instance_features.items():\n            if self.instances is None or k not in self.instances:\n                raise RuntimeError(f\"Instance {k} is not specified in instances.\")\n\n            if n_features == 0:\n                n_features = len(v)\n            else:\n                if len(v) != n_features:\n                    raise RuntimeError(\"Instances must have the same number of features.\")\n\n    return n_features\n</code></pre>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.count_objectives","title":"count_objectives","text":"<pre><code>count_objectives() -&gt; int\n</code></pre> <p>Counts the number of objectives.</p> Source code in <code>smac/scenario.py</code> <pre><code>def count_objectives(self) -&gt; int:\n    \"\"\"Counts the number of objectives.\"\"\"\n    if isinstance(self.objectives, list):\n        return len(self.objectives)\n\n    return 1\n</code></pre>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.load","title":"load  <code>staticmethod</code>","text":"<pre><code>load(path: Path) -&gt; Scenario\n</code></pre> <p>Loads a scenario and the configuration space from a file.</p> Source code in <code>smac/scenario.py</code> <pre><code>@staticmethod\ndef load(path: Path) -&gt; Scenario:\n    \"\"\"Loads a scenario and the configuration space from a file.\"\"\"\n    filename = path / \"scenario.json\"\n    with open(filename, \"r\") as fh:\n        data = json.load(fh)\n\n    # Convert `output_directory` to path object again\n    data[\"output_directory\"] = Path(data[\"output_directory\"])\n    meta = data[\"_meta\"]\n    del data[\"_meta\"]\n\n    # Read configspace\n    configspace_filename = path / \"configspace.json\"\n    configspace = ConfigurationSpace.from_json(configspace_filename)\n\n    data[\"configspace\"] = configspace\n\n    scenario = Scenario(**data)\n    scenario._set_meta(meta)\n\n    return scenario\n</code></pre>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.make_serializable","title":"make_serializable  <code>staticmethod</code>","text":"<pre><code>make_serializable(scenario: Scenario) -&gt; dict[str, Any]\n</code></pre> <p>Makes the scenario serializable.</p> Source code in <code>smac/scenario.py</code> <pre><code>@staticmethod\ndef make_serializable(scenario: Scenario) -&gt; dict[str, Any]:\n    \"\"\"Makes the scenario serializable.\"\"\"\n    s = copy.deepcopy(scenario.__dict__)\n    del s[\"configspace\"]\n    s[\"output_directory\"] = str(s[\"output_directory\"])\n\n    return json.loads(json.dumps(s))\n</code></pre>"},{"location":"api/smac/scenario/#smac.scenario.Scenario.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Saves internal variables and the configuration space to a file.</p> Source code in <code>smac/scenario.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Saves internal variables and the configuration space to a file.\"\"\"\n    if self.meta == {}:\n        logger.warning(\"Scenario will saved without meta data. Please call the facade first to set meta data.\")\n\n    if self.name is None:\n        raise RuntimeError(\n            \"Please specify meta data for generating a name. Alternatively, you can specify a name manually.\"\n        )\n\n    self.output_directory.mkdir(parents=True, exist_ok=True)\n\n    data = {}\n    for k, v in self.__dict__.items():\n        if k in [\"configspace\", \"output_directory\"]:\n            continue\n\n        data[k] = v\n\n    # Convert `output_directory`\n    data[\"output_directory\"] = str(self.output_directory)\n\n    # Save everything\n    filename = self.output_directory / \"scenario.json\"\n    with open(filename, \"w\") as fh:\n        json.dump(data, fh, indent=4, cls=NumpyEncoder)\n\n    # Save configspace on its own\n    configspace_filename = self.output_directory / \"configspace.json\"\n    self.configspace.to_json(configspace_filename)\n</code></pre>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/","title":"Abstract acquisition function","text":""},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function","title":"smac.acquisition.function.abstract_acquisition_function","text":""},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction","title":"AbstractAcquisitionFunction","text":"<pre><code>AbstractAcquisitionFunction()\n</code></pre> <p>Abstract base class for acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._model: AbstractModel | None = None\n</code></pre>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: AbstractModel | None\n</code></pre> <p>Return the used surrogate model in the acquisition function.</p>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the full name of the acquisition function.</p>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/abstract_acquisition_function/#smac.acquisition.function.abstract_acquisition_function.AbstractAcquisitionFunction.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/function/confidence_bound/","title":"Confidence bound","text":""},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound","title":"smac.acquisition.function.confidence_bound","text":""},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB","title":"LCB","text":"<pre><code>LCB(beta: float = 1.0)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionFunction</code></p> <p>Computes the lower confidence bound for a given x over the best so far value as acquisition value.</p> <p>:math:<code>LCB(X) = \\mu(\\mathbf{X}) - \\sqrt(\\beta_t)\\sigma(\\mathbf{X})</code> [SKKS10]</p> <p>with</p> <p>:math:<code>\\beta_t = 2 \\log( |D| t^2 / \\beta)</code></p> <p>:math:<code>\\text{Input space} D</code> :math:<code>\\text{Number of input dimensions} |D|</code> :math:<code>\\text{Number of data points} t</code> :math:<code>\\text{Exploration/exploitation tradeoff} \\beta</code></p> <p>Returns -LCB(X) as the acquisition_function optimizer maximizes the acquisition value.</p>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB--parameters","title":"Parameters","text":"<p>beta : float, defaults to 1.0     Controls the balance between exploration and exploitation of the acquisition function.</p>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB--attributes","title":"Attributes","text":"<p>_beta : float     Exploration-exploitation trade-off parameter. _num_data : int     Number of data points seen so far.</p> Source code in <code>smac/acquisition/function/confidence_bound.py</code> <pre><code>def __init__(self, beta: float = 1.0) -&gt; None:\n    super(LCB, self).__init__()\n    self._beta: float = beta\n    self._num_data: int | None = None\n</code></pre>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: AbstractModel | None\n</code></pre> <p>Return the used surrogate model in the acquisition function.</p>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/confidence_bound/#smac.acquisition.function.confidence_bound.LCB.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/function/expected_improvement/","title":"Expected improvement","text":""},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement","title":"smac.acquisition.function.expected_improvement","text":""},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI","title":"EI","text":"<pre><code>EI(xi: float = 0.0, log: bool = False)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionFunction</code></p> <p>The Expected Improvement (EI) criterion is used to decide where to evaluate a function f(x) next. The goal is to balance exploration and exploitation. Expected Improvement (with or without function values in log space) acquisition function</p> <p>:math:<code>EI(X) := \\mathbb{E}\\left[ \\max\\{0, f(\\mathbf{X^+}) - f_{t+1}(\\mathbf{X}) - \\xi \\} \\right]</code>, with :math:<code>f(X^+)</code> as the best location.</p> <p>Reference for EI: Jones, D.R. and Schonlau, M. and Welch, W.J. (1998). Efficient Global Optimization of Expensive Black-Box Functions. Journal of Global Optimization 13, 455\u2013492</p> <p>Reference for logEI: Hutter, F. and Hoos, H. and Leyton-Brown, K. and Murphy, K. (2009). An experimental investigation of model-based parameter optimisation: SPO and beyond. In: Conference on Genetic and Evolutionary Computation</p> <p>The logEI implemententation is based on the derivation of the orginal equation by: Watanabe, S. (2024). Derivation of Closed Form of Expected Improvement for Gaussian Process Trained on Log-Transformed Objective. arxiv.org/abs/2411.18095</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI--parameters","title":"Parameters","text":"<p>xi : float, defaults to 0.0     Controls the balance between exploration and exploitation of the     acquisition function. log : bool, defaults to False     Whether the function values are in log-space.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI--attributes","title":"Attributes","text":"<p>_xi : float     Exploration-exloitation trade-off parameter. _log: bool     Function values in log-space or not. _eta : float     Current incumbent function value (best value observed so far).</p> Source code in <code>smac/acquisition/function/expected_improvement.py</code> <pre><code>def __init__(\n    self,\n    xi: float = 0.0,\n    log: bool = False,\n) -&gt; None:\n    super(EI, self).__init__()\n\n    self._xi: float = xi\n    self._log: bool = log\n    self._eta: float | None = None\n</code></pre>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: AbstractModel | None\n</code></pre> <p>Return the used surrogate model in the acquisition function.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EI.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS","title":"EIPS","text":"<pre><code>EIPS(xi: float = 0.0)\n</code></pre> <p>               Bases: <code>EI</code></p> <p>Expected Improvement per Second acquisition function</p> <p>:math:<code>EI(X) := \\frac{\\mathbb{E}\\left[\\max\\{0,f(\\mathbf{X^+})-f_{t+1}(\\mathbf{X})-\\xi\\right]\\}]}{np.log(r(x))}</code>, with :math:<code>f(X^+)</code> as the best location and :math:<code>r(x)</code> as runtime.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS--parameters","title":"Parameters","text":"<p>xi : float, defaults to 0.0     Controls the balance between exploration and exploitation of the acquisition function.</p> Source code in <code>smac/acquisition/function/expected_improvement.py</code> <pre><code>def __init__(self, xi: float = 0.0) -&gt; None:\n    super(EIPS, self).__init__(xi=xi)\n</code></pre>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: AbstractModel | None\n</code></pre> <p>Return the used surrogate model in the acquisition function.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/expected_improvement/#smac.acquisition.function.expected_improvement.EIPS.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/","title":"Integrated acquisition function","text":""},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function","title":"smac.acquisition.function.integrated_acquisition_function","text":""},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction","title":"IntegratedAcquisitionFunction","text":"<pre><code>IntegratedAcquisitionFunction(\n    acquisition_function: AbstractAcquisitionFunction,\n)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionFunction</code></p> <p>Compute the integrated acquisition function by marginalizing over model hyperparameters</p> <p>See \"Practical Bayesian Optimization of Machine Learning Algorithms\" by Jasper Snoek et al. (papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) for further details.</p>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction--parameters","title":"Parameters","text":"<p>acquisition_function : AbstractAcquisitionFunction     Acquisition function to be integrated.</p>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction--attributes","title":"Attributes","text":"<p>_acquisition_function : AbstractAcquisitionFunction     Acquisition function to be integrated. _functions: list[AbstractAcquisitionFunction]     Holds n (n = number of models) copies of the acquisition function. _eta : float     Current incumbent function value.</p> Source code in <code>smac/acquisition/function/integrated_acquisition_function.py</code> <pre><code>def __init__(self, acquisition_function: AbstractAcquisitionFunction) -&gt; None:\n    super().__init__()\n    self._acquisition_function: AbstractAcquisitionFunction = acquisition_function\n    self._functions: list[AbstractAcquisitionFunction] = []\n    self._eta: float | None = None\n</code></pre>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: AbstractModel | None\n</code></pre> <p>Return the used surrogate model in the acquisition function.</p>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/integrated_acquisition_function/#smac.acquisition.function.integrated_acquisition_function.IntegratedAcquisitionFunction.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/function/prior_acquisition_function/","title":"Prior acquisition function","text":""},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function","title":"smac.acquisition.function.prior_acquisition_function","text":""},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function.PriorAcquisitionFunction","title":"PriorAcquisitionFunction","text":"<pre><code>PriorAcquisitionFunction(\n    acquisition_function: AbstractAcquisitionFunction,\n    decay_beta: float,\n    prior_floor: float = 1e-12,\n    discretize: bool = False,\n    discrete_bins_factor: float = 10.0,\n)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionFunction</code></p> <p>Weight the acquisition function with a user-defined prior over the optimum.</p> <p>See \"piBO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization\" by Carl Hvarfner et al. [HSSL22] for further details.</p>"},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function.PriorAcquisitionFunction--parameters","title":"Parameters","text":"<p>decay_beta: float     Decay factor on the user prior. A solid default value for decay_beta (empirically founded) is     <code>scenario.n_trials</code> / 10. prior_floor : float, defaults to 1e-12     Lowest possible value of the prior to ensure non-negativity for all values in the search space. discretize : bool, defaults to False     Whether to discretize (bin) the densities for continous parameters. Triggered for Random Forest models and     continous hyperparameters to avoid a pathological case where all Random Forest randomness is removed     (RF surrogates require piecewise constant acquisition functions to be well-behaved). discrete_bins_factor : float, defaults to 10.0     If discretizing, the multiple on the number of allowed bins for each parameter.</p> Source code in <code>smac/acquisition/function/prior_acquisition_function.py</code> <pre><code>def __init__(\n    self,\n    acquisition_function: AbstractAcquisitionFunction,\n    decay_beta: float,\n    prior_floor: float = 1e-12,\n    discretize: bool = False,\n    discrete_bins_factor: float = 10.0,\n):\n    super().__init__()\n    self._acquisition_function: AbstractAcquisitionFunction = acquisition_function\n    self._functions: list[AbstractAcquisitionFunction] = []\n    self._eta: float | None = None\n\n    self._hyperparameters: dict[Any, Configuration] | None = None\n    self._decay_beta = decay_beta\n    self._prior_floor = prior_floor\n    self._discretize = discretize\n    self._discrete_bins_factor = discrete_bins_factor\n\n    # check if the acquisition function is LCB or TS - then the acquisition function values\n    # need to be rescaled to assure positiveness &amp; correct magnitude\n    if isinstance(self._acquisition_function, IntegratedAcquisitionFunction):\n        acquisition_type = self._acquisition_function._acquisition_function\n    else:\n        acquisition_type = self._acquisition_function\n\n    self._rescale = isinstance(acquisition_type, (LCB, TS))\n\n    # Variables needed to adapt the weighting of the prior\n    self._initial_design_size = None\n    self._iteration_number = 0\n</code></pre>"},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function.PriorAcquisitionFunction.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function.PriorAcquisitionFunction.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function.PriorAcquisitionFunction.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function.PriorAcquisitionFunction.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/prior_acquisition_function/#smac.acquisition.function.prior_acquisition_function.PriorAcquisitionFunction.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/function/probability_improvement/","title":"Probability improvement","text":""},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement","title":"smac.acquisition.function.probability_improvement","text":""},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI","title":"PI","text":"<pre><code>PI(xi: float = 0.0)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionFunction</code></p> <p>Probability of Improvement</p> <p>:math:<code>P(f_{t+1}(\\mathbf{X})\\geq f(\\mathbf{X^+}))</code> :math:<code>:= \\Phi(\\\\frac{ \\mu(\\mathbf{X})-f(\\mathbf{X^+}) } { \\sigma(\\mathbf{X}) })</code> with :math:<code>f(X^+)</code> as the incumbent and :math:<code>\\Phi</code> the cdf of the standard normal.</p>"},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI--parameters","title":"Parameters","text":"<p>xi : float, defaults to 0.0     Controls the balance between exploration and exploitation of the acquisition function.</p> Source code in <code>smac/acquisition/function/probability_improvement.py</code> <pre><code>def __init__(self, xi: float = 0.0):\n    super(PI, self).__init__()\n    self._xi: float = xi\n    self._eta: float | None = None\n</code></pre>"},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: AbstractModel | None\n</code></pre> <p>Return the used surrogate model in the acquisition function.</p>"},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/probability_improvement/#smac.acquisition.function.probability_improvement.PI.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/function/thompson/","title":"Thompson","text":""},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson","title":"smac.acquisition.function.thompson","text":""},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS","title":"TS","text":"<pre><code>TS()\n</code></pre> <p>               Bases: <code>AbstractAcquisitionFunction</code></p> <p>Thompson Sampling</p>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS--warning","title":"Warning","text":"<p>Thompson Sampling can only be used together with <code>RandomSearch</code>. Please do not use <code>LocalAndSortedRandomSearch</code> to optimize the TS acquisition function!</p> <p>:math:`TS(X) ~ \\mathcal{N}(\\mu(\\mathbf{X}),\\sigma(\\mathbf{X}))' Returns -TS(X) as the acquisition_function optimizer maximizes the acquisition value.</p>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS--parameters","title":"Parameters","text":"<p>xi : float, defaults to 0.0     TS does not require xi here, we only wants to make it consistent with other acquisition functions.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._model: AbstractModel | None = None\n</code></pre>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: AbstractModel | None\n</code></pre> <p>Return the used surrogate model in the acquisition function.</p>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS.__call__","title":"__call__","text":"<pre><code>__call__(configurations: list[Configuration]) -&gt; ndarray\n</code></pre> <p>Compute the acquisition value for a given configuration.</p>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS.__call__--parameters","title":"Parameters","text":"<p>configurations : list[Configuration]     The configurations where the acquisition function should be evaluated.</p>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS.__call__--returns","title":"Returns","text":"<p>np.ndarray [N, 1]     Acquisition values for X</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def __call__(self, configurations: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Compute the acquisition value for a given configuration.\n\n    Parameters\n    ----------\n    configurations : list[Configuration]\n        The configurations where the acquisition function should be evaluated.\n\n    Returns\n    -------\n    np.ndarray [N, 1]\n        Acquisition values for X\n    \"\"\"\n    X = convert_configurations_to_array(configurations)\n    if len(X.shape) == 1:\n        X = X[np.newaxis, :]\n\n    acq = self._compute(X)\n    if np.any(np.isnan(acq)):\n        idx = np.where(np.isnan(acq))[0]\n        acq[idx, :] = -np.finfo(float).max\n\n    return acq\n</code></pre>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS.update","title":"update","text":"<pre><code>update(model: AbstractModel, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the acquisition function attributes required for calculation.</p> <p>This method will be called after fitting the model, but before maximizing the acquisition function. As an examples, EI uses it to update the current fmin. The default implementation only updates the attributes of the acquisition function which are already present.</p> <p>Calls <code>_update</code> to update the acquisition function attributes.</p>"},{"location":"api/smac/acquisition/function/thompson/#smac.acquisition.function.thompson.TS.update--parameters","title":"Parameters","text":"<p>model : AbstractModel     The model which was used to fit the data. kwargs : Any     Additional arguments to update the specific acquisition function.</p> Source code in <code>smac/acquisition/function/abstract_acquisition_function.py</code> <pre><code>def update(self, model: AbstractModel, **kwargs: Any) -&gt; None:\n    \"\"\"Update the acquisition function attributes required for calculation.\n\n    This method will be called after fitting the model, but before maximizing the acquisition\n    function. As an examples, EI uses it to update the current fmin. The default implementation only updates the\n    attributes of the acquisition function which are already present.\n\n    Calls `_update` to update the acquisition function attributes.\n\n    Parameters\n    ----------\n    model : AbstractModel\n        The model which was used to fit the data.\n    kwargs : Any\n        Additional arguments to update the specific acquisition function.\n    \"\"\"\n    self.model = model\n    self._update(**kwargs)\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/","title":"Abstract acquisition maximizer","text":""},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer","title":"smac.acquisition.maximizer.abstract_acquisition_maximizer","text":""},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer.AbstractAcquisitionMaximizer","title":"AbstractAcquisitionMaximizer","text":"<pre><code>AbstractAcquisitionMaximizer(\n    configspace: ConfigurationSpace,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    challengers: int = 5000,\n    seed: int = 0,\n)\n</code></pre> <p>Abstract class for the acquisition maximization.</p> <p>In order to use this class it has to be subclassed and the method <code>_maximize</code> must be implemented.</p>"},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer.AbstractAcquisitionMaximizer--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace acquisition_function : AbstractAcquisitionFunction challengers : int, defaults to 5000 Number of configurations sampled during the optimization process, details depend on the used maximizer. Also, the number of configurations that is returned by calling <code>maximize</code>. seed : int, defaults to 0</p> Source code in <code>smac/acquisition/maximizer/abstract_acquisition_maximizer.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    challengers: int = 5000,\n    seed: int = 0,\n):\n    self._configspace = configspace\n    self._acquisition_function = acquisition_function\n    self._challengers = challengers\n    self._seed = seed\n    self._rng = np.random.RandomState(seed=seed)\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer.AbstractAcquisitionMaximizer.acquisition_function","title":"acquisition_function  <code>property</code> <code>writable</code>","text":"<pre><code>acquisition_function: AbstractAcquisitionFunction | None\n</code></pre> <p>The acquisition function used for maximization.</p>"},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer.AbstractAcquisitionMaximizer.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Return the meta-data of the created object.</p>"},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer.AbstractAcquisitionMaximizer.maximize","title":"maximize","text":"<pre><code>maximize(\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]\n</code></pre> <p>Maximize acquisition function using <code>_maximize</code>, implemented by a subclass.</p>"},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer.AbstractAcquisitionMaximizer.maximize--parameters","title":"Parameters","text":"<p>previous_configs: list[Configuration]     Previous evaluated configurations. n_points: int, defaults to None     Number of points to be sampled &amp; number of configurations to be returned. If <code>n_points</code> is not specified,     <code>self._challengers</code> is used. Semantics depend on concrete implementation. random_design: AbstractRandomDesign, defaults to None     Part of the returned ChallengerList such that we can interleave random configurations     by a scheme defined by the random design. The method <code>random_design.next_iteration()</code>     is called at the end of this function.</p>"},{"location":"api/smac/acquisition/maximizer/abstract_acquisition_maximizer/#smac.acquisition.maximizer.abstract_acquisition_maximizer.AbstractAcquisitionMaximizer.maximize--returns","title":"Returns","text":"<p>challengers : Iterator[Configuration]     An iterable consisting of configurations.</p> Source code in <code>smac/acquisition/maximizer/abstract_acquisition_maximizer.py</code> <pre><code>def maximize(\n    self,\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]:\n    \"\"\"Maximize acquisition function using `_maximize`, implemented by a subclass.\n\n    Parameters\n    ----------\n    previous_configs: list[Configuration]\n        Previous evaluated configurations.\n    n_points: int, defaults to None\n        Number of points to be sampled &amp; number of configurations to be returned. If `n_points` is not specified,\n        `self._challengers` is used. Semantics depend on concrete implementation.\n    random_design: AbstractRandomDesign, defaults to None\n        Part of the returned ChallengerList such that we can interleave random configurations\n        by a scheme defined by the random design. The method `random_design.next_iteration()`\n        is called at the end of this function.\n\n    Returns\n    -------\n    challengers : Iterator[Configuration]\n        An iterable consisting of configurations.\n    \"\"\"\n    if n_points is None:\n        n_points = self._challengers\n\n    def next_configs_by_acquisition_value() -&gt; list[Configuration]:\n        assert n_points is not None\n        # since maximize returns a tuple of acquisition value and configuration,\n        # and we only need the configuration, we return the second element of the tuple\n        # for each element in the list\n        return [t[1] for t in self._maximize(previous_configs, n_points)]\n\n    challengers = ChallengerList(\n        self._configspace,\n        next_configs_by_acquisition_value,\n        random_design,\n    )\n\n    if random_design is not None:\n        random_design.next_iteration()\n\n    return challengers\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/","title":"Differential evolution","text":""},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution","title":"smac.acquisition.maximizer.differential_evolution","text":""},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.DifferentialEvolution","title":"DifferentialEvolution","text":"<pre><code>DifferentialEvolution(\n    configspace: ConfigurationSpace,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    max_iter: int = 1000,\n    challengers: int = 50000,\n    strategy: str = \"best1bin\",\n    polish: bool = True,\n    mutation: tuple[float, float] = (0.5, 1.0),\n    recombination: float = 0.7,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionMaximizer</code></p> <p>Get candidate solutions via <code>DifferentialEvolutionSolvers</code> from scipy.</p> <p>According to scipy 1.9.2 documentation:</p> <p>'Finds the global minimum of a multivariate function. Differential Evolution is stochastic in nature (does not use gradient methods) to find the minimum, and can search large areas of candidate space, but often requires larger numbers of function evaluations than conventional gradient-based techniques. The algorithm is due to Storn and Price [1].'</p> <p>[1] Storn, R and Price, K, Differential Evolution - a Simple and Efficient Heuristic for Global  Optimization over Continuous Spaces, Journal of Global Optimization, 1997, 11, 341 - 359.</p>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.DifferentialEvolution--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace acquisition_function : AbstractAcquisitionFunction challengers : int, defaults to 50000     Number of challengers. max_iter: int | None, defaults to None     Maximum number of iterations that the DE will perform. strategy: str, defaults to \"best1bin\"     The strategy to use for the DE. polish: bool, defaults to True     Whether to polish the final solution using L-BFGS-B. mutation: tuple[float, float], defaults to (0.5, 1.0)     The mutation constant. recombination: float, defaults to 0.7     The recombination constant. seed : int, defaults to 0</p> Source code in <code>smac/acquisition/maximizer/differential_evolution.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    max_iter: int = 1000,\n    challengers: int = 50000,\n    strategy: str = \"best1bin\",\n    polish: bool = True,\n    mutation: tuple[float, float] = (0.5, 1.0),\n    recombination: float = 0.7,\n    seed: int = 0,\n):\n    super().__init__(configspace, acquisition_function, challengers, seed)\n    # raise NotImplementedError(\"DifferentialEvolution is not yet implemented.\")\n    self.max_iter = max_iter\n    self.strategy = strategy\n    self.polish = polish\n    self.mutation = mutation\n    self.recombination = recombination\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.DifferentialEvolution.acquisition_function","title":"acquisition_function  <code>property</code> <code>writable</code>","text":"<pre><code>acquisition_function: AbstractAcquisitionFunction | None\n</code></pre> <p>The acquisition function used for maximization.</p>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.DifferentialEvolution.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Return the meta-data of the created object.</p>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.DifferentialEvolution.maximize","title":"maximize","text":"<pre><code>maximize(\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]\n</code></pre> <p>Maximize acquisition function using <code>_maximize</code>, implemented by a subclass.</p>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.DifferentialEvolution.maximize--parameters","title":"Parameters","text":"<p>previous_configs: list[Configuration]     Previous evaluated configurations. n_points: int, defaults to None     Number of points to be sampled &amp; number of configurations to be returned. If <code>n_points</code> is not specified,     <code>self._challengers</code> is used. Semantics depend on concrete implementation. random_design: AbstractRandomDesign, defaults to None     Part of the returned ChallengerList such that we can interleave random configurations     by a scheme defined by the random design. The method <code>random_design.next_iteration()</code>     is called at the end of this function.</p>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.DifferentialEvolution.maximize--returns","title":"Returns","text":"<p>challengers : Iterator[Configuration]     An iterable consisting of configurations.</p> Source code in <code>smac/acquisition/maximizer/abstract_acquisition_maximizer.py</code> <pre><code>def maximize(\n    self,\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]:\n    \"\"\"Maximize acquisition function using `_maximize`, implemented by a subclass.\n\n    Parameters\n    ----------\n    previous_configs: list[Configuration]\n        Previous evaluated configurations.\n    n_points: int, defaults to None\n        Number of points to be sampled &amp; number of configurations to be returned. If `n_points` is not specified,\n        `self._challengers` is used. Semantics depend on concrete implementation.\n    random_design: AbstractRandomDesign, defaults to None\n        Part of the returned ChallengerList such that we can interleave random configurations\n        by a scheme defined by the random design. The method `random_design.next_iteration()`\n        is called at the end of this function.\n\n    Returns\n    -------\n    challengers : Iterator[Configuration]\n        An iterable consisting of configurations.\n    \"\"\"\n    if n_points is None:\n        n_points = self._challengers\n\n    def next_configs_by_acquisition_value() -&gt; list[Configuration]:\n        assert n_points is not None\n        # since maximize returns a tuple of acquisition value and configuration,\n        # and we only need the configuration, we return the second element of the tuple\n        # for each element in the list\n        return [t[1] for t in self._maximize(previous_configs, n_points)]\n\n    challengers = ChallengerList(\n        self._configspace,\n        next_configs_by_acquisition_value,\n        random_design,\n    )\n\n    if random_design is not None:\n        random_design.next_iteration()\n\n    return challengers\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.check_kwarg","title":"check_kwarg","text":"<pre><code>check_kwarg(cls: type, kwarg_name: str) -&gt; bool\n</code></pre> <p>Checks if a given class accepts a specific keyword argument in its init method.</p>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.check_kwarg--parameters","title":"Parameters","text":"<pre><code>cls (type): The class to inspect.\nkwarg_name (str): The name of the keyword argument to check.\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/differential_evolution/#smac.acquisition.maximizer.differential_evolution.check_kwarg--returns","title":"Returns","text":"<pre><code>bool: True if the class's __init__ method accepts the keyword argument,\n      otherwise False.\n</code></pre> Source code in <code>smac/acquisition/maximizer/differential_evolution.py</code> <pre><code>def check_kwarg(cls: type, kwarg_name: str) -&gt; bool:\n    \"\"\"\n    Checks if a given class accepts a specific keyword argument in its __init__ method.\n\n    Parameters\n    ----------\n        cls (type): The class to inspect.\n        kwarg_name (str): The name of the keyword argument to check.\n\n    Returns\n    -------\n        bool: True if the class's __init__ method accepts the keyword argument,\n              otherwise False.\n    \"\"\"\n    # Get the signature of the class's __init__ method\n    init_signature = inspect.signature(cls.__init__)  # type: ignore[misc]\n\n    # Check if the kwarg_name is present in the signature as a parameter\n    for param in init_signature.parameters.values():\n        if param.name == kwarg_name and param.default != inspect.Parameter.empty:\n            return True  # It accepts the kwarg\n    return False  # It does not accept the kwarg\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/helpers/","title":"Helpers","text":""},{"location":"api/smac/acquisition/maximizer/helpers/#smac.acquisition.maximizer.helpers","title":"smac.acquisition.maximizer.helpers","text":""},{"location":"api/smac/acquisition/maximizer/helpers/#smac.acquisition.maximizer.helpers.ChallengerList","title":"ChallengerList","text":"<pre><code>ChallengerList(\n    configspace: ConfigurationSpace,\n    challenger_callback: Callable,\n    random_design: (\n        AbstractRandomDesign | None\n    ) = ProbabilityRandomDesign(\n        seed=0, probability=0.08447232371720552\n    ),\n)\n</code></pre> <p>               Bases: <code>Iterator</code></p> <p>Helper class to interleave random configurations in a list of challengers.</p> <p>Provides an iterator which returns a random configuration in each second iteration. Reduces time necessary to generate a list of new challengers as one does not need to sample several hundreds of random configurations in each iteration which are never looked at.</p>"},{"location":"api/smac/acquisition/maximizer/helpers/#smac.acquisition.maximizer.helpers.ChallengerList--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace challenger_callback : Callable     Callback function which returns a list of challengers (without interleaved random configurations), must a be a     python closure. random_design : AbstractRandomDesign | None, defaults to ModulusRandomDesign(modulus=2.0)     Which random design should be used.</p> Source code in <code>smac/acquisition/maximizer/helpers.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    challenger_callback: Callable,\n    random_design: AbstractRandomDesign | None = ProbabilityRandomDesign(seed=0, probability=0.08447232371720552),\n):\n    self._challengers_callback = challenger_callback\n    self._challengers: list[Configuration] | None = None\n    self._configspace = configspace\n    self._index = 0\n    self._iteration = 1  # 1-based to prevent from starting with a random configuration\n    self._random_design = random_design\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/local_and_random_search/","title":"Local and random search","text":""},{"location":"api/smac/acquisition/maximizer/local_and_random_search/#smac.acquisition.maximizer.local_and_random_search","title":"smac.acquisition.maximizer.local_and_random_search","text":""},{"location":"api/smac/acquisition/maximizer/local_and_random_search/#smac.acquisition.maximizer.local_and_random_search.LocalAndSortedRandomSearch","title":"LocalAndSortedRandomSearch","text":"<pre><code>LocalAndSortedRandomSearch(\n    configspace: ConfigurationSpace,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    challengers: int = 5000,\n    max_steps: int | None = None,\n    n_steps_plateau_walk: int = 10,\n    local_search_iterations: int = 10,\n    seed: int = 0,\n    uniform_configspace: ConfigurationSpace | None = None,\n    prior_sampling_fraction: float | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionMaximizer</code></p> <p>Implement SMAC's default acquisition function optimization.</p> <p>This optimizer performs local search from the previous best points according to the acquisition function, uses the acquisition function to sort randomly sampled configurations. Random configurations are interleaved by the main SMAC code.</p> <p>The Random configurations are interleaved to circumvent issues from a constant prediction from the Random Forest model at the beginning of the optimization process.</p>"},{"location":"api/smac/acquisition/maximizer/local_and_random_search/#smac.acquisition.maximizer.local_and_random_search.LocalAndSortedRandomSearch--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace uniform_configspace : ConfigurationSpace     A version of the user-defined ConfigurationSpace where all parameters are uniform (or have their weights removed     in the case of a categorical hyperparameter). Can optionally be given and sampling ratios be defined via the     <code>prior_sampling_fraction</code> parameter. acquisition_function : AbstractAcquisitionFunction | None, defaults to None challengers : int, defaults to 5000     Number of challengers. max_steps: int | None, defaults to None     [LocalSearch] Maximum number of steps that the local search will perform. n_steps_plateau_walk: int, defaults to 10     [LocalSearch] number of steps during a plateau walk before local search terminates. local_search_iterations: int, defauts to 10     [Local Search] number of local search iterations. prior_sampling_fraction: float, defaults to 0.5     The ratio of random samples that are taken from the user-defined ConfigurationSpace, as opposed to the uniform     version (needs <code>uniform_configspace</code>to be defined). seed : int, defaults to 0</p> Source code in <code>smac/acquisition/maximizer/local_and_random_search.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    challengers: int = 5000,\n    max_steps: int | None = None,\n    n_steps_plateau_walk: int = 10,\n    local_search_iterations: int = 10,\n    seed: int = 0,\n    uniform_configspace: ConfigurationSpace | None = None,\n    prior_sampling_fraction: float | None = None,\n) -&gt; None:\n    super().__init__(\n        configspace,\n        acquisition_function=acquisition_function,\n        challengers=challengers,\n        seed=seed,\n    )\n\n    if uniform_configspace is not None and prior_sampling_fraction is None:\n        prior_sampling_fraction = 0.5\n    if uniform_configspace is None and prior_sampling_fraction is not None:\n        raise ValueError(\"If `prior_sampling_fraction` is given, `uniform_configspace` must be defined.\")\n    if uniform_configspace is not None and prior_sampling_fraction is not None:\n        self._prior_random_search = RandomSearch(\n            acquisition_function=acquisition_function,\n            configspace=configspace,\n            seed=seed,\n        )\n\n        self._uniform_random_search = RandomSearch(\n            acquisition_function=acquisition_function,\n            configspace=uniform_configspace,\n            seed=seed,\n        )\n    else:\n        self._random_search = RandomSearch(\n            configspace=configspace,\n            acquisition_function=acquisition_function,\n            seed=seed,\n        )\n\n    self._local_search = LocalSearch(\n        configspace=configspace,\n        acquisition_function=acquisition_function,\n        max_steps=max_steps,\n        n_steps_plateau_walk=n_steps_plateau_walk,\n        seed=seed,\n    )\n\n    self._local_search_iterations = local_search_iterations\n    self._prior_sampling_fraction = prior_sampling_fraction\n    self._uniform_configspace = uniform_configspace\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/local_and_random_search/#smac.acquisition.maximizer.local_and_random_search.LocalAndSortedRandomSearch.acquisition_function","title":"acquisition_function  <code>property</code> <code>writable</code>","text":"<pre><code>acquisition_function: AbstractAcquisitionFunction | None\n</code></pre> <p>Returns the used acquisition function.</p>"},{"location":"api/smac/acquisition/maximizer/local_and_random_search/#smac.acquisition.maximizer.local_and_random_search.LocalAndSortedRandomSearch.maximize","title":"maximize","text":"<pre><code>maximize(\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]\n</code></pre> <p>Maximize acquisition function using <code>_maximize</code>, implemented by a subclass.</p>"},{"location":"api/smac/acquisition/maximizer/local_and_random_search/#smac.acquisition.maximizer.local_and_random_search.LocalAndSortedRandomSearch.maximize--parameters","title":"Parameters","text":"<p>previous_configs: list[Configuration]     Previous evaluated configurations. n_points: int, defaults to None     Number of points to be sampled &amp; number of configurations to be returned. If <code>n_points</code> is not specified,     <code>self._challengers</code> is used. Semantics depend on concrete implementation. random_design: AbstractRandomDesign, defaults to None     Part of the returned ChallengerList such that we can interleave random configurations     by a scheme defined by the random design. The method <code>random_design.next_iteration()</code>     is called at the end of this function.</p>"},{"location":"api/smac/acquisition/maximizer/local_and_random_search/#smac.acquisition.maximizer.local_and_random_search.LocalAndSortedRandomSearch.maximize--returns","title":"Returns","text":"<p>challengers : Iterator[Configuration]     An iterable consisting of configurations.</p> Source code in <code>smac/acquisition/maximizer/abstract_acquisition_maximizer.py</code> <pre><code>def maximize(\n    self,\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]:\n    \"\"\"Maximize acquisition function using `_maximize`, implemented by a subclass.\n\n    Parameters\n    ----------\n    previous_configs: list[Configuration]\n        Previous evaluated configurations.\n    n_points: int, defaults to None\n        Number of points to be sampled &amp; number of configurations to be returned. If `n_points` is not specified,\n        `self._challengers` is used. Semantics depend on concrete implementation.\n    random_design: AbstractRandomDesign, defaults to None\n        Part of the returned ChallengerList such that we can interleave random configurations\n        by a scheme defined by the random design. The method `random_design.next_iteration()`\n        is called at the end of this function.\n\n    Returns\n    -------\n    challengers : Iterator[Configuration]\n        An iterable consisting of configurations.\n    \"\"\"\n    if n_points is None:\n        n_points = self._challengers\n\n    def next_configs_by_acquisition_value() -&gt; list[Configuration]:\n        assert n_points is not None\n        # since maximize returns a tuple of acquisition value and configuration,\n        # and we only need the configuration, we return the second element of the tuple\n        # for each element in the list\n        return [t[1] for t in self._maximize(previous_configs, n_points)]\n\n    challengers = ChallengerList(\n        self._configspace,\n        next_configs_by_acquisition_value,\n        random_design,\n    )\n\n    if random_design is not None:\n        random_design.next_iteration()\n\n    return challengers\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/local_search/","title":"Local search","text":""},{"location":"api/smac/acquisition/maximizer/local_search/#smac.acquisition.maximizer.local_search","title":"smac.acquisition.maximizer.local_search","text":""},{"location":"api/smac/acquisition/maximizer/local_search/#smac.acquisition.maximizer.local_search.LocalSearch","title":"LocalSearch","text":"<pre><code>LocalSearch(\n    configspace: ConfigurationSpace,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    challengers: int = 5000,\n    max_steps: int | None = None,\n    n_steps_plateau_walk: int = 10,\n    vectorization_min_obtain: int = 2,\n    vectorization_max_obtain: int = 64,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionMaximizer</code></p> <p>Implementation of SMAC's local search.</p>"},{"location":"api/smac/acquisition/maximizer/local_search/#smac.acquisition.maximizer.local_search.LocalSearch--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace acquisition_function : AbstractAcquisitionFunction challengers : int, defaults to 5000     Number of challengers. max_steps: int | None, defaults to None     Maximum number of iterations that the local search will perform. n_steps_plateau_walk: int, defaults to 10     Number of steps during a plateau walk before local search terminates. vectorization_min_obtain : int, defaults to 2     Minimal number of neighbors to obtain at once for each local search for vectorized calls. Can be tuned to     reduce the overhead of SMAC. vectorization_max_obtain : int, defaults to 64     Maximal number of neighbors to obtain at once for each local search for vectorized calls. Can be tuned to     reduce the overhead of SMAC. seed : int, defaults to 0</p> Source code in <code>smac/acquisition/maximizer/local_search.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    challengers: int = 5000,\n    max_steps: int | None = None,\n    n_steps_plateau_walk: int = 10,\n    vectorization_min_obtain: int = 2,\n    vectorization_max_obtain: int = 64,\n    seed: int = 0,\n) -&gt; None:\n    super().__init__(\n        configspace,\n        acquisition_function,\n        challengers=challengers,\n        seed=seed,\n    )\n\n    self._max_steps = max_steps\n    self._n_steps_plateau_walk = n_steps_plateau_walk\n    self._vectorization_min_obtain = vectorization_min_obtain\n    self._vectorization_max_obtain = vectorization_max_obtain\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/local_search/#smac.acquisition.maximizer.local_search.LocalSearch.acquisition_function","title":"acquisition_function  <code>property</code> <code>writable</code>","text":"<pre><code>acquisition_function: AbstractAcquisitionFunction | None\n</code></pre> <p>The acquisition function used for maximization.</p>"},{"location":"api/smac/acquisition/maximizer/local_search/#smac.acquisition.maximizer.local_search.LocalSearch.maximize","title":"maximize","text":"<pre><code>maximize(\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]\n</code></pre> <p>Maximize acquisition function using <code>_maximize</code>, implemented by a subclass.</p>"},{"location":"api/smac/acquisition/maximizer/local_search/#smac.acquisition.maximizer.local_search.LocalSearch.maximize--parameters","title":"Parameters","text":"<p>previous_configs: list[Configuration]     Previous evaluated configurations. n_points: int, defaults to None     Number of points to be sampled &amp; number of configurations to be returned. If <code>n_points</code> is not specified,     <code>self._challengers</code> is used. Semantics depend on concrete implementation. random_design: AbstractRandomDesign, defaults to None     Part of the returned ChallengerList such that we can interleave random configurations     by a scheme defined by the random design. The method <code>random_design.next_iteration()</code>     is called at the end of this function.</p>"},{"location":"api/smac/acquisition/maximizer/local_search/#smac.acquisition.maximizer.local_search.LocalSearch.maximize--returns","title":"Returns","text":"<p>challengers : Iterator[Configuration]     An iterable consisting of configurations.</p> Source code in <code>smac/acquisition/maximizer/abstract_acquisition_maximizer.py</code> <pre><code>def maximize(\n    self,\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]:\n    \"\"\"Maximize acquisition function using `_maximize`, implemented by a subclass.\n\n    Parameters\n    ----------\n    previous_configs: list[Configuration]\n        Previous evaluated configurations.\n    n_points: int, defaults to None\n        Number of points to be sampled &amp; number of configurations to be returned. If `n_points` is not specified,\n        `self._challengers` is used. Semantics depend on concrete implementation.\n    random_design: AbstractRandomDesign, defaults to None\n        Part of the returned ChallengerList such that we can interleave random configurations\n        by a scheme defined by the random design. The method `random_design.next_iteration()`\n        is called at the end of this function.\n\n    Returns\n    -------\n    challengers : Iterator[Configuration]\n        An iterable consisting of configurations.\n    \"\"\"\n    if n_points is None:\n        n_points = self._challengers\n\n    def next_configs_by_acquisition_value() -&gt; list[Configuration]:\n        assert n_points is not None\n        # since maximize returns a tuple of acquisition value and configuration,\n        # and we only need the configuration, we return the second element of the tuple\n        # for each element in the list\n        return [t[1] for t in self._maximize(previous_configs, n_points)]\n\n    challengers = ChallengerList(\n        self._configspace,\n        next_configs_by_acquisition_value,\n        random_design,\n    )\n\n    if random_design is not None:\n        random_design.next_iteration()\n\n    return challengers\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/random_search/","title":"Random search","text":""},{"location":"api/smac/acquisition/maximizer/random_search/#smac.acquisition.maximizer.random_search","title":"smac.acquisition.maximizer.random_search","text":""},{"location":"api/smac/acquisition/maximizer/random_search/#smac.acquisition.maximizer.random_search.RandomSearch","title":"RandomSearch","text":"<pre><code>RandomSearch(\n    configspace: ConfigurationSpace,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    challengers: int = 5000,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractAcquisitionMaximizer</code></p> <p>Get candidate solutions via random sampling of configurations.</p> Source code in <code>smac/acquisition/maximizer/abstract_acquisition_maximizer.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    challengers: int = 5000,\n    seed: int = 0,\n):\n    self._configspace = configspace\n    self._acquisition_function = acquisition_function\n    self._challengers = challengers\n    self._seed = seed\n    self._rng = np.random.RandomState(seed=seed)\n</code></pre>"},{"location":"api/smac/acquisition/maximizer/random_search/#smac.acquisition.maximizer.random_search.RandomSearch.acquisition_function","title":"acquisition_function  <code>property</code> <code>writable</code>","text":"<pre><code>acquisition_function: AbstractAcquisitionFunction | None\n</code></pre> <p>The acquisition function used for maximization.</p>"},{"location":"api/smac/acquisition/maximizer/random_search/#smac.acquisition.maximizer.random_search.RandomSearch.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Return the meta-data of the created object.</p>"},{"location":"api/smac/acquisition/maximizer/random_search/#smac.acquisition.maximizer.random_search.RandomSearch.maximize","title":"maximize","text":"<pre><code>maximize(\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]\n</code></pre> <p>Maximize acquisition function using <code>_maximize</code>, implemented by a subclass.</p>"},{"location":"api/smac/acquisition/maximizer/random_search/#smac.acquisition.maximizer.random_search.RandomSearch.maximize--parameters","title":"Parameters","text":"<p>previous_configs: list[Configuration]     Previous evaluated configurations. n_points: int, defaults to None     Number of points to be sampled &amp; number of configurations to be returned. If <code>n_points</code> is not specified,     <code>self._challengers</code> is used. Semantics depend on concrete implementation. random_design: AbstractRandomDesign, defaults to None     Part of the returned ChallengerList such that we can interleave random configurations     by a scheme defined by the random design. The method <code>random_design.next_iteration()</code>     is called at the end of this function.</p>"},{"location":"api/smac/acquisition/maximizer/random_search/#smac.acquisition.maximizer.random_search.RandomSearch.maximize--returns","title":"Returns","text":"<p>challengers : Iterator[Configuration]     An iterable consisting of configurations.</p> Source code in <code>smac/acquisition/maximizer/abstract_acquisition_maximizer.py</code> <pre><code>def maximize(\n    self,\n    previous_configs: list[Configuration],\n    n_points: int | None = None,\n    random_design: AbstractRandomDesign | None = None,\n) -&gt; Iterator[Configuration]:\n    \"\"\"Maximize acquisition function using `_maximize`, implemented by a subclass.\n\n    Parameters\n    ----------\n    previous_configs: list[Configuration]\n        Previous evaluated configurations.\n    n_points: int, defaults to None\n        Number of points to be sampled &amp; number of configurations to be returned. If `n_points` is not specified,\n        `self._challengers` is used. Semantics depend on concrete implementation.\n    random_design: AbstractRandomDesign, defaults to None\n        Part of the returned ChallengerList such that we can interleave random configurations\n        by a scheme defined by the random design. The method `random_design.next_iteration()`\n        is called at the end of this function.\n\n    Returns\n    -------\n    challengers : Iterator[Configuration]\n        An iterable consisting of configurations.\n    \"\"\"\n    if n_points is None:\n        n_points = self._challengers\n\n    def next_configs_by_acquisition_value() -&gt; list[Configuration]:\n        assert n_points is not None\n        # since maximize returns a tuple of acquisition value and configuration,\n        # and we only need the configuration, we return the second element of the tuple\n        # for each element in the list\n        return [t[1] for t in self._maximize(previous_configs, n_points)]\n\n    challengers = ChallengerList(\n        self._configspace,\n        next_configs_by_acquisition_value,\n        random_design,\n    )\n\n    if random_design is not None:\n        random_design.next_iteration()\n\n    return challengers\n</code></pre>"},{"location":"api/smac/callback/callback/","title":"Callback","text":""},{"location":"api/smac/callback/callback/#smac.callback.callback","title":"smac.callback.callback","text":""},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback","title":"Callback","text":"<pre><code>Callback()\n</code></pre> <p>Callback interface with several methods that are called at different stages of the optimization process.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_ask_end","title":"on_ask_end","text":"<pre><code>on_ask_end(smbo: SMBO, info: TrialInfo) -&gt; None\n</code></pre> <p>Called after the intensifier is asked for the next trial.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_ask_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo) -&gt; None:\n    \"\"\"Called after the intensifier is asked for the next trial.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_ask_start","title":"on_ask_start","text":"<pre><code>on_ask_start(smbo: SMBO) -&gt; None\n</code></pre> <p>Called before the intensifier is asked for the next trial.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_ask_start(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called before the intensifier is asked for the next trial.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_end","title":"on_end","text":"<pre><code>on_end(smbo: SMBO) -&gt; None\n</code></pre> <p>Called after the optimization finished.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_end(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called after the optimization finished.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_iteration_end","title":"on_iteration_end","text":"<pre><code>on_iteration_end(smbo: SMBO) -&gt; None\n</code></pre> <p>Called after an iteration ended.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_iteration_end(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called after an iteration ended.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_iteration_start","title":"on_iteration_start","text":"<pre><code>on_iteration_start(smbo: SMBO) -&gt; None\n</code></pre> <p>Called before the next run is sampled.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_iteration_start(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called before the next run is sampled.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_next_configurations_end","title":"on_next_configurations_end","text":"<pre><code>on_next_configurations_end(\n    config_selector: ConfigSelector, config: Configuration\n) -&gt; None\n</code></pre> <p>Called after the intensification asks for new configurations. Essentially, this callback is called before the surrogate model is trained and before the acquisition function is called.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_next_configurations_end(\n    self, config_selector: smac.main.config_selector.ConfigSelector, config: Configuration\n) -&gt; None:\n    \"\"\"Called after the intensification asks for new configurations. Essentially, this callback is called\n    before the surrogate model is trained and before the acquisition function is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_next_configurations_start","title":"on_next_configurations_start","text":"<pre><code>on_next_configurations_start(\n    config_selector: ConfigSelector,\n) -&gt; None\n</code></pre> <p>Called before the intensification asks for new configurations. Essentially, this callback is called before the surrogate model is trained and before the acquisition function is called.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_next_configurations_start(self, config_selector: smac.main.config_selector.ConfigSelector) -&gt; None:\n    \"\"\"Called before the intensification asks for new configurations. Essentially, this callback is called\n    before the surrogate model is trained and before the acquisition function is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_start","title":"on_start","text":"<pre><code>on_start(smbo: SMBO) -&gt; None\n</code></pre> <p>Called before the optimization starts.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_start(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called before the optimization starts.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_tell_end","title":"on_tell_end","text":"<pre><code>on_tell_end(\n    smbo: SMBO, info: TrialInfo, value: TrialValue\n) -&gt; bool | None\n</code></pre> <p>Called after the stats are updated and the trial is added to the runhistory. Optionally, returns false to gracefully stop the optimization.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n    \"\"\"Called after the stats are updated and the trial is added to the runhistory. Optionally, returns false\n    to gracefully stop the optimization.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/callback/#smac.callback.callback.Callback.on_tell_start","title":"on_tell_start","text":"<pre><code>on_tell_start(\n    smbo: SMBO, info: TrialInfo, value: TrialValue\n) -&gt; bool | None\n</code></pre> <p>Called before the stats are updated and the trial is added to the runhistory. Optionally, returns false to gracefully stop the optimization.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_tell_start(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n    \"\"\"Called before the stats are updated and the trial is added to the runhistory. Optionally, returns false\n    to gracefully stop the optimization.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/","title":"Metadata callback","text":""},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback","title":"smac.callback.metadata_callback","text":""},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback","title":"MetadataCallback","text":"<pre><code>MetadataCallback(**kwargs: str | int | float | dict | list)\n</code></pre> <p>               Bases: <code>Callback</code></p> Source code in <code>smac/callback/metadata_callback.py</code> <pre><code>def __init__(self, **kwargs: str | int | float | dict | list) -&gt; None:\n    # Arguments must be json serializable\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_ask_end","title":"on_ask_end","text":"<pre><code>on_ask_end(smbo: SMBO, info: TrialInfo) -&gt; None\n</code></pre> <p>Called after the intensifier is asked for the next trial.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_ask_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo) -&gt; None:\n    \"\"\"Called after the intensifier is asked for the next trial.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_ask_start","title":"on_ask_start","text":"<pre><code>on_ask_start(smbo: SMBO) -&gt; None\n</code></pre> <p>Called before the intensifier is asked for the next trial.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_ask_start(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called before the intensifier is asked for the next trial.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_end","title":"on_end","text":"<pre><code>on_end(smbo: SMBO) -&gt; None\n</code></pre> <p>Called after the optimization finished.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_end(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called after the optimization finished.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_iteration_end","title":"on_iteration_end","text":"<pre><code>on_iteration_end(smbo: SMBO) -&gt; None\n</code></pre> <p>Called after an iteration ended.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_iteration_end(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called after an iteration ended.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_iteration_start","title":"on_iteration_start","text":"<pre><code>on_iteration_start(smbo: SMBO) -&gt; None\n</code></pre> <p>Called before the next run is sampled.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_iteration_start(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n    \"\"\"Called before the next run is sampled.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_next_configurations_end","title":"on_next_configurations_end","text":"<pre><code>on_next_configurations_end(\n    config_selector: ConfigSelector, config: Configuration\n) -&gt; None\n</code></pre> <p>Called after the intensification asks for new configurations. Essentially, this callback is called before the surrogate model is trained and before the acquisition function is called.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_next_configurations_end(\n    self, config_selector: smac.main.config_selector.ConfigSelector, config: Configuration\n) -&gt; None:\n    \"\"\"Called after the intensification asks for new configurations. Essentially, this callback is called\n    before the surrogate model is trained and before the acquisition function is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_next_configurations_start","title":"on_next_configurations_start","text":"<pre><code>on_next_configurations_start(\n    config_selector: ConfigSelector,\n) -&gt; None\n</code></pre> <p>Called before the intensification asks for new configurations. Essentially, this callback is called before the surrogate model is trained and before the acquisition function is called.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_next_configurations_start(self, config_selector: smac.main.config_selector.ConfigSelector) -&gt; None:\n    \"\"\"Called before the intensification asks for new configurations. Essentially, this callback is called\n    before the surrogate model is trained and before the acquisition function is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_start","title":"on_start","text":"<pre><code>on_start(smbo: SMBO) -&gt; None\n</code></pre> <p>Called before the optimization starts.</p> Source code in <code>smac/callback/metadata_callback.py</code> <pre><code>def on_start(self, smbo: SMBO) -&gt; None:\n    \"\"\"Called before the optimization starts.\"\"\"\n    path = smbo._scenario.output_directory\n    meta_dict = {\n        \"utc_time\": str(datetime.utcnow()),\n        \"os\": platform.platform(),\n        \"smac_version\": getattr(smac, \"version\"),\n    }\n    for key, value in self.kwargs.items():\n        meta_dict[key] = value\n\n    path.mkdir(parents=True, exist_ok=True)\n\n    with open(path / \"metadata.json\", \"w\") as fp:\n        json.dump(meta_dict, fp, indent=2, cls=NumpyEncoder)\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_tell_end","title":"on_tell_end","text":"<pre><code>on_tell_end(\n    smbo: SMBO, info: TrialInfo, value: TrialValue\n) -&gt; bool | None\n</code></pre> <p>Called after the stats are updated and the trial is added to the runhistory. Optionally, returns false to gracefully stop the optimization.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n    \"\"\"Called after the stats are updated and the trial is added to the runhistory. Optionally, returns false\n    to gracefully stop the optimization.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/callback/metadata_callback/#smac.callback.metadata_callback.MetadataCallback.on_tell_start","title":"on_tell_start","text":"<pre><code>on_tell_start(\n    smbo: SMBO, info: TrialInfo, value: TrialValue\n) -&gt; bool | None\n</code></pre> <p>Called before the stats are updated and the trial is added to the runhistory. Optionally, returns false to gracefully stop the optimization.</p> Source code in <code>smac/callback/callback.py</code> <pre><code>def on_tell_start(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n    \"\"\"Called before the stats are updated and the trial is added to the runhistory. Optionally, returns false\n    to gracefully stop the optimization.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/","title":"Abstract facade","text":""},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade","title":"smac.facade.abstract_facade","text":""},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade","title":"AbstractFacade","text":"<pre><code>AbstractFacade(\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    acquisition_maximizer: (\n        AbstractAcquisitionMaximizer | None\n    ) = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    runhistory_encoder: (\n        AbstractRunHistoryEncoder | None\n    ) = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None\n)\n</code></pre> <p>Facade is an abstraction on top of the SMBO backend to organize the components of a Bayesian Optimization loop in a configurable and separable manner to suit the various needs of different (hyperparameter) optimization pipelines.</p> <p>With the exception to scenario and <code>target_function</code>, which are expected of the user, the parameters <code>model</code>, <code>acquisition_function</code>, <code>acquisition_maximizer</code>, <code>initial_design</code>, <code>random_design</code>, <code>intensifier</code>, <code>multi_objective_algorithm</code>, <code>runhistory_encoder</code> can either be explicitly specified in the subclasses' <code>get_*</code> methods (defining a specific BO pipeline) or be instantiated by the user to overwrite a pipeline components explicitly.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade--parameters","title":"Parameters","text":"<p>scenario : Scenario     The scenario object, holding all environmental information. target_function : Callable | str | AbstractRunner     This function is called internally to judge a trial's performance. If a string is passed,     it is assumed to be a script. In this case, <code>TargetFunctionScriptRunner</code> is used to run the script. model : AbstractModel | None, defaults to None     The surrogate model. acquisition_function : AbstractAcquisitionFunction | None, defaults to None     The acquisition function. acquisition_maximizer : AbstractAcquisitionMaximizer | None, defaults to None     The acquisition maximizer, deciding which configuration is most promising based on the surrogate model and     acquisition function. initial_design : InitialDesign | None, defaults to None     The sampled configurations from the initial design are evaluated before the Bayesian optimization loop starts. random_design : RandomDesign | None, defaults to None     The random design is used in the acquisition maximizer, deciding whether the next configuration should be drawn     from the acquisition function or randomly. intensifier : AbstractIntensifier | None, defaults to None     The intensifier decides which trial (combination of configuration, seed, budget and instance) should be run     next. multi_objective_algorithm : AbstractMultiObjectiveAlgorithm | None, defaults to None     In case of multiple objectives, the objectives need to be interpreted so that an optimization is possible.     The multi-objective algorithm takes care of that. runhistory_encoder : RunHistoryEncoder | None, defaults to None     Based on the runhistory, the surrogate model is trained. However, the data first needs to be encoded, which     is done by the runhistory encoder. For example, inactive hyperparameters need to be encoded or cost values     can be log transformed. logging_level: int | Path | Literal[False] | None     The level of logging (the lowest level 0 indicates the debug level). If a path is passed, a yaml file is     expected with the logging configuration. If nothing is passed, the default logging.yml from SMAC is used.     If False is passed, SMAC will not do any customization of the logging setup and the responsibility is left     to the user. callbacks: list[Callback], defaults to []     Callbacks, which are incorporated into the optimization loop. overwrite: bool, defaults to False     When True, overwrites the run results if a previous run is found that is     consistent in the meta data with the current setup. When False and a previous run is found that is     consistent in the meta data, the run is continued. When False and a previous run is found that is     not consistent in the meta data, the the user is asked for the exact behaviour (overwrite completely     or rename old run first). dask_client: Client | None, defaults to None     User-created dask client, which can be used to start a dask cluster and then attach SMAC to it. This will not     be closed automatically and will have to be closed manually if provided explicitly. If none is provided     (default), a local one will be created for you and closed upon completion.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    acquisition_maximizer: AbstractAcquisitionMaximizer | None = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    runhistory_encoder: AbstractRunHistoryEncoder | None = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: int | Path | Literal[False] | None = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None,\n):\n    setup_logging(logging_level)\n\n    if callbacks is None:\n        callbacks = []\n\n    if model is None:\n        model = self.get_model(scenario)\n\n    if acquisition_function is None:\n        acquisition_function = self.get_acquisition_function(scenario)\n\n    if acquisition_maximizer is None:\n        acquisition_maximizer = self.get_acquisition_maximizer(scenario)\n\n    if initial_design is None:\n        initial_design = self.get_initial_design(scenario)\n\n    if random_design is None:\n        random_design = self.get_random_design(scenario)\n\n    if intensifier is None:\n        intensifier = self.get_intensifier(scenario)\n\n    if multi_objective_algorithm is None and scenario.count_objectives() &gt; 1:\n        multi_objective_algorithm = self.get_multi_objective_algorithm(scenario=scenario)\n\n    if runhistory_encoder is None:\n        runhistory_encoder = self.get_runhistory_encoder(scenario)\n\n    if config_selector is None:\n        config_selector = self.get_config_selector(scenario)\n\n    # Initialize empty stats and runhistory object\n    runhistory = RunHistory(multi_objective_algorithm=multi_objective_algorithm)\n\n    # Set the seed for configuration space\n    scenario.configspace.seed(scenario.seed)\n\n    # Set variables globally\n    self._scenario = scenario\n    self._model = model\n    self._acquisition_function = acquisition_function\n    self._acquisition_maximizer = acquisition_maximizer\n    self._initial_design = initial_design\n    self._random_design = random_design\n    self._intensifier = intensifier\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._runhistory = runhistory\n    self._runhistory_encoder = runhistory_encoder\n    self._config_selector = config_selector\n    self._callbacks = callbacks\n    self._overwrite = overwrite\n\n    # Prepare the algorithm executer\n    runner: AbstractRunner\n    if isinstance(target_function, AbstractRunner):\n        runner = target_function\n    elif isinstance(target_function, str):\n        runner = TargetFunctionScriptRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n    else:\n        runner = TargetFunctionRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n\n    # In case of multiple jobs, we need to wrap the runner again using DaskParallelRunner\n    if (n_workers := scenario.n_workers) &gt; 1 or dask_client is not None:\n        if dask_client is not None and n_workers &gt; 1:\n            logger.warning(\n                \"Provided `dask_client`. Ignore `scenario.n_workers`, directly set `n_workers` in `dask_client`.\"\n            )\n        else:\n            available_workers = joblib.cpu_count()\n            if n_workers &gt; available_workers:\n                logger.info(f\"Workers are reduced to {n_workers}.\")\n                n_workers = available_workers\n\n        # We use a dask runner for parallelization\n        runner = DaskParallelRunner(single_worker=runner, dask_client=dask_client)\n\n    # Set the runner to access it globally\n    self._runner = runner\n\n    # Adding dependencies of the components\n    self._update_dependencies()\n\n    # We have to update our meta data (basically arguments of the components)\n    self._scenario._set_meta(self.meta)\n\n    # We have to validate if the object compositions are correct and actually make sense\n    self._validate()\n\n    # Finally we configure our optimizer\n    self._optimizer = self._get_optimizer()\n    assert self._optimizer\n\n    # Register callbacks here\n    for callback in callbacks:\n        self._optimizer.register_callback(callback)\n\n    # Additionally, we register the runhistory callback from the intensifier to efficiently update our incumbent\n    # every time new information are available\n    self._optimizer.register_callback(self._intensifier.get_callback(), index=0)\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Generates a hash based on all components of the facade. This is used for the run name or to determine whether a run should be continued or not.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.optimizer","title":"optimizer  <code>property</code>","text":"<pre><code>optimizer: SMBO\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The runhistory which is filled with all trials during the optimization process.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.scenario","title":"scenario  <code>property</code>","text":"<pre><code>scenario: Scenario\n</code></pre> <p>The scenario object which holds all environment information.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\"\"\"\n    return self._optimizer.ask()\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_acquisition_function","title":"get_acquisition_function  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_acquisition_function(\n    scenario: Scenario,\n) -&gt; AbstractAcquisitionFunction\n</code></pre> <p>Returns the acquisition function instance used in the BO loop, defining the exploration/exploitation trade-off.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_acquisition_function(scenario: Scenario) -&gt; AbstractAcquisitionFunction:\n    \"\"\"Returns the acquisition function instance used in the BO loop,\n    defining the exploration/exploitation trade-off.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_acquisition_maximizer","title":"get_acquisition_maximizer  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_acquisition_maximizer(\n    scenario: Scenario,\n) -&gt; AbstractAcquisitionMaximizer\n</code></pre> <p>Returns the acquisition optimizer instance to be used in the BO loop, specifying how the acquisition function instance is optimized.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_acquisition_maximizer(scenario: Scenario) -&gt; AbstractAcquisitionMaximizer:\n    \"\"\"Returns the acquisition optimizer instance to be used in the BO loop,\n    specifying how the acquisition function instance is optimized.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_config_selector","title":"get_config_selector  <code>staticmethod</code>","text":"<pre><code>get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16\n) -&gt; ConfigSelector\n</code></pre> <p>Returns the default configuration selector.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\ndef get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n) -&gt; ConfigSelector:\n    \"\"\"Returns the default configuration selector.\"\"\"\n    return ConfigSelector(scenario, retrain_after=retrain_after, retries=retries)\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_initial_design","title":"get_initial_design  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_initial_design(\n    scenario: Scenario,\n) -&gt; AbstractInitialDesign\n</code></pre> <p>Returns an instance of the initial design class to be used in the BO loop, specifying how the configurations the BO loop is 'warm-started' with are selected.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_initial_design(scenario: Scenario) -&gt; AbstractInitialDesign:\n    \"\"\"Returns an instance of the initial design class to be used in the BO loop,\n    specifying how the configurations the BO loop is 'warm-started' with are selected.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_intensifier","title":"get_intensifier  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_intensifier(scenario: Scenario) -&gt; AbstractIntensifier\n</code></pre> <p>Returns the intensifier instance to be used in the BO loop, specifying how to challenge the incumbent configuration on other problem instances.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_intensifier(scenario: Scenario) -&gt; AbstractIntensifier:\n    \"\"\"Returns the intensifier instance to be used in the BO loop,\n    specifying how to challenge the incumbent configuration on other problem instances.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_model","title":"get_model  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_model(scenario: Scenario) -&gt; AbstractModel\n</code></pre> <p>Returns the surrogate cost model instance used in the BO loop.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_model(scenario: Scenario) -&gt; AbstractModel:\n    \"\"\"Returns the surrogate cost model instance used in the BO loop.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_multi_objective_algorithm","title":"get_multi_objective_algorithm  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_multi_objective_algorithm(\n    scenario: Scenario,\n) -&gt; AbstractMultiObjectiveAlgorithm\n</code></pre> <p>Returns the multi-objective algorithm instance to be used in the BO loop, specifying the scalarization strategy for multiple objectives' costs.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_multi_objective_algorithm(scenario: Scenario) -&gt; AbstractMultiObjectiveAlgorithm:\n    \"\"\"Returns the multi-objective algorithm instance to be used in the BO loop,\n    specifying the scalarization strategy for multiple objectives' costs.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_random_design","title":"get_random_design  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_random_design(\n    scenario: Scenario,\n) -&gt; AbstractRandomDesign\n</code></pre> <p>Returns an instance of the random design class to be used in the BO loop, specifying how to interleave the BO iterations with randomly selected configurations.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_random_design(scenario: Scenario) -&gt; AbstractRandomDesign:\n    \"\"\"Returns an instance of the random design class to be used in the BO loop,\n    specifying how to interleave the BO iterations with randomly selected configurations.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.get_runhistory_encoder","title":"get_runhistory_encoder  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; AbstractRunHistoryEncoder\n</code></pre> <p>Returns an instance of the runhistory encoder class to be used in the BO loop, specifying how the runhistory is to be prepared for the next surrogate model.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_runhistory_encoder(scenario: Scenario) -&gt; AbstractRunHistoryEncoder:\n    \"\"\"Returns an instance of the runhistory encoder class to be used in the BO loop,\n    specifying how the runhistory is to be prepared for the next surrogate model.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Optimizes the configuration of the algorithm.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     We first note that this argument is valid only dask_runner!     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     Best found configuration.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"\n    Optimizes the configuration of the algorithm.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        We first note that this argument is valid only dask_runner!\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        Best found configuration.\n    \"\"\"\n    incumbents = None\n    if isinstance(data_to_scatter, dict) and len(data_to_scatter) == 0:\n        raise ValueError(\"data_to_scatter must be None or dict with some elements, but got an empty dict.\")\n\n    try:\n        incumbents = self._optimizer.optimize(data_to_scatter=data_to_scatter)\n    finally:\n        self._optimizer.save()\n\n    return incumbents\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the intensifier.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.tell--parameters","title":"Parameters","text":"<p>info: TrialInfo     Describes the trial from which to process the results. value: TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def tell(self, info: TrialInfo, value: TrialValue, save: bool = True) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the intensifier.\n\n    Parameters\n    ----------\n    info: TrialInfo\n        Describes the trial from which to process the results.\n    value: TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    return self._optimizer.tell(info, value, save=save)\n</code></pre>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | list[float]\n</code></pre> <p>Validates a configuration on seeds different from the ones used in the optimization process and on the highest budget (if budget type is real-valued).</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate instances : list[str] | None, defaults to None     Which instances to validate. If None, all instances specified in the scenario are used.     In case that the budget type is real-valued, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/facade/abstract_facade/#smac.facade.abstract_facade.AbstractFacade.validate--returns","title":"Returns","text":"<p>cost : float | list[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"Validates a configuration on seeds different from the ones used in the optimization process and on the\n    highest budget (if budget type is real-valued).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n    instances : list[str] | None, defaults to None\n        Which instances to validate. If None, all instances specified in the scenario are used.\n        In case that the budget type is real-valued, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | list[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    return self._optimizer.validate(config, seed=seed)\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/","title":"Algorithm configuration facade","text":""},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade","title":"smac.facade.algorithm_configuration_facade","text":""},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade","title":"AlgorithmConfigurationFacade","text":"<pre><code>AlgorithmConfigurationFacade(\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    acquisition_maximizer: (\n        AbstractAcquisitionMaximizer | None\n    ) = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    runhistory_encoder: (\n        AbstractRunHistoryEncoder | None\n    ) = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractFacade</code></p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    acquisition_maximizer: AbstractAcquisitionMaximizer | None = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    runhistory_encoder: AbstractRunHistoryEncoder | None = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: int | Path | Literal[False] | None = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None,\n):\n    setup_logging(logging_level)\n\n    if callbacks is None:\n        callbacks = []\n\n    if model is None:\n        model = self.get_model(scenario)\n\n    if acquisition_function is None:\n        acquisition_function = self.get_acquisition_function(scenario)\n\n    if acquisition_maximizer is None:\n        acquisition_maximizer = self.get_acquisition_maximizer(scenario)\n\n    if initial_design is None:\n        initial_design = self.get_initial_design(scenario)\n\n    if random_design is None:\n        random_design = self.get_random_design(scenario)\n\n    if intensifier is None:\n        intensifier = self.get_intensifier(scenario)\n\n    if multi_objective_algorithm is None and scenario.count_objectives() &gt; 1:\n        multi_objective_algorithm = self.get_multi_objective_algorithm(scenario=scenario)\n\n    if runhistory_encoder is None:\n        runhistory_encoder = self.get_runhistory_encoder(scenario)\n\n    if config_selector is None:\n        config_selector = self.get_config_selector(scenario)\n\n    # Initialize empty stats and runhistory object\n    runhistory = RunHistory(multi_objective_algorithm=multi_objective_algorithm)\n\n    # Set the seed for configuration space\n    scenario.configspace.seed(scenario.seed)\n\n    # Set variables globally\n    self._scenario = scenario\n    self._model = model\n    self._acquisition_function = acquisition_function\n    self._acquisition_maximizer = acquisition_maximizer\n    self._initial_design = initial_design\n    self._random_design = random_design\n    self._intensifier = intensifier\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._runhistory = runhistory\n    self._runhistory_encoder = runhistory_encoder\n    self._config_selector = config_selector\n    self._callbacks = callbacks\n    self._overwrite = overwrite\n\n    # Prepare the algorithm executer\n    runner: AbstractRunner\n    if isinstance(target_function, AbstractRunner):\n        runner = target_function\n    elif isinstance(target_function, str):\n        runner = TargetFunctionScriptRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n    else:\n        runner = TargetFunctionRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n\n    # In case of multiple jobs, we need to wrap the runner again using DaskParallelRunner\n    if (n_workers := scenario.n_workers) &gt; 1 or dask_client is not None:\n        if dask_client is not None and n_workers &gt; 1:\n            logger.warning(\n                \"Provided `dask_client`. Ignore `scenario.n_workers`, directly set `n_workers` in `dask_client`.\"\n            )\n        else:\n            available_workers = joblib.cpu_count()\n            if n_workers &gt; available_workers:\n                logger.info(f\"Workers are reduced to {n_workers}.\")\n                n_workers = available_workers\n\n        # We use a dask runner for parallelization\n        runner = DaskParallelRunner(single_worker=runner, dask_client=dask_client)\n\n    # Set the runner to access it globally\n    self._runner = runner\n\n    # Adding dependencies of the components\n    self._update_dependencies()\n\n    # We have to update our meta data (basically arguments of the components)\n    self._scenario._set_meta(self.meta)\n\n    # We have to validate if the object compositions are correct and actually make sense\n    self._validate()\n\n    # Finally we configure our optimizer\n    self._optimizer = self._get_optimizer()\n    assert self._optimizer\n\n    # Register callbacks here\n    for callback in callbacks:\n        self._optimizer.register_callback(callback)\n\n    # Additionally, we register the runhistory callback from the intensifier to efficiently update our incumbent\n    # every time new information are available\n    self._optimizer.register_callback(self._intensifier.get_callback(), index=0)\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Generates a hash based on all components of the facade. This is used for the run name or to determine whether a run should be continued or not.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.optimizer","title":"optimizer  <code>property</code>","text":"<pre><code>optimizer: SMBO\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The runhistory which is filled with all trials during the optimization process.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.scenario","title":"scenario  <code>property</code>","text":"<pre><code>scenario: Scenario\n</code></pre> <p>The scenario object which holds all environment information.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\"\"\"\n    return self._optimizer.ask()\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_acquisition_function","title":"get_acquisition_function  <code>staticmethod</code>","text":"<pre><code>get_acquisition_function(\n    scenario: Scenario, *, xi: float = 0.0\n) -&gt; EI\n</code></pre> <p>Returns an Expected Improvement acquisition function.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_acquisition_function--parameters","title":"Parameters","text":"<p>scenario : Scenario xi : float, defaults to 0.0     Controls the balance between exploration and exploitation of the     acquisition function.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_function(  # type: ignore\n    scenario: Scenario,\n    *,\n    xi: float = 0.0,\n) -&gt; EI:\n    \"\"\"Returns an Expected Improvement acquisition function.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    xi : float, defaults to 0.0\n        Controls the balance between exploration and exploitation of the\n        acquisition function.\n    \"\"\"\n    return EI(xi=xi)\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_acquisition_maximizer","title":"get_acquisition_maximizer  <code>staticmethod</code>","text":"<pre><code>get_acquisition_maximizer(\n    scenario: Scenario,\n) -&gt; LocalAndSortedRandomSearch\n</code></pre> <p>Returns local and sorted random search as acquisition maximizer.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_maximizer(  # type: ignore\n    scenario: Scenario,\n) -&gt; LocalAndSortedRandomSearch:\n    \"\"\"Returns local and sorted random search as acquisition maximizer.\"\"\"\n    optimizer = LocalAndSortedRandomSearch(\n        scenario.configspace,\n        seed=scenario.seed,\n    )\n\n    return optimizer\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_config_selector","title":"get_config_selector  <code>staticmethod</code>","text":"<pre><code>get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16\n) -&gt; ConfigSelector\n</code></pre> <p>Returns the default configuration selector.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\ndef get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n) -&gt; ConfigSelector:\n    \"\"\"Returns the default configuration selector.\"\"\"\n    return ConfigSelector(scenario, retrain_after=retrain_after, retries=retries)\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_initial_design","title":"get_initial_design  <code>staticmethod</code>","text":"<pre><code>get_initial_design(\n    scenario: Scenario,\n    *,\n    additional_configs: list[Configuration] = None\n) -&gt; DefaultInitialDesign\n</code></pre> <p>Returns an initial design, which returns the default configuration.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_initial_design--parameters","title":"Parameters","text":"<p>additional_configs: list[Configuration], defaults to []     Adds additional configurations to the initial design.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_initial_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    additional_configs: list[Configuration] = None,\n) -&gt; DefaultInitialDesign:\n    \"\"\"Returns an initial design, which returns the default configuration.\n\n    Parameters\n    ----------\n    additional_configs: list[Configuration], defaults to []\n        Adds additional configurations to the initial design.\n    \"\"\"\n    if additional_configs is None:\n        additional_configs = []\n    return DefaultInitialDesign(\n        scenario=scenario,\n        additional_configs=additional_configs,\n    )\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_intensifier","title":"get_intensifier  <code>staticmethod</code>","text":"<pre><code>get_intensifier(\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 2000,\n    max_incumbents: int = 10\n) -&gt; Intensifier\n</code></pre> <p>Returns <code>Intensifier</code> as intensifier. Supports budgets.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_intensifier--parameters","title":"Parameters","text":"<p>max_config_calls : int, defaults to 3     Maximum number of configuration evaluations. Basically, how many instance-seed keys should be evaluated at     maximum for a configuration. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_intensifier(\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 2000,\n    max_incumbents: int = 10,\n) -&gt; Intensifier:\n    \"\"\"Returns ``Intensifier`` as intensifier. Supports budgets.\n\n    Parameters\n    ----------\n    max_config_calls : int, defaults to 3\n        Maximum number of configuration evaluations. Basically, how many instance-seed keys should be evaluated at\n        maximum for a configuration.\n    max_incumbents : int, defaults to 10\n        How many incumbents to keep track of in the case of multi-objective.\n    \"\"\"\n    return Intensifier(\n        scenario=scenario,\n        max_config_calls=max_config_calls,\n        max_incumbents=max_incumbents,\n    )\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_model","title":"get_model  <code>staticmethod</code>","text":"<pre><code>get_model(\n    scenario: Scenario,\n    *,\n    n_trees: int = 10,\n    ratio_features: float = 5.0 / 6.0,\n    min_samples_split: int = 3,\n    min_samples_leaf: int = 3,\n    max_depth: int = 20,\n    bootstrapping: bool = True,\n    pca_components: int = 4\n) -&gt; RandomForest\n</code></pre> <p>Returns a random forest as surrogate model.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_model--parameters","title":"Parameters","text":"<p>n_trees : int, defaults to 10     The number of trees in the random forest. ratio_features : float, defaults to 5.0 / 6.0     The ratio of features that are considered for splitting. min_samples_split : int, defaults to 3     The minimum number of data points to perform a split. min_samples_leaf : int, defaults to 3     The minimum number of data points in a leaf. max_depth : int, defaults to 20     The maximum depth of a single tree. bootstrapping : bool, defaults to True     Enables bootstrapping. pca_components : float, defaults to 4     Number of components to keep when using PCA to reduce dimensionality of instance features.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_model(  # type: ignore\n    scenario: Scenario,\n    *,\n    n_trees: int = 10,\n    ratio_features: float = 5.0 / 6.0,\n    min_samples_split: int = 3,\n    min_samples_leaf: int = 3,\n    max_depth: int = 20,\n    bootstrapping: bool = True,\n    pca_components: int = 4,\n) -&gt; RandomForest:\n    \"\"\"Returns a random forest as surrogate model.\n\n    Parameters\n    ----------\n    n_trees : int, defaults to 10\n        The number of trees in the random forest.\n    ratio_features : float, defaults to 5.0 / 6.0\n        The ratio of features that are considered for splitting.\n    min_samples_split : int, defaults to 3\n        The minimum number of data points to perform a split.\n    min_samples_leaf : int, defaults to 3\n        The minimum number of data points in a leaf.\n    max_depth : int, defaults to 20\n        The maximum depth of a single tree.\n    bootstrapping : bool, defaults to True\n        Enables bootstrapping.\n    pca_components : float, defaults to 4\n        Number of components to keep when using PCA to reduce dimensionality of instance features.\n    \"\"\"\n    return RandomForest(\n        configspace=scenario.configspace,\n        n_trees=n_trees,\n        ratio_features=ratio_features,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        max_depth=max_depth,\n        bootstrapping=bootstrapping,\n        log_y=False,\n        instance_features=scenario.instance_features,\n        pca_components=pca_components,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_multi_objective_algorithm","title":"get_multi_objective_algorithm  <code>staticmethod</code>","text":"<pre><code>get_multi_objective_algorithm(\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None\n) -&gt; MeanAggregationStrategy\n</code></pre> <p>Returns the mean aggregation strategy for the multi objective algorithm.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_multi_objective_algorithm--parameters","title":"Parameters","text":"<p>scenario : Scenario objective_weights : list[float] | None, defaults to None     Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of     objectives.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_multi_objective_algorithm(  # type: ignore\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None,\n) -&gt; MeanAggregationStrategy:\n    \"\"\"Returns the mean aggregation strategy for the multi objective algorithm.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    objective_weights : list[float] | None, defaults to None\n        Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of\n        objectives.\n    \"\"\"\n    return MeanAggregationStrategy(\n        scenario=scenario,\n        objective_weights=objective_weights,\n    )\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_random_design","title":"get_random_design  <code>staticmethod</code>","text":"<pre><code>get_random_design(\n    scenario: Scenario, *, probability: float = 0.5\n) -&gt; ProbabilityRandomDesign\n</code></pre> <p>Returns <code>ProbabilityRandomDesign</code> for interleaving configurations.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_random_design--parameters","title":"Parameters","text":"<p>probability : float, defaults to 0.5     Probability that a configuration will be drawn at random.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_random_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    probability: float = 0.5,\n) -&gt; ProbabilityRandomDesign:\n    \"\"\"Returns ``ProbabilityRandomDesign`` for interleaving configurations.\n\n    Parameters\n    ----------\n    probability : float, defaults to 0.5\n        Probability that a configuration will be drawn at random.\n    \"\"\"\n    return ProbabilityRandomDesign(probability=probability, seed=scenario.seed)\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.get_runhistory_encoder","title":"get_runhistory_encoder  <code>staticmethod</code>","text":"<pre><code>get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; RunHistoryEncoder\n</code></pre> <p>Returns the default runhistory encoder.</p> Source code in <code>smac/facade/algorithm_configuration_facade.py</code> <pre><code>@staticmethod\ndef get_runhistory_encoder(scenario: Scenario) -&gt; RunHistoryEncoder:\n    \"\"\"Returns the default runhistory encoder.\"\"\"\n    return RunHistoryEncoder(scenario)\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Optimizes the configuration of the algorithm.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     We first note that this argument is valid only dask_runner!     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     Best found configuration.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"\n    Optimizes the configuration of the algorithm.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        We first note that this argument is valid only dask_runner!\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        Best found configuration.\n    \"\"\"\n    incumbents = None\n    if isinstance(data_to_scatter, dict) and len(data_to_scatter) == 0:\n        raise ValueError(\"data_to_scatter must be None or dict with some elements, but got an empty dict.\")\n\n    try:\n        incumbents = self._optimizer.optimize(data_to_scatter=data_to_scatter)\n    finally:\n        self._optimizer.save()\n\n    return incumbents\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the intensifier.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.tell--parameters","title":"Parameters","text":"<p>info: TrialInfo     Describes the trial from which to process the results. value: TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def tell(self, info: TrialInfo, value: TrialValue, save: bool = True) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the intensifier.\n\n    Parameters\n    ----------\n    info: TrialInfo\n        Describes the trial from which to process the results.\n    value: TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    return self._optimizer.tell(info, value, save=save)\n</code></pre>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | list[float]\n</code></pre> <p>Validates a configuration on seeds different from the ones used in the optimization process and on the highest budget (if budget type is real-valued).</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate instances : list[str] | None, defaults to None     Which instances to validate. If None, all instances specified in the scenario are used.     In case that the budget type is real-valued, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/facade/algorithm_configuration_facade/#smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade.validate--returns","title":"Returns","text":"<p>cost : float | list[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"Validates a configuration on seeds different from the ones used in the optimization process and on the\n    highest budget (if budget type is real-valued).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n    instances : list[str] | None, defaults to None\n        Which instances to validate. If None, all instances specified in the scenario are used.\n        In case that the budget type is real-valued, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | list[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    return self._optimizer.validate(config, seed=seed)\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/","title":"Blackbox facade","text":""},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade","title":"smac.facade.blackbox_facade","text":""},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade","title":"BlackBoxFacade","text":"<pre><code>BlackBoxFacade(\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    acquisition_maximizer: (\n        AbstractAcquisitionMaximizer | None\n    ) = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    runhistory_encoder: (\n        AbstractRunHistoryEncoder | None\n    ) = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractFacade</code></p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    acquisition_maximizer: AbstractAcquisitionMaximizer | None = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    runhistory_encoder: AbstractRunHistoryEncoder | None = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: int | Path | Literal[False] | None = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None,\n):\n    setup_logging(logging_level)\n\n    if callbacks is None:\n        callbacks = []\n\n    if model is None:\n        model = self.get_model(scenario)\n\n    if acquisition_function is None:\n        acquisition_function = self.get_acquisition_function(scenario)\n\n    if acquisition_maximizer is None:\n        acquisition_maximizer = self.get_acquisition_maximizer(scenario)\n\n    if initial_design is None:\n        initial_design = self.get_initial_design(scenario)\n\n    if random_design is None:\n        random_design = self.get_random_design(scenario)\n\n    if intensifier is None:\n        intensifier = self.get_intensifier(scenario)\n\n    if multi_objective_algorithm is None and scenario.count_objectives() &gt; 1:\n        multi_objective_algorithm = self.get_multi_objective_algorithm(scenario=scenario)\n\n    if runhistory_encoder is None:\n        runhistory_encoder = self.get_runhistory_encoder(scenario)\n\n    if config_selector is None:\n        config_selector = self.get_config_selector(scenario)\n\n    # Initialize empty stats and runhistory object\n    runhistory = RunHistory(multi_objective_algorithm=multi_objective_algorithm)\n\n    # Set the seed for configuration space\n    scenario.configspace.seed(scenario.seed)\n\n    # Set variables globally\n    self._scenario = scenario\n    self._model = model\n    self._acquisition_function = acquisition_function\n    self._acquisition_maximizer = acquisition_maximizer\n    self._initial_design = initial_design\n    self._random_design = random_design\n    self._intensifier = intensifier\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._runhistory = runhistory\n    self._runhistory_encoder = runhistory_encoder\n    self._config_selector = config_selector\n    self._callbacks = callbacks\n    self._overwrite = overwrite\n\n    # Prepare the algorithm executer\n    runner: AbstractRunner\n    if isinstance(target_function, AbstractRunner):\n        runner = target_function\n    elif isinstance(target_function, str):\n        runner = TargetFunctionScriptRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n    else:\n        runner = TargetFunctionRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n\n    # In case of multiple jobs, we need to wrap the runner again using DaskParallelRunner\n    if (n_workers := scenario.n_workers) &gt; 1 or dask_client is not None:\n        if dask_client is not None and n_workers &gt; 1:\n            logger.warning(\n                \"Provided `dask_client`. Ignore `scenario.n_workers`, directly set `n_workers` in `dask_client`.\"\n            )\n        else:\n            available_workers = joblib.cpu_count()\n            if n_workers &gt; available_workers:\n                logger.info(f\"Workers are reduced to {n_workers}.\")\n                n_workers = available_workers\n\n        # We use a dask runner for parallelization\n        runner = DaskParallelRunner(single_worker=runner, dask_client=dask_client)\n\n    # Set the runner to access it globally\n    self._runner = runner\n\n    # Adding dependencies of the components\n    self._update_dependencies()\n\n    # We have to update our meta data (basically arguments of the components)\n    self._scenario._set_meta(self.meta)\n\n    # We have to validate if the object compositions are correct and actually make sense\n    self._validate()\n\n    # Finally we configure our optimizer\n    self._optimizer = self._get_optimizer()\n    assert self._optimizer\n\n    # Register callbacks here\n    for callback in callbacks:\n        self._optimizer.register_callback(callback)\n\n    # Additionally, we register the runhistory callback from the intensifier to efficiently update our incumbent\n    # every time new information are available\n    self._optimizer.register_callback(self._intensifier.get_callback(), index=0)\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Generates a hash based on all components of the facade. This is used for the run name or to determine whether a run should be continued or not.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.optimizer","title":"optimizer  <code>property</code>","text":"<pre><code>optimizer: SMBO\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The runhistory which is filled with all trials during the optimization process.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.scenario","title":"scenario  <code>property</code>","text":"<pre><code>scenario: Scenario\n</code></pre> <p>The scenario object which holds all environment information.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\"\"\"\n    return self._optimizer.ask()\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_acquisition_function","title":"get_acquisition_function  <code>staticmethod</code>","text":"<pre><code>get_acquisition_function(\n    scenario: Scenario, *, xi: float = 0.0\n) -&gt; EI\n</code></pre> <p>Returns an Expected Improvement acquisition function.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_acquisition_function--parameters","title":"Parameters","text":"<p>scenario : Scenario xi : float, defaults to 0.0     Controls the balance between exploration and exploitation of the     acquisition function.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_function(  # type: ignore\n    scenario: Scenario,\n    *,\n    xi: float = 0.0,\n) -&gt; EI:\n    \"\"\"Returns an Expected Improvement acquisition function.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    xi : float, defaults to 0.0\n        Controls the balance between exploration and exploitation of the\n        acquisition function.\n    \"\"\"\n    return EI(xi=xi)\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_acquisition_maximizer","title":"get_acquisition_maximizer  <code>staticmethod</code>","text":"<pre><code>get_acquisition_maximizer(\n    scenario: Scenario,\n    *,\n    challengers: int = 1000,\n    local_search_iterations: int = 10\n) -&gt; LocalAndSortedRandomSearch\n</code></pre> <p>Returns local and sorted random search as acquisition maximizer.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_acquisition_maximizer--parameters","title":"Parameters","text":"<p>challengers : int, defaults to 1000     Number of challengers. local_search_iterations: int, defaults to 10     Number of local search iterations.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_maximizer(  # type: ignore\n    scenario: Scenario,\n    *,\n    challengers: int = 1000,\n    local_search_iterations: int = 10,\n) -&gt; LocalAndSortedRandomSearch:\n    \"\"\"Returns local and sorted random search as acquisition maximizer.\n\n    Parameters\n    ----------\n    challengers : int, defaults to 1000\n        Number of challengers.\n    local_search_iterations: int, defaults to 10\n        Number of local search iterations.\n    \"\"\"\n    return LocalAndSortedRandomSearch(\n        configspace=scenario.configspace,\n        challengers=challengers,\n        local_search_iterations=local_search_iterations,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_config_selector","title":"get_config_selector  <code>staticmethod</code>","text":"<pre><code>get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 1,\n    retries: int = 16\n) -&gt; ConfigSelector\n</code></pre> <p>Returns the default configuration selector.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 1,\n    retries: int = 16,\n) -&gt; ConfigSelector:\n    \"\"\"Returns the default configuration selector.\"\"\"\n    return super(BlackBoxFacade, BlackBoxFacade).get_config_selector(\n        scenario, retrain_after=retrain_after, retries=retries\n    )\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_initial_design","title":"get_initial_design  <code>staticmethod</code>","text":"<pre><code>get_initial_design(\n    scenario: Scenario,\n    *,\n    n_configs: int | None = None,\n    n_configs_per_hyperparamter: int = 8,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None\n) -&gt; SobolInitialDesign\n</code></pre> <p>Returns a Sobol design instance.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_initial_design--parameters","title":"Parameters","text":"<p>scenario : Scenario n_configs : int | None, defaults to None     Number of initial configurations (disables the arguments <code>n_configs_per_hyperparameter</code>). n_configs_per_hyperparameter: int, defaults to 8     Number of initial configurations per hyperparameter. For example, if my configuration space covers five     hyperparameters and <code>n_configs_per_hyperparameter</code> is set to 10, then 50 initial configurations will be     samples. max_ratio: float, defaults to 0.25     Use at most <code>scenario.n_trials</code> * <code>max_ratio</code> number of configurations in the initial design.     Additional configurations are not affected by this parameter. additional_configs: list[Configuration], defaults to []     Adds additional configurations to the initial design.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_initial_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    n_configs: int | None = None,\n    n_configs_per_hyperparamter: int = 8,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n) -&gt; SobolInitialDesign:\n    \"\"\"Returns a Sobol design instance.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    n_configs : int | None, defaults to None\n        Number of initial configurations (disables the arguments ``n_configs_per_hyperparameter``).\n    n_configs_per_hyperparameter: int, defaults to 8\n        Number of initial configurations per hyperparameter. For example, if my configuration space covers five\n        hyperparameters and ``n_configs_per_hyperparameter`` is set to 10, then 50 initial configurations will be\n        samples.\n    max_ratio: float, defaults to 0.25\n        Use at most ``scenario.n_trials`` * ``max_ratio`` number of configurations in the initial design.\n        Additional configurations are not affected by this parameter.\n    additional_configs: list[Configuration], defaults to []\n        Adds additional configurations to the initial design.\n    \"\"\"\n    if additional_configs is None:\n        additional_configs = []\n    return SobolInitialDesign(\n        scenario=scenario,\n        n_configs=n_configs,\n        n_configs_per_hyperparameter=n_configs_per_hyperparamter,\n        max_ratio=max_ratio,\n        additional_configs=additional_configs,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_intensifier","title":"get_intensifier  <code>staticmethod</code>","text":"<pre><code>get_intensifier(\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 3,\n    max_incumbents: int = 20\n) -&gt; Intensifier\n</code></pre> <p>Returns <code>Intensifier</code> as intensifier. Uses the default configuration for <code>race_against</code>.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_intensifier--parameters","title":"Parameters","text":"<p>scenario : Scenario max_config_calls : int, defaults to 3     Maximum number of configuration evaluations. Basically, how many instance-seed keys should be evaluated at     maximum for a configuration. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_intensifier(  # type: ignore\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 3,\n    max_incumbents: int = 20,\n) -&gt; Intensifier:\n    \"\"\"Returns ``Intensifier`` as intensifier. Uses the default configuration for ``race_against``.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    max_config_calls : int, defaults to 3\n        Maximum number of configuration evaluations. Basically, how many instance-seed keys should be evaluated at\n        maximum for a configuration.\n    max_incumbents : int, defaults to 10\n        How many incumbents to keep track of in the case of multi-objective.\n    \"\"\"\n    return Intensifier(\n        scenario=scenario,\n        max_config_calls=max_config_calls,\n        max_incumbents=max_incumbents,\n    )\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_kernel","title":"get_kernel  <code>staticmethod</code>","text":"<pre><code>get_kernel(scenario: Scenario) -&gt; Kernel\n</code></pre> <p>Returns a kernel for the Gaussian Process surrogate model.</p> <p>The kernel is a composite of kernels depending on the type of hyperparameters: categorical (HammingKernel), continuous (Matern), and noise kernels (White).</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_kernel(scenario: Scenario) -&gt; kernels.Kernel:\n    \"\"\"Returns a kernel for the Gaussian Process surrogate model.\n\n    The kernel is a composite of kernels depending on the type of hyperparameters:\n    categorical (HammingKernel), continuous (Matern), and noise kernels (White).\n    \"\"\"\n    types, _ = get_types(scenario.configspace, instance_features=None)\n    cont_dims = np.where(np.array(types) == 0)[0]\n    cat_dims = np.where(np.array(types) != 0)[0]\n\n    if (len(cont_dims) + len(cat_dims)) != len(list(scenario.configspace.values())):\n        raise ValueError(\n            \"The inferred number of continuous and categorical hyperparameters \"\n            \"must equal the total number of hyperparameters. Got \"\n            f\"{(len(cont_dims) + len(cat_dims))} != {len(list(scenario.configspace.values()))}.\"\n        )\n\n    # Constant Kernel\n    cov_amp = ConstantKernel(\n        2.0,\n        constant_value_bounds=(np.exp(-10), np.exp(2)),\n        prior=LogNormalPrior(\n            mean=0.0,\n            sigma=1.0,\n            seed=scenario.seed,\n        ),\n    )\n\n    # Continuous / Categorical Kernels\n    exp_kernel, ham_kernel = 0.0, 0.0\n    if len(cont_dims) &gt; 0:\n        exp_kernel = MaternKernel(\n            np.ones([len(cont_dims)]),\n            [(np.exp(-6.754111155189306), np.exp(0.0858637988771976)) for _ in range(len(cont_dims))],\n            nu=2.5,\n            operate_on=cont_dims,\n        )\n    if len(cat_dims) &gt; 0:\n        ham_kernel = HammingKernel(\n            np.ones([len(cat_dims)]),\n            [(np.exp(-6.754111155189306), np.exp(0.0858637988771976)) for _ in range(len(cat_dims))],\n            operate_on=cat_dims,\n        )\n\n    # Noise Kernel\n    noise_kernel = WhiteKernel(\n        noise_level=1e-8,\n        noise_level_bounds=(np.exp(-25), np.exp(2)),\n        prior=HorseshoePrior(scale=0.1, seed=scenario.seed),\n    )\n\n    # Continuous and categecorical HPs\n    if len(cont_dims) &gt; 0 and len(cat_dims) &gt; 0:\n        kernel = cov_amp * (exp_kernel * ham_kernel) + noise_kernel\n\n    # Only continuous HPs\n    elif len(cont_dims) &gt; 0 and len(cat_dims) == 0:\n        kernel = cov_amp * exp_kernel + noise_kernel\n\n    # Only categorical HPs\n    elif len(cont_dims) == 0 and len(cat_dims) &gt; 0:\n        kernel = cov_amp * ham_kernel + noise_kernel\n\n    else:\n        raise ValueError(\"The number of continuous and categorical hyperparameters must be greater than zero.\")\n\n    return kernel\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_model","title":"get_model  <code>staticmethod</code>","text":"<pre><code>get_model(\n    scenario: Scenario,\n    *,\n    model_type: str | None = None,\n    kernel: Kernel | None = None\n) -&gt; AbstractGaussianProcess\n</code></pre> <p>Returns a Gaussian Process surrogate model.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_model--parameters","title":"Parameters","text":"<p>scenario : Scenario model_type : str | None, defaults to None     Which Gaussian Process model should be chosen. Choose between <code>vanilla</code> and <code>mcmc</code>. kernel : kernels.Kernel | None, defaults to None     The kernel used in the surrogate model.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_model--returns","title":"Returns","text":"<p>model : GaussianProcess | MCMCGaussianProcess     The instantiated gaussian process.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_model(\n    scenario: Scenario,\n    *,\n    model_type: str | None = None,\n    kernel: kernels.Kernel | None = None,\n) -&gt; AbstractGaussianProcess:\n    \"\"\"Returns a Gaussian Process surrogate model.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    model_type : str | None, defaults to None\n        Which Gaussian Process model should be chosen. Choose between `vanilla` and `mcmc`.\n    kernel : kernels.Kernel | None, defaults to None\n        The kernel used in the surrogate model.\n\n    Returns\n    -------\n    model : GaussianProcess | MCMCGaussianProcess\n        The instantiated gaussian process.\n    \"\"\"\n    available_model_types = [None, \"vanilla\", \"mcmc\"]\n    if model_type not in available_model_types:\n        types = [str(t) for t in available_model_types]\n        raise ValueError(f\"The model_type `{model_type}` is not supported. Choose one of {', '.join(types)}\")\n\n    if kernel is None:\n        kernel = BlackBoxFacade.get_kernel(scenario=scenario)\n\n    if model_type is None or model_type == \"vanilla\":\n        return GaussianProcess(\n            configspace=scenario.configspace,\n            kernel=kernel,\n            normalize_y=True,\n            seed=scenario.seed,\n        )\n    elif model_type == \"mcmc\":\n        n_mcmc_walkers = 3 * len(kernel.theta)\n        if n_mcmc_walkers % 2 == 1:\n            n_mcmc_walkers += 1\n\n        return MCMCGaussianProcess(\n            configspace=scenario.configspace,\n            kernel=kernel,\n            n_mcmc_walkers=n_mcmc_walkers,\n            chain_length=250,\n            burning_steps=250,\n            normalize_y=True,\n            seed=scenario.seed,\n        )\n    else:\n        raise ValueError(\"Unknown model type %s\" % model_type)\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_multi_objective_algorithm","title":"get_multi_objective_algorithm  <code>staticmethod</code>","text":"<pre><code>get_multi_objective_algorithm(\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None\n) -&gt; MeanAggregationStrategy\n</code></pre> <p>Returns the mean aggregation strategy for the multi-objective algorithm.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_multi_objective_algorithm--parameters","title":"Parameters","text":"<p>scenario : Scenario objective_weights : list[float] | None, defaults to None     Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of     objectives.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_multi_objective_algorithm(  # type: ignore\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None,\n) -&gt; MeanAggregationStrategy:\n    \"\"\"Returns the mean aggregation strategy for the multi-objective algorithm.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    objective_weights : list[float] | None, defaults to None\n        Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of\n        objectives.\n    \"\"\"\n    return MeanAggregationStrategy(\n        scenario=scenario,\n        objective_weights=objective_weights,\n    )\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_random_design","title":"get_random_design  <code>staticmethod</code>","text":"<pre><code>get_random_design(\n    scenario: Scenario,\n    *,\n    probability: float = 0.08447232371720552\n) -&gt; ProbabilityRandomDesign\n</code></pre> <p>Returns <code>ProbabilityRandomDesign</code> for interleaving configurations.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_random_design--parameters","title":"Parameters","text":"<p>probability : float, defaults to 0.08447232371720552     Probability that a configuration will be drawn at random.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_random_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    probability: float = 0.08447232371720552,\n) -&gt; ProbabilityRandomDesign:\n    \"\"\"Returns ``ProbabilityRandomDesign`` for interleaving configurations.\n\n    Parameters\n    ----------\n    probability : float, defaults to 0.08447232371720552\n        Probability that a configuration will be drawn at random.\n    \"\"\"\n    return ProbabilityRandomDesign(seed=scenario.seed, probability=probability)\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.get_runhistory_encoder","title":"get_runhistory_encoder  <code>staticmethod</code>","text":"<pre><code>get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; RunHistoryEncoder\n</code></pre> <p>Returns the default runhistory encoder.</p> Source code in <code>smac/facade/blackbox_facade.py</code> <pre><code>@staticmethod\ndef get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; RunHistoryEncoder:\n    \"\"\"Returns the default runhistory encoder.\"\"\"\n    return RunHistoryEncoder(scenario)\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Optimizes the configuration of the algorithm.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     We first note that this argument is valid only dask_runner!     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     Best found configuration.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"\n    Optimizes the configuration of the algorithm.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        We first note that this argument is valid only dask_runner!\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        Best found configuration.\n    \"\"\"\n    incumbents = None\n    if isinstance(data_to_scatter, dict) and len(data_to_scatter) == 0:\n        raise ValueError(\"data_to_scatter must be None or dict with some elements, but got an empty dict.\")\n\n    try:\n        incumbents = self._optimizer.optimize(data_to_scatter=data_to_scatter)\n    finally:\n        self._optimizer.save()\n\n    return incumbents\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the intensifier.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.tell--parameters","title":"Parameters","text":"<p>info: TrialInfo     Describes the trial from which to process the results. value: TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def tell(self, info: TrialInfo, value: TrialValue, save: bool = True) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the intensifier.\n\n    Parameters\n    ----------\n    info: TrialInfo\n        Describes the trial from which to process the results.\n    value: TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    return self._optimizer.tell(info, value, save=save)\n</code></pre>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | list[float]\n</code></pre> <p>Validates a configuration on seeds different from the ones used in the optimization process and on the highest budget (if budget type is real-valued).</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate instances : list[str] | None, defaults to None     Which instances to validate. If None, all instances specified in the scenario are used.     In case that the budget type is real-valued, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/facade/blackbox_facade/#smac.facade.blackbox_facade.BlackBoxFacade.validate--returns","title":"Returns","text":"<p>cost : float | list[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"Validates a configuration on seeds different from the ones used in the optimization process and on the\n    highest budget (if budget type is real-valued).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n    instances : list[str] | None, defaults to None\n        Which instances to validate. If None, all instances specified in the scenario are used.\n        In case that the budget type is real-valued, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | list[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    return self._optimizer.validate(config, seed=seed)\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/","title":"Hyperband facade","text":""},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade","title":"smac.facade.hyperband_facade","text":""},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade","title":"HyperbandFacade","text":"<pre><code>HyperbandFacade(\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    acquisition_maximizer: (\n        AbstractAcquisitionMaximizer | None\n    ) = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    runhistory_encoder: (\n        AbstractRunHistoryEncoder | None\n    ) = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None\n)\n</code></pre> <p>               Bases: <code>RandomFacade</code></p> <p>Facade to use model-free Hyperband [LJDR18] for algorithm configuration.</p> <p>Uses Random Aggressive Online Racing (ROAR) to compare configurations, a random initial design and the Hyperband intensifier.</p> <p>Warning</p> <p><code>smac.main.config_selector.ConfigSelector</code> contains the <code>min_trials</code> parameter. This parameter determines how many samples are required to train the surrogate model. If budgets are involved, the highest budgets are checked first. For example, if min_trials is three, but we find only two trials in the runhistory for the highest budget, we will use trials of a lower budget instead.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    acquisition_maximizer: AbstractAcquisitionMaximizer | None = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    runhistory_encoder: AbstractRunHistoryEncoder | None = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: int | Path | Literal[False] | None = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None,\n):\n    setup_logging(logging_level)\n\n    if callbacks is None:\n        callbacks = []\n\n    if model is None:\n        model = self.get_model(scenario)\n\n    if acquisition_function is None:\n        acquisition_function = self.get_acquisition_function(scenario)\n\n    if acquisition_maximizer is None:\n        acquisition_maximizer = self.get_acquisition_maximizer(scenario)\n\n    if initial_design is None:\n        initial_design = self.get_initial_design(scenario)\n\n    if random_design is None:\n        random_design = self.get_random_design(scenario)\n\n    if intensifier is None:\n        intensifier = self.get_intensifier(scenario)\n\n    if multi_objective_algorithm is None and scenario.count_objectives() &gt; 1:\n        multi_objective_algorithm = self.get_multi_objective_algorithm(scenario=scenario)\n\n    if runhistory_encoder is None:\n        runhistory_encoder = self.get_runhistory_encoder(scenario)\n\n    if config_selector is None:\n        config_selector = self.get_config_selector(scenario)\n\n    # Initialize empty stats and runhistory object\n    runhistory = RunHistory(multi_objective_algorithm=multi_objective_algorithm)\n\n    # Set the seed for configuration space\n    scenario.configspace.seed(scenario.seed)\n\n    # Set variables globally\n    self._scenario = scenario\n    self._model = model\n    self._acquisition_function = acquisition_function\n    self._acquisition_maximizer = acquisition_maximizer\n    self._initial_design = initial_design\n    self._random_design = random_design\n    self._intensifier = intensifier\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._runhistory = runhistory\n    self._runhistory_encoder = runhistory_encoder\n    self._config_selector = config_selector\n    self._callbacks = callbacks\n    self._overwrite = overwrite\n\n    # Prepare the algorithm executer\n    runner: AbstractRunner\n    if isinstance(target_function, AbstractRunner):\n        runner = target_function\n    elif isinstance(target_function, str):\n        runner = TargetFunctionScriptRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n    else:\n        runner = TargetFunctionRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n\n    # In case of multiple jobs, we need to wrap the runner again using DaskParallelRunner\n    if (n_workers := scenario.n_workers) &gt; 1 or dask_client is not None:\n        if dask_client is not None and n_workers &gt; 1:\n            logger.warning(\n                \"Provided `dask_client`. Ignore `scenario.n_workers`, directly set `n_workers` in `dask_client`.\"\n            )\n        else:\n            available_workers = joblib.cpu_count()\n            if n_workers &gt; available_workers:\n                logger.info(f\"Workers are reduced to {n_workers}.\")\n                n_workers = available_workers\n\n        # We use a dask runner for parallelization\n        runner = DaskParallelRunner(single_worker=runner, dask_client=dask_client)\n\n    # Set the runner to access it globally\n    self._runner = runner\n\n    # Adding dependencies of the components\n    self._update_dependencies()\n\n    # We have to update our meta data (basically arguments of the components)\n    self._scenario._set_meta(self.meta)\n\n    # We have to validate if the object compositions are correct and actually make sense\n    self._validate()\n\n    # Finally we configure our optimizer\n    self._optimizer = self._get_optimizer()\n    assert self._optimizer\n\n    # Register callbacks here\n    for callback in callbacks:\n        self._optimizer.register_callback(callback)\n\n    # Additionally, we register the runhistory callback from the intensifier to efficiently update our incumbent\n    # every time new information are available\n    self._optimizer.register_callback(self._intensifier.get_callback(), index=0)\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Generates a hash based on all components of the facade. This is used for the run name or to determine whether a run should be continued or not.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.optimizer","title":"optimizer  <code>property</code>","text":"<pre><code>optimizer: SMBO\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The runhistory which is filled with all trials during the optimization process.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.scenario","title":"scenario  <code>property</code>","text":"<pre><code>scenario: Scenario\n</code></pre> <p>The scenario object which holds all environment information.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\"\"\"\n    return self._optimizer.ask()\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_acquisition_function","title":"get_acquisition_function  <code>staticmethod</code>","text":"<pre><code>get_acquisition_function(\n    scenario: Scenario,\n) -&gt; AbstractAcquisitionFunction\n</code></pre> <p>The random facade is not using an acquisition function. Therefore, we simply return a dummy function.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_function(scenario: Scenario) -&gt; AbstractAcquisitionFunction:\n    \"\"\"The random facade is not using an acquisition function. Therefore, we simply return a dummy function.\"\"\"\n\n    class DummyAcquisitionFunction(AbstractAcquisitionFunction):\n        def _compute(self, X: np.ndarray) -&gt; np.ndarray:\n            return X\n\n    return DummyAcquisitionFunction()\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_acquisition_maximizer","title":"get_acquisition_maximizer  <code>staticmethod</code>","text":"<pre><code>get_acquisition_maximizer(\n    scenario: Scenario,\n) -&gt; RandomSearch\n</code></pre> <p>We return <code>RandomSearch</code> as maximizer which samples configurations randomly from the configuration space and therefore neither uses the acquisition function nor the model.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_maximizer(scenario: Scenario) -&gt; RandomSearch:\n    \"\"\"We return ``RandomSearch`` as maximizer which samples configurations randomly from the configuration\n    space and therefore neither uses the acquisition function nor the model.\n    \"\"\"\n    return RandomSearch(\n        scenario.configspace,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_config_selector","title":"get_config_selector  <code>staticmethod</code>","text":"<pre><code>get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16\n) -&gt; ConfigSelector\n</code></pre> <p>Returns the default configuration selector.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\ndef get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n) -&gt; ConfigSelector:\n    \"\"\"Returns the default configuration selector.\"\"\"\n    return ConfigSelector(scenario, retrain_after=retrain_after, retries=retries)\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_initial_design","title":"get_initial_design  <code>staticmethod</code>","text":"<pre><code>get_initial_design(\n    scenario: Scenario,\n    *,\n    additional_configs: list[Configuration] = None\n) -&gt; DefaultInitialDesign\n</code></pre> <p>Returns an initial design, which returns the default configuration.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_initial_design--parameters","title":"Parameters","text":"<p>additional_configs: list[Configuration], defaults to []     Adds additional configurations to the initial design.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_initial_design(\n    scenario: Scenario,\n    *,\n    additional_configs: list[Configuration] = None,\n) -&gt; DefaultInitialDesign:\n    \"\"\"Returns an initial design, which returns the default configuration.\n\n    Parameters\n    ----------\n    additional_configs: list[Configuration], defaults to []\n        Adds additional configurations to the initial design.\n    \"\"\"\n    if additional_configs is None:\n        additional_configs = []\n    return DefaultInitialDesign(\n        scenario=scenario,\n        additional_configs=additional_configs,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_intensifier","title":"get_intensifier  <code>staticmethod</code>","text":"<pre><code>get_intensifier(\n    scenario: Scenario,\n    *,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\"\n) -&gt; Hyperband\n</code></pre> <p>Returns a Hyperband intensifier instance. Budgets are supported.</p> int, defaults to 3 <p>Input that controls the proportion of configurations discarded in each round of Successive Halving.</p> <p>n_seeds : int, defaults to 1     How many seeds to use for each instance. instance_seed_order : str, defaults to \"shuffle_once\"     How to order the instance-seed pairs. Can be set to:     * None: No shuffling at all and use the instance-seed order provided by the user.     * \"shuffle_once\": Shuffle the instance-seed keys once and use the same order across all runs.     * \"shuffle\": Shuffle the instance-seed keys for each bracket individually. incumbent_selection : str, defaults to \"any_budget\"     How to select the incumbent when using budgets. Can be set to:     * \"any_budget\": Incumbent is the best on any budget i.e., best performance regardless of budget.     * \"highest_observed_budget\": Incumbent is the best in the highest budget run so far.     * \"highest_budget\": Incumbent is selected only based on the highest budget. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective.</p> Source code in <code>smac/facade/hyperband_facade.py</code> <pre><code>@staticmethod\ndef get_intensifier(  # type: ignore\n    scenario: Scenario,\n    *,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\",\n) -&gt; Hyperband:\n    \"\"\"Returns a Hyperband intensifier instance. Budgets are supported.\n\n    eta : int, defaults to 3\n        Input that controls the proportion of configurations discarded in each round of Successive Halving.\n    n_seeds : int, defaults to 1\n        How many seeds to use for each instance.\n    instance_seed_order : str, defaults to \"shuffle_once\"\n        How to order the instance-seed pairs. Can be set to:\n        * None: No shuffling at all and use the instance-seed order provided by the user.\n        * \"shuffle_once\": Shuffle the instance-seed keys once and use the same order across all runs.\n        * \"shuffle\": Shuffle the instance-seed keys for each bracket individually.\n    incumbent_selection : str, defaults to \"any_budget\"\n        How to select the incumbent when using budgets. Can be set to:\n        * \"any_budget\": Incumbent is the best on any budget i.e., best performance regardless of budget.\n        * \"highest_observed_budget\": Incumbent is the best in the highest budget run so far.\n        * \"highest_budget\": Incumbent is selected only based on the highest budget.\n    max_incumbents : int, defaults to 10\n        How many incumbents to keep track of in the case of multi-objective.\n    \"\"\"\n    return Hyperband(\n        scenario=scenario,\n        eta=eta,\n        n_seeds=n_seeds,\n        instance_seed_order=instance_seed_order,\n        max_incumbents=max_incumbents,\n        incumbent_selection=incumbent_selection,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_model","title":"get_model  <code>staticmethod</code>","text":"<pre><code>get_model(scenario: Scenario) -&gt; RandomModel\n</code></pre> <p>The model is used in the acquisition function. Since we do not use an acquisition function, we return a dummy model (returning random values in this case).</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_model(scenario: Scenario) -&gt; RandomModel:\n    \"\"\"The model is used in the acquisition function. Since we do not use an acquisition function, we return a\n    dummy model (returning random values in this case).\n    \"\"\"\n    return RandomModel(\n        configspace=scenario.configspace,\n        instance_features=scenario.instance_features,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_multi_objective_algorithm","title":"get_multi_objective_algorithm  <code>staticmethod</code>","text":"<pre><code>get_multi_objective_algorithm(\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None\n) -&gt; MeanAggregationStrategy\n</code></pre> <p>Returns the mean aggregation strategy for the multi-objective algorithm.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_multi_objective_algorithm--parameters","title":"Parameters","text":"<p>scenario : Scenario objective_weights : list[float] | None, defaults to None     Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of     objectives.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_multi_objective_algorithm(  # type: ignore\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None,\n) -&gt; MeanAggregationStrategy:\n    \"\"\"Returns the mean aggregation strategy for the multi-objective algorithm.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    objective_weights : list[float] | None, defaults to None\n        Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of\n        objectives.\n    \"\"\"\n    return MeanAggregationStrategy(\n        scenario=scenario,\n        objective_weights=objective_weights,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_random_design","title":"get_random_design  <code>staticmethod</code>","text":"<pre><code>get_random_design(\n    scenario: Scenario,\n) -&gt; AbstractRandomDesign\n</code></pre> <p>Just like the acquisition function, we do not use a random design. Therefore, we return a dummy design.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_random_design(scenario: Scenario) -&gt; AbstractRandomDesign:\n    \"\"\"Just like the acquisition function, we do not use a random design. Therefore, we return a dummy design.\"\"\"\n\n    class DummyRandomDesign(AbstractRandomDesign):\n        def check(self, iteration: int) -&gt; bool:\n            return True\n\n    return DummyRandomDesign()\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.get_runhistory_encoder","title":"get_runhistory_encoder  <code>staticmethod</code>","text":"<pre><code>get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; RunHistoryEncoder\n</code></pre> <p>Returns the default runhistory encoder.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_runhistory_encoder(scenario: Scenario) -&gt; RunHistoryEncoder:\n    \"\"\"Returns the default runhistory encoder.\"\"\"\n    return RunHistoryEncoder(scenario)\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Optimizes the configuration of the algorithm.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     We first note that this argument is valid only dask_runner!     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     Best found configuration.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"\n    Optimizes the configuration of the algorithm.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        We first note that this argument is valid only dask_runner!\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        Best found configuration.\n    \"\"\"\n    incumbents = None\n    if isinstance(data_to_scatter, dict) and len(data_to_scatter) == 0:\n        raise ValueError(\"data_to_scatter must be None or dict with some elements, but got an empty dict.\")\n\n    try:\n        incumbents = self._optimizer.optimize(data_to_scatter=data_to_scatter)\n    finally:\n        self._optimizer.save()\n\n    return incumbents\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the intensifier.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.tell--parameters","title":"Parameters","text":"<p>info: TrialInfo     Describes the trial from which to process the results. value: TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def tell(self, info: TrialInfo, value: TrialValue, save: bool = True) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the intensifier.\n\n    Parameters\n    ----------\n    info: TrialInfo\n        Describes the trial from which to process the results.\n    value: TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    return self._optimizer.tell(info, value, save=save)\n</code></pre>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | list[float]\n</code></pre> <p>Validates a configuration on seeds different from the ones used in the optimization process and on the highest budget (if budget type is real-valued).</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate instances : list[str] | None, defaults to None     Which instances to validate. If None, all instances specified in the scenario are used.     In case that the budget type is real-valued, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/facade/hyperband_facade/#smac.facade.hyperband_facade.HyperbandFacade.validate--returns","title":"Returns","text":"<p>cost : float | list[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"Validates a configuration on seeds different from the ones used in the optimization process and on the\n    highest budget (if budget type is real-valued).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n    instances : list[str] | None, defaults to None\n        Which instances to validate. If None, all instances specified in the scenario are used.\n        In case that the budget type is real-valued, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | list[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    return self._optimizer.validate(config, seed=seed)\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/","title":"Hyperparameter optimization facade","text":""},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade","title":"smac.facade.hyperparameter_optimization_facade","text":""},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade","title":"HyperparameterOptimizationFacade","text":"<pre><code>HyperparameterOptimizationFacade(\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    acquisition_maximizer: (\n        AbstractAcquisitionMaximizer | None\n    ) = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    runhistory_encoder: (\n        AbstractRunHistoryEncoder | None\n    ) = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractFacade</code></p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    acquisition_maximizer: AbstractAcquisitionMaximizer | None = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    runhistory_encoder: AbstractRunHistoryEncoder | None = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: int | Path | Literal[False] | None = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None,\n):\n    setup_logging(logging_level)\n\n    if callbacks is None:\n        callbacks = []\n\n    if model is None:\n        model = self.get_model(scenario)\n\n    if acquisition_function is None:\n        acquisition_function = self.get_acquisition_function(scenario)\n\n    if acquisition_maximizer is None:\n        acquisition_maximizer = self.get_acquisition_maximizer(scenario)\n\n    if initial_design is None:\n        initial_design = self.get_initial_design(scenario)\n\n    if random_design is None:\n        random_design = self.get_random_design(scenario)\n\n    if intensifier is None:\n        intensifier = self.get_intensifier(scenario)\n\n    if multi_objective_algorithm is None and scenario.count_objectives() &gt; 1:\n        multi_objective_algorithm = self.get_multi_objective_algorithm(scenario=scenario)\n\n    if runhistory_encoder is None:\n        runhistory_encoder = self.get_runhistory_encoder(scenario)\n\n    if config_selector is None:\n        config_selector = self.get_config_selector(scenario)\n\n    # Initialize empty stats and runhistory object\n    runhistory = RunHistory(multi_objective_algorithm=multi_objective_algorithm)\n\n    # Set the seed for configuration space\n    scenario.configspace.seed(scenario.seed)\n\n    # Set variables globally\n    self._scenario = scenario\n    self._model = model\n    self._acquisition_function = acquisition_function\n    self._acquisition_maximizer = acquisition_maximizer\n    self._initial_design = initial_design\n    self._random_design = random_design\n    self._intensifier = intensifier\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._runhistory = runhistory\n    self._runhistory_encoder = runhistory_encoder\n    self._config_selector = config_selector\n    self._callbacks = callbacks\n    self._overwrite = overwrite\n\n    # Prepare the algorithm executer\n    runner: AbstractRunner\n    if isinstance(target_function, AbstractRunner):\n        runner = target_function\n    elif isinstance(target_function, str):\n        runner = TargetFunctionScriptRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n    else:\n        runner = TargetFunctionRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n\n    # In case of multiple jobs, we need to wrap the runner again using DaskParallelRunner\n    if (n_workers := scenario.n_workers) &gt; 1 or dask_client is not None:\n        if dask_client is not None and n_workers &gt; 1:\n            logger.warning(\n                \"Provided `dask_client`. Ignore `scenario.n_workers`, directly set `n_workers` in `dask_client`.\"\n            )\n        else:\n            available_workers = joblib.cpu_count()\n            if n_workers &gt; available_workers:\n                logger.info(f\"Workers are reduced to {n_workers}.\")\n                n_workers = available_workers\n\n        # We use a dask runner for parallelization\n        runner = DaskParallelRunner(single_worker=runner, dask_client=dask_client)\n\n    # Set the runner to access it globally\n    self._runner = runner\n\n    # Adding dependencies of the components\n    self._update_dependencies()\n\n    # We have to update our meta data (basically arguments of the components)\n    self._scenario._set_meta(self.meta)\n\n    # We have to validate if the object compositions are correct and actually make sense\n    self._validate()\n\n    # Finally we configure our optimizer\n    self._optimizer = self._get_optimizer()\n    assert self._optimizer\n\n    # Register callbacks here\n    for callback in callbacks:\n        self._optimizer.register_callback(callback)\n\n    # Additionally, we register the runhistory callback from the intensifier to efficiently update our incumbent\n    # every time new information are available\n    self._optimizer.register_callback(self._intensifier.get_callback(), index=0)\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Generates a hash based on all components of the facade. This is used for the run name or to determine whether a run should be continued or not.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.optimizer","title":"optimizer  <code>property</code>","text":"<pre><code>optimizer: SMBO\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The runhistory which is filled with all trials during the optimization process.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.scenario","title":"scenario  <code>property</code>","text":"<pre><code>scenario: Scenario\n</code></pre> <p>The scenario object which holds all environment information.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\"\"\"\n    return self._optimizer.ask()\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_acquisition_function","title":"get_acquisition_function  <code>staticmethod</code>","text":"<pre><code>get_acquisition_function(\n    scenario: Scenario, *, xi: float = 0.0\n) -&gt; EI\n</code></pre> <p>Returns an Expected Improvement acquisition function.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_acquisition_function--parameters","title":"Parameters","text":"<p>scenario : Scenario xi : float, defaults to 0.0     Controls the balance between exploration and exploitation of the     acquisition function.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_function(  # type: ignore\n    scenario: Scenario,\n    *,\n    xi: float = 0.0,\n) -&gt; EI:\n    \"\"\"Returns an Expected Improvement acquisition function.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    xi : float, defaults to 0.0\n        Controls the balance between exploration and exploitation of the\n        acquisition function.\n    \"\"\"\n    return EI(xi=xi, log=True)\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_acquisition_maximizer","title":"get_acquisition_maximizer  <code>staticmethod</code>","text":"<pre><code>get_acquisition_maximizer(\n    scenario: Scenario,\n    *,\n    challengers: int = 10000,\n    local_search_iterations: int = 10\n) -&gt; LocalAndSortedRandomSearch\n</code></pre> <p>Returns local and sorted random search as acquisition maximizer.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_acquisition_maximizer--warning","title":"Warning","text":"<p>If you experience RAM issues, try to reduce the number of challengers.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_acquisition_maximizer--parameters","title":"Parameters","text":"<p>challengers : int, defaults to 10000     Number of challengers. local_search_iterations: int, defaults to 10     Number of local search iterations.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_maximizer(  # type: ignore\n    scenario: Scenario,\n    *,\n    challengers: int = 10000,\n    local_search_iterations: int = 10,\n) -&gt; LocalAndSortedRandomSearch:\n    \"\"\"Returns local and sorted random search as acquisition maximizer.\n\n    Warning\n    -------\n    If you experience RAM issues, try to reduce the number of challengers.\n\n    Parameters\n    ----------\n    challengers : int, defaults to 10000\n        Number of challengers.\n    local_search_iterations: int, defaults to 10\n        Number of local search iterations.\n    \"\"\"\n    optimizer = LocalAndSortedRandomSearch(\n        scenario.configspace,\n        challengers=challengers,\n        local_search_iterations=local_search_iterations,\n        seed=scenario.seed,\n    )\n\n    return optimizer\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_config_selector","title":"get_config_selector  <code>staticmethod</code>","text":"<pre><code>get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16\n) -&gt; ConfigSelector\n</code></pre> <p>Returns the default configuration selector.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\ndef get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n) -&gt; ConfigSelector:\n    \"\"\"Returns the default configuration selector.\"\"\"\n    return ConfigSelector(scenario, retrain_after=retrain_after, retries=retries)\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_initial_design","title":"get_initial_design  <code>staticmethod</code>","text":"<pre><code>get_initial_design(\n    scenario: Scenario,\n    *,\n    n_configs: int | None = None,\n    n_configs_per_hyperparamter: int = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] | None = None\n) -&gt; SobolInitialDesign\n</code></pre> <p>Returns a Sobol design instance.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_initial_design--parameters","title":"Parameters","text":"<p>scenario : Scenario n_configs : int | None, defaults to None     Number of initial configurations (disables the arguments <code>n_configs_per_hyperparameter</code>). n_configs_per_hyperparameter: int, defaults to 10     Number of initial configurations per hyperparameter. For example, if my configuration space covers five     hyperparameters and <code>n_configs_per_hyperparameter</code> is set to 10, then 50 initial configurations will be     samples. max_ratio: float, defaults to 0.25     Use at most <code>scenario.n_trials</code> * <code>max_ratio</code> number of configurations in the initial design.     Additional configurations are not affected by this parameter. additional_configs: list[Configuration], defaults to []     Adds additional configurations to the initial design.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_initial_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    n_configs: int | None = None,\n    n_configs_per_hyperparamter: int = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] | None = None,\n) -&gt; SobolInitialDesign:\n    \"\"\"Returns a Sobol design instance.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    n_configs : int | None, defaults to None\n        Number of initial configurations (disables the arguments ``n_configs_per_hyperparameter``).\n    n_configs_per_hyperparameter: int, defaults to 10\n        Number of initial configurations per hyperparameter. For example, if my configuration space covers five\n        hyperparameters and ``n_configs_per_hyperparameter`` is set to 10, then 50 initial configurations will be\n        samples.\n    max_ratio: float, defaults to 0.25\n        Use at most ``scenario.n_trials`` * ``max_ratio`` number of configurations in the initial design.\n        Additional configurations are not affected by this parameter.\n    additional_configs: list[Configuration], defaults to []\n        Adds additional configurations to the initial design.\n    \"\"\"\n    return SobolInitialDesign(\n        scenario=scenario,\n        n_configs=n_configs,\n        n_configs_per_hyperparameter=n_configs_per_hyperparamter,\n        max_ratio=max_ratio,\n        additional_configs=additional_configs,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_intensifier","title":"get_intensifier  <code>staticmethod</code>","text":"<pre><code>get_intensifier(\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 3,\n    max_incumbents: int = 10\n) -&gt; Intensifier\n</code></pre> <p>Returns <code>Intensifier</code> as intensifier. Uses the default configuration for <code>race_against</code>.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_intensifier--parameters","title":"Parameters","text":"<p>scenario : Scenario max_config_calls : int, defaults to 3     Maximum number of configuration evaluations. Basically, how many instance-seed keys should be max evaluated     for a configuration. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_intensifier(  # type: ignore\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 3,\n    max_incumbents: int = 10,\n) -&gt; Intensifier:\n    \"\"\"Returns ``Intensifier`` as intensifier. Uses the default configuration for ``race_against``.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    max_config_calls : int, defaults to 3\n        Maximum number of configuration evaluations. Basically, how many instance-seed keys should be max evaluated\n        for a configuration.\n    max_incumbents : int, defaults to 10\n        How many incumbents to keep track of in the case of multi-objective.\n    \"\"\"\n    return Intensifier(\n        scenario=scenario,\n        max_config_calls=max_config_calls,\n        max_incumbents=max_incumbents,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_model","title":"get_model  <code>staticmethod</code>","text":"<pre><code>get_model(\n    scenario: Scenario,\n    *,\n    n_trees: int = 10,\n    ratio_features: float = 1.0,\n    min_samples_split: int = 2,\n    min_samples_leaf: int = 1,\n    max_depth: int = 2**20,\n    bootstrapping: bool = True\n) -&gt; RandomForest\n</code></pre> <p>Returns a random forest as surrogate model.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_model--parameters","title":"Parameters","text":"<p>n_trees : int, defaults to 10     The number of trees in the random forest. ratio_features : float, defaults to 5.0 / 6.0     The ratio of features that are considered for splitting. min_samples_split : int, defaults to 3     The minimum number of data points to perform a split. min_samples_leaf : int, defaults to 3     The minimum number of data points in a leaf. max_depth : int, defaults to 20     The maximum depth of a single tree. bootstrapping : bool, defaults to True     Enables bootstrapping.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_model(  # type: ignore\n    scenario: Scenario,\n    *,\n    n_trees: int = 10,\n    ratio_features: float = 1.0,\n    min_samples_split: int = 2,\n    min_samples_leaf: int = 1,\n    max_depth: int = 2**20,\n    bootstrapping: bool = True,\n) -&gt; RandomForest:\n    \"\"\"Returns a random forest as surrogate model.\n\n    Parameters\n    ----------\n    n_trees : int, defaults to 10\n        The number of trees in the random forest.\n    ratio_features : float, defaults to 5.0 / 6.0\n        The ratio of features that are considered for splitting.\n    min_samples_split : int, defaults to 3\n        The minimum number of data points to perform a split.\n    min_samples_leaf : int, defaults to 3\n        The minimum number of data points in a leaf.\n    max_depth : int, defaults to 20\n        The maximum depth of a single tree.\n    bootstrapping : bool, defaults to True\n        Enables bootstrapping.\n    \"\"\"\n    return RandomForest(\n        log_y=True,\n        n_trees=n_trees,\n        bootstrapping=bootstrapping,\n        ratio_features=ratio_features,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        max_depth=max_depth,\n        configspace=scenario.configspace,\n        instance_features=scenario.instance_features,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_multi_objective_algorithm","title":"get_multi_objective_algorithm  <code>staticmethod</code>","text":"<pre><code>get_multi_objective_algorithm(\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None\n) -&gt; MeanAggregationStrategy\n</code></pre> <p>Returns the mean aggregation strategy for the multi-objective algorithm.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_multi_objective_algorithm--parameters","title":"Parameters","text":"<p>scenario : Scenario objective_weights : list[float] | None, defaults to None     Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of     objectives.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_multi_objective_algorithm(  # type: ignore\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None,\n) -&gt; MeanAggregationStrategy:\n    \"\"\"Returns the mean aggregation strategy for the multi-objective algorithm.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    objective_weights : list[float] | None, defaults to None\n        Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of\n        objectives.\n    \"\"\"\n    return MeanAggregationStrategy(\n        scenario=scenario,\n        objective_weights=objective_weights,\n    )\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_random_design","title":"get_random_design  <code>staticmethod</code>","text":"<pre><code>get_random_design(\n    scenario: Scenario, *, probability: float = 0.2\n) -&gt; ProbabilityRandomDesign\n</code></pre> <p>Returns <code>ProbabilityRandomDesign</code> for interleaving configurations.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_random_design--parameters","title":"Parameters","text":"<p>probability : float, defaults to 0.2     Probability that a configuration will be drawn at random.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_random_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    probability: float = 0.2,\n) -&gt; ProbabilityRandomDesign:\n    \"\"\"Returns ``ProbabilityRandomDesign`` for interleaving configurations.\n\n    Parameters\n    ----------\n    probability : float, defaults to 0.2\n        Probability that a configuration will be drawn at random.\n    \"\"\"\n    return ProbabilityRandomDesign(probability=probability, seed=scenario.seed)\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.get_runhistory_encoder","title":"get_runhistory_encoder  <code>staticmethod</code>","text":"<pre><code>get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; RunHistoryLogScaledEncoder\n</code></pre> <p>Returns a log scaled runhistory encoder. That means that costs are log scaled before training the surrogate model.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_runhistory_encoder(  # type: ignore\n    scenario: Scenario,\n) -&gt; RunHistoryLogScaledEncoder:\n    \"\"\"Returns a log scaled runhistory encoder. That means that costs are log scaled before\n    training the surrogate model.\n    \"\"\"\n    return RunHistoryLogScaledEncoder(scenario)\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Optimizes the configuration of the algorithm.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     We first note that this argument is valid only dask_runner!     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     Best found configuration.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"\n    Optimizes the configuration of the algorithm.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        We first note that this argument is valid only dask_runner!\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        Best found configuration.\n    \"\"\"\n    incumbents = None\n    if isinstance(data_to_scatter, dict) and len(data_to_scatter) == 0:\n        raise ValueError(\"data_to_scatter must be None or dict with some elements, but got an empty dict.\")\n\n    try:\n        incumbents = self._optimizer.optimize(data_to_scatter=data_to_scatter)\n    finally:\n        self._optimizer.save()\n\n    return incumbents\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the intensifier.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.tell--parameters","title":"Parameters","text":"<p>info: TrialInfo     Describes the trial from which to process the results. value: TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def tell(self, info: TrialInfo, value: TrialValue, save: bool = True) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the intensifier.\n\n    Parameters\n    ----------\n    info: TrialInfo\n        Describes the trial from which to process the results.\n    value: TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    return self._optimizer.tell(info, value, save=save)\n</code></pre>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | list[float]\n</code></pre> <p>Validates a configuration on seeds different from the ones used in the optimization process and on the highest budget (if budget type is real-valued).</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate instances : list[str] | None, defaults to None     Which instances to validate. If None, all instances specified in the scenario are used.     In case that the budget type is real-valued, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/facade/hyperparameter_optimization_facade/#smac.facade.hyperparameter_optimization_facade.HyperparameterOptimizationFacade.validate--returns","title":"Returns","text":"<p>cost : float | list[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"Validates a configuration on seeds different from the ones used in the optimization process and on the\n    highest budget (if budget type is real-valued).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n    instances : list[str] | None, defaults to None\n        Which instances to validate. If None, all instances specified in the scenario are used.\n        In case that the budget type is real-valued, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | list[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    return self._optimizer.validate(config, seed=seed)\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/","title":"Multi fidelity facade","text":""},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade","title":"smac.facade.multi_fidelity_facade","text":""},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade","title":"MultiFidelityFacade","text":"<pre><code>MultiFidelityFacade(\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    acquisition_maximizer: (\n        AbstractAcquisitionMaximizer | None\n    ) = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    runhistory_encoder: (\n        AbstractRunHistoryEncoder | None\n    ) = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None\n)\n</code></pre> <p>               Bases: <code>HyperparameterOptimizationFacade</code></p> <p>This facade configures SMAC in a multi-fidelity setting.</p> <p>Warning</p> <p><code>smac.main.config_selector.ConfigSelector</code> contains the <code>min_trials</code> parameter. This parameter determines how many samples are required to train the surrogate model. If budgets are involved, the highest budgets are checked first. For example, if min_trials is three, but we find only two trials in the runhistory for the highest budget, we will use trials of a lower budget instead.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    acquisition_maximizer: AbstractAcquisitionMaximizer | None = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    runhistory_encoder: AbstractRunHistoryEncoder | None = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: int | Path | Literal[False] | None = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None,\n):\n    setup_logging(logging_level)\n\n    if callbacks is None:\n        callbacks = []\n\n    if model is None:\n        model = self.get_model(scenario)\n\n    if acquisition_function is None:\n        acquisition_function = self.get_acquisition_function(scenario)\n\n    if acquisition_maximizer is None:\n        acquisition_maximizer = self.get_acquisition_maximizer(scenario)\n\n    if initial_design is None:\n        initial_design = self.get_initial_design(scenario)\n\n    if random_design is None:\n        random_design = self.get_random_design(scenario)\n\n    if intensifier is None:\n        intensifier = self.get_intensifier(scenario)\n\n    if multi_objective_algorithm is None and scenario.count_objectives() &gt; 1:\n        multi_objective_algorithm = self.get_multi_objective_algorithm(scenario=scenario)\n\n    if runhistory_encoder is None:\n        runhistory_encoder = self.get_runhistory_encoder(scenario)\n\n    if config_selector is None:\n        config_selector = self.get_config_selector(scenario)\n\n    # Initialize empty stats and runhistory object\n    runhistory = RunHistory(multi_objective_algorithm=multi_objective_algorithm)\n\n    # Set the seed for configuration space\n    scenario.configspace.seed(scenario.seed)\n\n    # Set variables globally\n    self._scenario = scenario\n    self._model = model\n    self._acquisition_function = acquisition_function\n    self._acquisition_maximizer = acquisition_maximizer\n    self._initial_design = initial_design\n    self._random_design = random_design\n    self._intensifier = intensifier\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._runhistory = runhistory\n    self._runhistory_encoder = runhistory_encoder\n    self._config_selector = config_selector\n    self._callbacks = callbacks\n    self._overwrite = overwrite\n\n    # Prepare the algorithm executer\n    runner: AbstractRunner\n    if isinstance(target_function, AbstractRunner):\n        runner = target_function\n    elif isinstance(target_function, str):\n        runner = TargetFunctionScriptRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n    else:\n        runner = TargetFunctionRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n\n    # In case of multiple jobs, we need to wrap the runner again using DaskParallelRunner\n    if (n_workers := scenario.n_workers) &gt; 1 or dask_client is not None:\n        if dask_client is not None and n_workers &gt; 1:\n            logger.warning(\n                \"Provided `dask_client`. Ignore `scenario.n_workers`, directly set `n_workers` in `dask_client`.\"\n            )\n        else:\n            available_workers = joblib.cpu_count()\n            if n_workers &gt; available_workers:\n                logger.info(f\"Workers are reduced to {n_workers}.\")\n                n_workers = available_workers\n\n        # We use a dask runner for parallelization\n        runner = DaskParallelRunner(single_worker=runner, dask_client=dask_client)\n\n    # Set the runner to access it globally\n    self._runner = runner\n\n    # Adding dependencies of the components\n    self._update_dependencies()\n\n    # We have to update our meta data (basically arguments of the components)\n    self._scenario._set_meta(self.meta)\n\n    # We have to validate if the object compositions are correct and actually make sense\n    self._validate()\n\n    # Finally we configure our optimizer\n    self._optimizer = self._get_optimizer()\n    assert self._optimizer\n\n    # Register callbacks here\n    for callback in callbacks:\n        self._optimizer.register_callback(callback)\n\n    # Additionally, we register the runhistory callback from the intensifier to efficiently update our incumbent\n    # every time new information are available\n    self._optimizer.register_callback(self._intensifier.get_callback(), index=0)\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Generates a hash based on all components of the facade. This is used for the run name or to determine whether a run should be continued or not.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.optimizer","title":"optimizer  <code>property</code>","text":"<pre><code>optimizer: SMBO\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The runhistory which is filled with all trials during the optimization process.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.scenario","title":"scenario  <code>property</code>","text":"<pre><code>scenario: Scenario\n</code></pre> <p>The scenario object which holds all environment information.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\"\"\"\n    return self._optimizer.ask()\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_acquisition_function","title":"get_acquisition_function  <code>staticmethod</code>","text":"<pre><code>get_acquisition_function(\n    scenario: Scenario, *, xi: float = 0.0\n) -&gt; EI\n</code></pre> <p>Returns an Expected Improvement acquisition function.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_acquisition_function--parameters","title":"Parameters","text":"<p>scenario : Scenario xi : float, defaults to 0.0     Controls the balance between exploration and exploitation of the     acquisition function.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_function(  # type: ignore\n    scenario: Scenario,\n    *,\n    xi: float = 0.0,\n) -&gt; EI:\n    \"\"\"Returns an Expected Improvement acquisition function.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    xi : float, defaults to 0.0\n        Controls the balance between exploration and exploitation of the\n        acquisition function.\n    \"\"\"\n    return EI(xi=xi, log=True)\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_acquisition_maximizer","title":"get_acquisition_maximizer  <code>staticmethod</code>","text":"<pre><code>get_acquisition_maximizer(\n    scenario: Scenario,\n    *,\n    challengers: int = 10000,\n    local_search_iterations: int = 10\n) -&gt; LocalAndSortedRandomSearch\n</code></pre> <p>Returns local and sorted random search as acquisition maximizer.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_acquisition_maximizer--warning","title":"Warning","text":"<p>If you experience RAM issues, try to reduce the number of challengers.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_acquisition_maximizer--parameters","title":"Parameters","text":"<p>challengers : int, defaults to 10000     Number of challengers. local_search_iterations: int, defaults to 10     Number of local search iterations.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_maximizer(  # type: ignore\n    scenario: Scenario,\n    *,\n    challengers: int = 10000,\n    local_search_iterations: int = 10,\n) -&gt; LocalAndSortedRandomSearch:\n    \"\"\"Returns local and sorted random search as acquisition maximizer.\n\n    Warning\n    -------\n    If you experience RAM issues, try to reduce the number of challengers.\n\n    Parameters\n    ----------\n    challengers : int, defaults to 10000\n        Number of challengers.\n    local_search_iterations: int, defaults to 10\n        Number of local search iterations.\n    \"\"\"\n    optimizer = LocalAndSortedRandomSearch(\n        scenario.configspace,\n        challengers=challengers,\n        local_search_iterations=local_search_iterations,\n        seed=scenario.seed,\n    )\n\n    return optimizer\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_config_selector","title":"get_config_selector  <code>staticmethod</code>","text":"<pre><code>get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16\n) -&gt; ConfigSelector\n</code></pre> <p>Returns the default configuration selector.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\ndef get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n) -&gt; ConfigSelector:\n    \"\"\"Returns the default configuration selector.\"\"\"\n    return ConfigSelector(scenario, retrain_after=retrain_after, retries=retries)\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_initial_design","title":"get_initial_design  <code>staticmethod</code>","text":"<pre><code>get_initial_design(\n    scenario: Scenario,\n    *,\n    n_configs: int | None = None,\n    n_configs_per_hyperparamter: int = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None\n) -&gt; RandomInitialDesign\n</code></pre> <p>Returns a random initial design.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_initial_design--parameters","title":"Parameters","text":"<p>scenario : Scenario n_configs : int | None, defaults to None     Number of initial configurations (disables the arguments <code>n_configs_per_hyperparameter</code>). n_configs_per_hyperparameter: int, defaults to 10     Number of initial configurations per hyperparameter. For example, if my configuration space covers five     hyperparameters and <code>n_configs_per_hyperparameter</code> is set to 10, then 50 initial configurations will be     samples. max_ratio: float, defaults to 0.25     Use at most <code>scenario.n_trials</code> * <code>max_ratio</code> number of configurations in the initial design.     Additional configurations are not affected by this parameter. additional_configs: list[Configuration], defaults to []     Adds additional configurations to the initial design.</p> Source code in <code>smac/facade/multi_fidelity_facade.py</code> <pre><code>@staticmethod\ndef get_initial_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    n_configs: int | None = None,\n    n_configs_per_hyperparamter: int = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n) -&gt; RandomInitialDesign:\n    \"\"\"Returns a random initial design.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    n_configs : int | None, defaults to None\n        Number of initial configurations (disables the arguments ``n_configs_per_hyperparameter``).\n    n_configs_per_hyperparameter: int, defaults to 10\n        Number of initial configurations per hyperparameter. For example, if my configuration space covers five\n        hyperparameters and ``n_configs_per_hyperparameter`` is set to 10, then 50 initial configurations will be\n        samples.\n    max_ratio: float, defaults to 0.25\n        Use at most ``scenario.n_trials`` * ``max_ratio`` number of configurations in the initial design.\n        Additional configurations are not affected by this parameter.\n    additional_configs: list[Configuration], defaults to []\n        Adds additional configurations to the initial design.\n    \"\"\"\n    if additional_configs is None:\n        additional_configs = []\n    return RandomInitialDesign(\n        scenario=scenario,\n        n_configs=n_configs,\n        n_configs_per_hyperparameter=n_configs_per_hyperparamter,\n        max_ratio=max_ratio,\n        additional_configs=additional_configs,\n    )\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_intensifier","title":"get_intensifier  <code>staticmethod</code>","text":"<pre><code>get_intensifier(\n    scenario: Scenario,\n    *,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\"\n) -&gt; Hyperband\n</code></pre> <p>Returns a Hyperband intensifier instance. Budgets are supported.</p> int, defaults to 3 <p>Input that controls the proportion of configurations discarded in each round of Successive Halving.</p> <p>n_seeds : int, defaults to 1     How many seeds to use for each instance. instance_seed_order : str, defaults to \"shuffle_once\"     How to order the instance-seed pairs. Can be set to:     * None: No shuffling at all and use the instance-seed order provided by the user.     * \"shuffle_once\": Shuffle the instance-seed keys once and use the same order across all runs.     * \"shuffle\": Shuffles the instance-seed keys for each bracket individually. incumbent_selection : str, defaults to \"any_budget\"     How to select the incumbent when using budgets. Can be set to:     * \"any_budget\": Incumbent is the best on any budget, i.e., the best performance regardless of budget.     * \"highest_observed_budget\": Incumbent is the best in the highest budget run so far.     refer to <code>runhistory.get_trials</code> for more details. Crucially, if true, then a     for a given config-instance-seed, only the highest (so far executed) budget is used for     comparison against the incumbent. Notice, that if the highest observed budget is smaller     than the highest budget of the incumbent, the configuration will be queued again to     be intensified again.     * \"highest_budget\": Incumbent is selected only based on the absolute highest budget     available only. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective.</p> Source code in <code>smac/facade/multi_fidelity_facade.py</code> <pre><code>@staticmethod\ndef get_intensifier(  # type: ignore\n    scenario: Scenario,\n    *,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\",\n) -&gt; Hyperband:\n    \"\"\"Returns a Hyperband intensifier instance. Budgets are supported.\n\n    eta : int, defaults to 3\n        Input that controls the proportion of configurations discarded in each round of Successive Halving.\n    n_seeds : int, defaults to 1\n        How many seeds to use for each instance.\n    instance_seed_order : str, defaults to \"shuffle_once\"\n        How to order the instance-seed pairs. Can be set to:\n        * None: No shuffling at all and use the instance-seed order provided by the user.\n        * \"shuffle_once\": Shuffle the instance-seed keys once and use the same order across all runs.\n        * \"shuffle\": Shuffles the instance-seed keys for each bracket individually.\n    incumbent_selection : str, defaults to \"any_budget\"\n        How to select the incumbent when using budgets. Can be set to:\n        * \"any_budget\": Incumbent is the best on any budget, i.e., the best performance regardless of budget.\n        * \"highest_observed_budget\": Incumbent is the best in the highest budget run so far.\n        refer to `runhistory.get_trials` for more details. Crucially, if true, then a\n        for a given config-instance-seed, only the highest (so far executed) budget is used for\n        comparison against the incumbent. Notice, that if the highest observed budget is smaller\n        than the highest budget of the incumbent, the configuration will be queued again to\n        be intensified again.\n        * \"highest_budget\": Incumbent is selected only based on the absolute highest budget\n        available only.\n    max_incumbents : int, defaults to 10\n        How many incumbents to keep track of in the case of multi-objective.\n    \"\"\"\n    return Hyperband(\n        scenario=scenario,\n        eta=eta,\n        n_seeds=n_seeds,\n        instance_seed_order=instance_seed_order,\n        max_incumbents=max_incumbents,\n        incumbent_selection=incumbent_selection,\n    )\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_model","title":"get_model  <code>staticmethod</code>","text":"<pre><code>get_model(\n    scenario: Scenario,\n    *,\n    n_trees: int = 10,\n    ratio_features: float = 1.0,\n    min_samples_split: int = 2,\n    min_samples_leaf: int = 1,\n    max_depth: int = 2**20,\n    bootstrapping: bool = True\n) -&gt; RandomForest\n</code></pre> <p>Returns a random forest as surrogate model.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_model--parameters","title":"Parameters","text":"<p>n_trees : int, defaults to 10     The number of trees in the random forest. ratio_features : float, defaults to 5.0 / 6.0     The ratio of features that are considered for splitting. min_samples_split : int, defaults to 3     The minimum number of data points to perform a split. min_samples_leaf : int, defaults to 3     The minimum number of data points in a leaf. max_depth : int, defaults to 20     The maximum depth of a single tree. bootstrapping : bool, defaults to True     Enables bootstrapping.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_model(  # type: ignore\n    scenario: Scenario,\n    *,\n    n_trees: int = 10,\n    ratio_features: float = 1.0,\n    min_samples_split: int = 2,\n    min_samples_leaf: int = 1,\n    max_depth: int = 2**20,\n    bootstrapping: bool = True,\n) -&gt; RandomForest:\n    \"\"\"Returns a random forest as surrogate model.\n\n    Parameters\n    ----------\n    n_trees : int, defaults to 10\n        The number of trees in the random forest.\n    ratio_features : float, defaults to 5.0 / 6.0\n        The ratio of features that are considered for splitting.\n    min_samples_split : int, defaults to 3\n        The minimum number of data points to perform a split.\n    min_samples_leaf : int, defaults to 3\n        The minimum number of data points in a leaf.\n    max_depth : int, defaults to 20\n        The maximum depth of a single tree.\n    bootstrapping : bool, defaults to True\n        Enables bootstrapping.\n    \"\"\"\n    return RandomForest(\n        log_y=True,\n        n_trees=n_trees,\n        bootstrapping=bootstrapping,\n        ratio_features=ratio_features,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        max_depth=max_depth,\n        configspace=scenario.configspace,\n        instance_features=scenario.instance_features,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_multi_objective_algorithm","title":"get_multi_objective_algorithm  <code>staticmethod</code>","text":"<pre><code>get_multi_objective_algorithm(\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None\n) -&gt; MeanAggregationStrategy\n</code></pre> <p>Returns the mean aggregation strategy for the multi-objective algorithm.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_multi_objective_algorithm--parameters","title":"Parameters","text":"<p>scenario : Scenario objective_weights : list[float] | None, defaults to None     Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of     objectives.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_multi_objective_algorithm(  # type: ignore\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None,\n) -&gt; MeanAggregationStrategy:\n    \"\"\"Returns the mean aggregation strategy for the multi-objective algorithm.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    objective_weights : list[float] | None, defaults to None\n        Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of\n        objectives.\n    \"\"\"\n    return MeanAggregationStrategy(\n        scenario=scenario,\n        objective_weights=objective_weights,\n    )\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_random_design","title":"get_random_design  <code>staticmethod</code>","text":"<pre><code>get_random_design(\n    scenario: Scenario, *, probability: float = 0.2\n) -&gt; ProbabilityRandomDesign\n</code></pre> <p>Returns <code>ProbabilityRandomDesign</code> for interleaving configurations.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_random_design--parameters","title":"Parameters","text":"<p>probability : float, defaults to 0.2     Probability that a configuration will be drawn at random.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_random_design(  # type: ignore\n    scenario: Scenario,\n    *,\n    probability: float = 0.2,\n) -&gt; ProbabilityRandomDesign:\n    \"\"\"Returns ``ProbabilityRandomDesign`` for interleaving configurations.\n\n    Parameters\n    ----------\n    probability : float, defaults to 0.2\n        Probability that a configuration will be drawn at random.\n    \"\"\"\n    return ProbabilityRandomDesign(probability=probability, seed=scenario.seed)\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.get_runhistory_encoder","title":"get_runhistory_encoder  <code>staticmethod</code>","text":"<pre><code>get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; RunHistoryLogScaledEncoder\n</code></pre> <p>Returns a log scaled runhistory encoder. That means that costs are log scaled before training the surrogate model.</p> Source code in <code>smac/facade/hyperparameter_optimization_facade.py</code> <pre><code>@staticmethod\ndef get_runhistory_encoder(  # type: ignore\n    scenario: Scenario,\n) -&gt; RunHistoryLogScaledEncoder:\n    \"\"\"Returns a log scaled runhistory encoder. That means that costs are log scaled before\n    training the surrogate model.\n    \"\"\"\n    return RunHistoryLogScaledEncoder(scenario)\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Optimizes the configuration of the algorithm.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     We first note that this argument is valid only dask_runner!     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     Best found configuration.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"\n    Optimizes the configuration of the algorithm.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        We first note that this argument is valid only dask_runner!\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        Best found configuration.\n    \"\"\"\n    incumbents = None\n    if isinstance(data_to_scatter, dict) and len(data_to_scatter) == 0:\n        raise ValueError(\"data_to_scatter must be None or dict with some elements, but got an empty dict.\")\n\n    try:\n        incumbents = self._optimizer.optimize(data_to_scatter=data_to_scatter)\n    finally:\n        self._optimizer.save()\n\n    return incumbents\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the intensifier.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.tell--parameters","title":"Parameters","text":"<p>info: TrialInfo     Describes the trial from which to process the results. value: TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def tell(self, info: TrialInfo, value: TrialValue, save: bool = True) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the intensifier.\n\n    Parameters\n    ----------\n    info: TrialInfo\n        Describes the trial from which to process the results.\n    value: TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    return self._optimizer.tell(info, value, save=save)\n</code></pre>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | list[float]\n</code></pre> <p>Validates a configuration on seeds different from the ones used in the optimization process and on the highest budget (if budget type is real-valued).</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate instances : list[str] | None, defaults to None     Which instances to validate. If None, all instances specified in the scenario are used.     In case that the budget type is real-valued, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/facade/multi_fidelity_facade/#smac.facade.multi_fidelity_facade.MultiFidelityFacade.validate--returns","title":"Returns","text":"<p>cost : float | list[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"Validates a configuration on seeds different from the ones used in the optimization process and on the\n    highest budget (if budget type is real-valued).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n    instances : list[str] | None, defaults to None\n        Which instances to validate. If None, all instances specified in the scenario are used.\n        In case that the budget type is real-valued, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | list[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    return self._optimizer.validate(config, seed=seed)\n</code></pre>"},{"location":"api/smac/facade/random_facade/","title":"Random facade","text":""},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade","title":"smac.facade.random_facade","text":""},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade","title":"RandomFacade","text":"<pre><code>RandomFacade(\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: (\n        AbstractAcquisitionFunction | None\n    ) = None,\n    acquisition_maximizer: (\n        AbstractAcquisitionMaximizer | None\n    ) = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    runhistory_encoder: (\n        AbstractRunHistoryEncoder | None\n    ) = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None\n)\n</code></pre> <p>               Bases: <code>AbstractFacade</code></p> <p>Facade to use Random Online Aggressive Racing (ROAR).</p> <p>Aggressive Racing: When we have a new configuration \u03b8, we want to compare it to the current best configuration, the incumbent \u03b8. ROAR uses the 'racing' approach, where we run few times for unpromising \u03b8 and many times for promising configurations. Once we are confident enough that \u03b8 is better than \u03b8, we update the incumbent \u03b8* \u27f5 \u03b8. <code>Aggressive</code> means rejecting low-performing configurations very early, often after a single run. This together is called <code>aggressive racing</code>.</p> <p>ROAR Loop: The main ROAR loop looks as follows:</p> <ol> <li>Select a configuration \u03b8 uniformly at random.</li> <li>Compare \u03b8 to incumbent \u03b8* online (one \u03b8 at a time):</li> <li>Reject/accept \u03b8 with <code>aggressive racing</code></li> </ol> <p>Setup: Uses a random model and random search for the optimization of the acquisition function.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade--note","title":"Note","text":"<p>The surrogate model and the acquisition function is not used during the optimization and therefore replaced by dummies.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable | str | AbstractRunner,\n    *,\n    model: AbstractModel | None = None,\n    acquisition_function: AbstractAcquisitionFunction | None = None,\n    acquisition_maximizer: AbstractAcquisitionMaximizer | None = None,\n    initial_design: AbstractInitialDesign | None = None,\n    random_design: AbstractRandomDesign | None = None,\n    intensifier: AbstractIntensifier | None = None,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    runhistory_encoder: AbstractRunHistoryEncoder | None = None,\n    config_selector: ConfigSelector | None = None,\n    logging_level: int | Path | Literal[False] | None = None,\n    callbacks: list[Callback] = None,\n    overwrite: bool = False,\n    dask_client: Client | None = None,\n):\n    setup_logging(logging_level)\n\n    if callbacks is None:\n        callbacks = []\n\n    if model is None:\n        model = self.get_model(scenario)\n\n    if acquisition_function is None:\n        acquisition_function = self.get_acquisition_function(scenario)\n\n    if acquisition_maximizer is None:\n        acquisition_maximizer = self.get_acquisition_maximizer(scenario)\n\n    if initial_design is None:\n        initial_design = self.get_initial_design(scenario)\n\n    if random_design is None:\n        random_design = self.get_random_design(scenario)\n\n    if intensifier is None:\n        intensifier = self.get_intensifier(scenario)\n\n    if multi_objective_algorithm is None and scenario.count_objectives() &gt; 1:\n        multi_objective_algorithm = self.get_multi_objective_algorithm(scenario=scenario)\n\n    if runhistory_encoder is None:\n        runhistory_encoder = self.get_runhistory_encoder(scenario)\n\n    if config_selector is None:\n        config_selector = self.get_config_selector(scenario)\n\n    # Initialize empty stats and runhistory object\n    runhistory = RunHistory(multi_objective_algorithm=multi_objective_algorithm)\n\n    # Set the seed for configuration space\n    scenario.configspace.seed(scenario.seed)\n\n    # Set variables globally\n    self._scenario = scenario\n    self._model = model\n    self._acquisition_function = acquisition_function\n    self._acquisition_maximizer = acquisition_maximizer\n    self._initial_design = initial_design\n    self._random_design = random_design\n    self._intensifier = intensifier\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._runhistory = runhistory\n    self._runhistory_encoder = runhistory_encoder\n    self._config_selector = config_selector\n    self._callbacks = callbacks\n    self._overwrite = overwrite\n\n    # Prepare the algorithm executer\n    runner: AbstractRunner\n    if isinstance(target_function, AbstractRunner):\n        runner = target_function\n    elif isinstance(target_function, str):\n        runner = TargetFunctionScriptRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n    else:\n        runner = TargetFunctionRunner(\n            scenario=scenario,\n            target_function=target_function,\n            required_arguments=self._get_signature_arguments(),\n        )\n\n    # In case of multiple jobs, we need to wrap the runner again using DaskParallelRunner\n    if (n_workers := scenario.n_workers) &gt; 1 or dask_client is not None:\n        if dask_client is not None and n_workers &gt; 1:\n            logger.warning(\n                \"Provided `dask_client`. Ignore `scenario.n_workers`, directly set `n_workers` in `dask_client`.\"\n            )\n        else:\n            available_workers = joblib.cpu_count()\n            if n_workers &gt; available_workers:\n                logger.info(f\"Workers are reduced to {n_workers}.\")\n                n_workers = available_workers\n\n        # We use a dask runner for parallelization\n        runner = DaskParallelRunner(single_worker=runner, dask_client=dask_client)\n\n    # Set the runner to access it globally\n    self._runner = runner\n\n    # Adding dependencies of the components\n    self._update_dependencies()\n\n    # We have to update our meta data (basically arguments of the components)\n    self._scenario._set_meta(self.meta)\n\n    # We have to validate if the object compositions are correct and actually make sense\n    self._validate()\n\n    # Finally we configure our optimizer\n    self._optimizer = self._get_optimizer()\n    assert self._optimizer\n\n    # Register callbacks here\n    for callback in callbacks:\n        self._optimizer.register_callback(callback)\n\n    # Additionally, we register the runhistory callback from the intensifier to efficiently update our incumbent\n    # every time new information are available\n    self._optimizer.register_callback(self._intensifier.get_callback(), index=0)\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Generates a hash based on all components of the facade. This is used for the run name or to determine whether a run should be continued or not.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.optimizer","title":"optimizer  <code>property</code>","text":"<pre><code>optimizer: SMBO\n</code></pre> <p>The optimizer which is responsible for the BO loop. Keeps track of useful information like status.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The runhistory which is filled with all trials during the optimization process.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.scenario","title":"scenario  <code>property</code>","text":"<pre><code>scenario: Scenario\n</code></pre> <p>The scenario object which holds all environment information.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\"\"\"\n    return self._optimizer.ask()\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_acquisition_function","title":"get_acquisition_function  <code>staticmethod</code>","text":"<pre><code>get_acquisition_function(\n    scenario: Scenario,\n) -&gt; AbstractAcquisitionFunction\n</code></pre> <p>The random facade is not using an acquisition function. Therefore, we simply return a dummy function.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_function(scenario: Scenario) -&gt; AbstractAcquisitionFunction:\n    \"\"\"The random facade is not using an acquisition function. Therefore, we simply return a dummy function.\"\"\"\n\n    class DummyAcquisitionFunction(AbstractAcquisitionFunction):\n        def _compute(self, X: np.ndarray) -&gt; np.ndarray:\n            return X\n\n    return DummyAcquisitionFunction()\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_acquisition_maximizer","title":"get_acquisition_maximizer  <code>staticmethod</code>","text":"<pre><code>get_acquisition_maximizer(\n    scenario: Scenario,\n) -&gt; RandomSearch\n</code></pre> <p>We return <code>RandomSearch</code> as maximizer which samples configurations randomly from the configuration space and therefore neither uses the acquisition function nor the model.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_acquisition_maximizer(scenario: Scenario) -&gt; RandomSearch:\n    \"\"\"We return ``RandomSearch`` as maximizer which samples configurations randomly from the configuration\n    space and therefore neither uses the acquisition function nor the model.\n    \"\"\"\n    return RandomSearch(\n        scenario.configspace,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_config_selector","title":"get_config_selector  <code>staticmethod</code>","text":"<pre><code>get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16\n) -&gt; ConfigSelector\n</code></pre> <p>Returns the default configuration selector.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>@staticmethod\ndef get_config_selector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n) -&gt; ConfigSelector:\n    \"\"\"Returns the default configuration selector.\"\"\"\n    return ConfigSelector(scenario, retrain_after=retrain_after, retries=retries)\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_initial_design","title":"get_initial_design  <code>staticmethod</code>","text":"<pre><code>get_initial_design(\n    scenario: Scenario,\n    *,\n    additional_configs: list[Configuration] = None\n) -&gt; DefaultInitialDesign\n</code></pre> <p>Returns an initial design, which returns the default configuration.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_initial_design--parameters","title":"Parameters","text":"<p>additional_configs: list[Configuration], defaults to []     Adds additional configurations to the initial design.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_initial_design(\n    scenario: Scenario,\n    *,\n    additional_configs: list[Configuration] = None,\n) -&gt; DefaultInitialDesign:\n    \"\"\"Returns an initial design, which returns the default configuration.\n\n    Parameters\n    ----------\n    additional_configs: list[Configuration], defaults to []\n        Adds additional configurations to the initial design.\n    \"\"\"\n    if additional_configs is None:\n        additional_configs = []\n    return DefaultInitialDesign(\n        scenario=scenario,\n        additional_configs=additional_configs,\n    )\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_intensifier","title":"get_intensifier  <code>staticmethod</code>","text":"<pre><code>get_intensifier(\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 3,\n    max_incumbents: int = 10\n) -&gt; Intensifier\n</code></pre> <p>Returns <code>Intensifier</code> as intensifier.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_intensifier--note","title":"Note","text":"<p>Please use the <code>HyperbandFacade</code> if you want to incorporate budgets.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_intensifier--warning","title":"Warning","text":"<p>If you are in an algorithm configuration setting, consider increasing <code>max_config_calls</code>.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_intensifier--parameters","title":"Parameters","text":"<p>max_config_calls : int, defaults to 3     Maximum number of configuration evaluations. Basically, how many instance-seed keys should be max evaluated     for a configuration. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_intensifier(\n    scenario: Scenario,\n    *,\n    max_config_calls: int = 3,\n    max_incumbents: int = 10,\n) -&gt; Intensifier:\n    \"\"\"Returns ``Intensifier`` as intensifier.\n\n    Note\n    ----\n    Please use the ``HyperbandFacade`` if you want to incorporate budgets.\n\n    Warning\n    -------\n    If you are in an algorithm configuration setting, consider increasing ``max_config_calls``.\n\n    Parameters\n    ----------\n    max_config_calls : int, defaults to 3\n        Maximum number of configuration evaluations. Basically, how many instance-seed keys should be max evaluated\n        for a configuration.\n    max_incumbents : int, defaults to 10\n        How many incumbents to keep track of in the case of multi-objective.\n    \"\"\"\n    return Intensifier(\n        scenario=scenario,\n        max_config_calls=max_config_calls,\n        max_incumbents=max_incumbents,\n    )\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_model","title":"get_model  <code>staticmethod</code>","text":"<pre><code>get_model(scenario: Scenario) -&gt; RandomModel\n</code></pre> <p>The model is used in the acquisition function. Since we do not use an acquisition function, we return a dummy model (returning random values in this case).</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_model(scenario: Scenario) -&gt; RandomModel:\n    \"\"\"The model is used in the acquisition function. Since we do not use an acquisition function, we return a\n    dummy model (returning random values in this case).\n    \"\"\"\n    return RandomModel(\n        configspace=scenario.configspace,\n        instance_features=scenario.instance_features,\n        seed=scenario.seed,\n    )\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_multi_objective_algorithm","title":"get_multi_objective_algorithm  <code>staticmethod</code>","text":"<pre><code>get_multi_objective_algorithm(\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None\n) -&gt; MeanAggregationStrategy\n</code></pre> <p>Returns the mean aggregation strategy for the multi-objective algorithm.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_multi_objective_algorithm--parameters","title":"Parameters","text":"<p>scenario : Scenario objective_weights : list[float] | None, defaults to None     Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of     objectives.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_multi_objective_algorithm(  # type: ignore\n    scenario: Scenario,\n    *,\n    objective_weights: list[float] | None = None,\n) -&gt; MeanAggregationStrategy:\n    \"\"\"Returns the mean aggregation strategy for the multi-objective algorithm.\n\n    Parameters\n    ----------\n    scenario : Scenario\n    objective_weights : list[float] | None, defaults to None\n        Weights for averaging the objectives in a weighted manner. Must be of the same length as the number of\n        objectives.\n    \"\"\"\n    return MeanAggregationStrategy(\n        scenario=scenario,\n        objective_weights=objective_weights,\n    )\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_random_design","title":"get_random_design  <code>staticmethod</code>","text":"<pre><code>get_random_design(\n    scenario: Scenario,\n) -&gt; AbstractRandomDesign\n</code></pre> <p>Just like the acquisition function, we do not use a random design. Therefore, we return a dummy design.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_random_design(scenario: Scenario) -&gt; AbstractRandomDesign:\n    \"\"\"Just like the acquisition function, we do not use a random design. Therefore, we return a dummy design.\"\"\"\n\n    class DummyRandomDesign(AbstractRandomDesign):\n        def check(self, iteration: int) -&gt; bool:\n            return True\n\n    return DummyRandomDesign()\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.get_runhistory_encoder","title":"get_runhistory_encoder  <code>staticmethod</code>","text":"<pre><code>get_runhistory_encoder(\n    scenario: Scenario,\n) -&gt; RunHistoryEncoder\n</code></pre> <p>Returns the default runhistory encoder.</p> Source code in <code>smac/facade/random_facade.py</code> <pre><code>@staticmethod\ndef get_runhistory_encoder(scenario: Scenario) -&gt; RunHistoryEncoder:\n    \"\"\"Returns the default runhistory encoder.\"\"\"\n    return RunHistoryEncoder(scenario)\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Optimizes the configuration of the algorithm.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     We first note that this argument is valid only dask_runner!     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     Best found configuration.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"\n    Optimizes the configuration of the algorithm.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        We first note that this argument is valid only dask_runner!\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        Best found configuration.\n    \"\"\"\n    incumbents = None\n    if isinstance(data_to_scatter, dict) and len(data_to_scatter) == 0:\n        raise ValueError(\"data_to_scatter must be None or dict with some elements, but got an empty dict.\")\n\n    try:\n        incumbents = self._optimizer.optimize(data_to_scatter=data_to_scatter)\n    finally:\n        self._optimizer.save()\n\n    return incumbents\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the intensifier.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.tell--parameters","title":"Parameters","text":"<p>info: TrialInfo     Describes the trial from which to process the results. value: TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def tell(self, info: TrialInfo, value: TrialValue, save: bool = True) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the intensifier.\n\n    Parameters\n    ----------\n    info: TrialInfo\n        Describes the trial from which to process the results.\n    value: TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    return self._optimizer.tell(info, value, save=save)\n</code></pre>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | list[float]\n</code></pre> <p>Validates a configuration on seeds different from the ones used in the optimization process and on the highest budget (if budget type is real-valued).</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate instances : list[str] | None, defaults to None     Which instances to validate. If None, all instances specified in the scenario are used.     In case that the budget type is real-valued, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/facade/random_facade/#smac.facade.random_facade.RandomFacade.validate--returns","title":"Returns","text":"<p>cost : float | list[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/facade/abstract_facade.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"Validates a configuration on seeds different from the ones used in the optimization process and on the\n    highest budget (if budget type is real-valued).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n    instances : list[str] | None, defaults to None\n        Which instances to validate. If None, all instances specified in the scenario are used.\n        In case that the budget type is real-valued, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | list[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    return self._optimizer.validate(config, seed=seed)\n</code></pre>"},{"location":"api/smac/initial_design/abstract_initial_design/","title":"Abstract initial design","text":""},{"location":"api/smac/initial_design/abstract_initial_design/#smac.initial_design.abstract_initial_design","title":"smac.initial_design.abstract_initial_design","text":""},{"location":"api/smac/initial_design/abstract_initial_design/#smac.initial_design.abstract_initial_design.AbstractInitialDesign","title":"AbstractInitialDesign","text":"<pre><code>AbstractInitialDesign(\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n)\n</code></pre> <p>Base class for initial design strategies that evaluates multiple configurations.</p>"},{"location":"api/smac/initial_design/abstract_initial_design/#smac.initial_design.abstract_initial_design.AbstractInitialDesign--parameters","title":"Parameters","text":"<p>scenario : Scenario n_configs : int | None, defaults to None     Number of initial configurations (disables the arguments <code>n_configs_per_hyperparameter</code>). n_configs_per_hyperparameter: int, defaults to 10     Number of initial configurations per hyperparameter. For example, if my configuration space covers five     hyperparameters and <code>n_configs_per_hyperparameter</code> is set to 10, then 50 initial configurations will be     samples. max_ratio: float, defaults to 0.25     Use at most <code>scenario.n_trials</code> * <code>max_ratio</code> number of configurations in the initial design.     Additional configurations are not affected by this parameter. additional_configs: list[Configuration], defaults to []     Adds additional configurations to the initial design. seed : int | None, default to None</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n):\n    self._configspace = scenario.configspace\n\n    if seed is None:\n        seed = scenario.seed\n\n    self.use_default_config = scenario.use_default_config\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._n_configs_per_hyperparameter = n_configs_per_hyperparameter\n\n    # make sure that additional configs is not a mutable default value\n    # this avoids issues\n    if additional_configs is None:\n        additional_configs = []\n\n    if self.use_default_config:\n        default_config = self._configspace.get_default_configuration()\n        default_config.origin = \"Initial Design: Default configuration\"\n        additional_configs.append(default_config)\n\n    self._additional_configs = additional_configs\n\n    n_params = len(list(self._configspace.values()))\n    if n_configs is not None:\n        logger.info(\"Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\")\n        self._n_configs = n_configs\n    elif n_configs_per_hyperparameter is not None:\n        self._n_configs = n_configs_per_hyperparameter * n_params\n    else:\n        raise ValueError(\n            \"Need to provide either argument `n_configs` or \"\n            \"`n_configs_per_hyperparameter` but provided none of them.\"\n        )\n\n    # If the number of configurations is too large, we reduce it\n    _n_configs = int(max(1, min(self._n_configs, (max_ratio * scenario.n_trials))))\n    if self._n_configs != _n_configs:\n        logger.info(\n            f\"Reducing the number of initial configurations from {self._n_configs} to \"\n            f\"{_n_configs} (max_ratio == {max_ratio}).\"\n        )\n        self._n_configs = _n_configs\n\n    # We allow no configs if we have additional configs\n    if n_configs is not None and n_configs == 0 and len(additional_configs) &gt; 0:\n        self._n_configs = 0\n\n    if self._n_configs + len(additional_configs) &gt; scenario.n_trials:\n        raise ValueError(\n            f\"Initial budget {self._n_configs} cannot be higher than the number of trials {scenario.n_trials}.\"\n        )\n</code></pre>"},{"location":"api/smac/initial_design/abstract_initial_design/#smac.initial_design.abstract_initial_design.AbstractInitialDesign.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/initial_design/abstract_initial_design/#smac.initial_design.abstract_initial_design.AbstractInitialDesign.select_configurations","title":"select_configurations","text":"<pre><code>select_configurations() -&gt; list[Configuration]\n</code></pre> <p>Selects the initial configurations. Internally, <code>_select_configurations</code> is called, which has to be implemented by the child class.</p>"},{"location":"api/smac/initial_design/abstract_initial_design/#smac.initial_design.abstract_initial_design.AbstractInitialDesign.select_configurations--returns","title":"Returns","text":"<p>configs : list[Configuration]     Configurations from the child class.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def select_configurations(self) -&gt; list[Configuration]:\n    \"\"\"Selects the initial configurations. Internally, `_select_configurations` is called,\n    which has to be implemented by the child class.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        Configurations from the child class.\n    \"\"\"\n    configs: list[Configuration] = []\n\n    if self._n_configs == 0:\n        logger.info(\"No initial configurations are used.\")\n    else:\n        configs += self._select_configurations()\n\n    # Adding additional configs\n    configs += self._additional_configs\n\n    for config in configs:\n        if config.origin is None:\n            config.origin = \"Initial design\"\n\n    # Removing duplicates\n    # (Reference: https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists)\n    configs = list(OrderedDict.fromkeys(configs))\n    logger.info(\n        f\"Using {len(configs) - len(self._additional_configs)} initial design configurations \"\n        f\"and {len(self._additional_configs)} additional configurations.\"\n    )\n\n    return configs\n</code></pre>"},{"location":"api/smac/initial_design/default_design/","title":"Default design","text":""},{"location":"api/smac/initial_design/default_design/#smac.initial_design.default_design","title":"smac.initial_design.default_design","text":""},{"location":"api/smac/initial_design/default_design/#smac.initial_design.default_design.DefaultInitialDesign","title":"DefaultInitialDesign","text":"<pre><code>DefaultInitialDesign(\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractInitialDesign</code></p> <p>Initial design that evaluates only the default configuration.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n):\n    self._configspace = scenario.configspace\n\n    if seed is None:\n        seed = scenario.seed\n\n    self.use_default_config = scenario.use_default_config\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._n_configs_per_hyperparameter = n_configs_per_hyperparameter\n\n    # make sure that additional configs is not a mutable default value\n    # this avoids issues\n    if additional_configs is None:\n        additional_configs = []\n\n    if self.use_default_config:\n        default_config = self._configspace.get_default_configuration()\n        default_config.origin = \"Initial Design: Default configuration\"\n        additional_configs.append(default_config)\n\n    self._additional_configs = additional_configs\n\n    n_params = len(list(self._configspace.values()))\n    if n_configs is not None:\n        logger.info(\"Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\")\n        self._n_configs = n_configs\n    elif n_configs_per_hyperparameter is not None:\n        self._n_configs = n_configs_per_hyperparameter * n_params\n    else:\n        raise ValueError(\n            \"Need to provide either argument `n_configs` or \"\n            \"`n_configs_per_hyperparameter` but provided none of them.\"\n        )\n\n    # If the number of configurations is too large, we reduce it\n    _n_configs = int(max(1, min(self._n_configs, (max_ratio * scenario.n_trials))))\n    if self._n_configs != _n_configs:\n        logger.info(\n            f\"Reducing the number of initial configurations from {self._n_configs} to \"\n            f\"{_n_configs} (max_ratio == {max_ratio}).\"\n        )\n        self._n_configs = _n_configs\n\n    # We allow no configs if we have additional configs\n    if n_configs is not None and n_configs == 0 and len(additional_configs) &gt; 0:\n        self._n_configs = 0\n\n    if self._n_configs + len(additional_configs) &gt; scenario.n_trials:\n        raise ValueError(\n            f\"Initial budget {self._n_configs} cannot be higher than the number of trials {scenario.n_trials}.\"\n        )\n</code></pre>"},{"location":"api/smac/initial_design/default_design/#smac.initial_design.default_design.DefaultInitialDesign.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/initial_design/default_design/#smac.initial_design.default_design.DefaultInitialDesign.select_configurations","title":"select_configurations","text":"<pre><code>select_configurations() -&gt; list[Configuration]\n</code></pre> <p>Selects the initial configurations. Internally, <code>_select_configurations</code> is called, which has to be implemented by the child class.</p>"},{"location":"api/smac/initial_design/default_design/#smac.initial_design.default_design.DefaultInitialDesign.select_configurations--returns","title":"Returns","text":"<p>configs : list[Configuration]     Configurations from the child class.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def select_configurations(self) -&gt; list[Configuration]:\n    \"\"\"Selects the initial configurations. Internally, `_select_configurations` is called,\n    which has to be implemented by the child class.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        Configurations from the child class.\n    \"\"\"\n    configs: list[Configuration] = []\n\n    if self._n_configs == 0:\n        logger.info(\"No initial configurations are used.\")\n    else:\n        configs += self._select_configurations()\n\n    # Adding additional configs\n    configs += self._additional_configs\n\n    for config in configs:\n        if config.origin is None:\n            config.origin = \"Initial design\"\n\n    # Removing duplicates\n    # (Reference: https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists)\n    configs = list(OrderedDict.fromkeys(configs))\n    logger.info(\n        f\"Using {len(configs) - len(self._additional_configs)} initial design configurations \"\n        f\"and {len(self._additional_configs)} additional configurations.\"\n    )\n\n    return configs\n</code></pre>"},{"location":"api/smac/initial_design/factorial_design/","title":"Factorial design","text":""},{"location":"api/smac/initial_design/factorial_design/#smac.initial_design.factorial_design","title":"smac.initial_design.factorial_design","text":""},{"location":"api/smac/initial_design/factorial_design/#smac.initial_design.factorial_design.FactorialInitialDesign","title":"FactorialInitialDesign","text":"<pre><code>FactorialInitialDesign(\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractInitialDesign</code></p> <p>Factorial initial design to select corner and middle configurations.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n):\n    self._configspace = scenario.configspace\n\n    if seed is None:\n        seed = scenario.seed\n\n    self.use_default_config = scenario.use_default_config\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._n_configs_per_hyperparameter = n_configs_per_hyperparameter\n\n    # make sure that additional configs is not a mutable default value\n    # this avoids issues\n    if additional_configs is None:\n        additional_configs = []\n\n    if self.use_default_config:\n        default_config = self._configspace.get_default_configuration()\n        default_config.origin = \"Initial Design: Default configuration\"\n        additional_configs.append(default_config)\n\n    self._additional_configs = additional_configs\n\n    n_params = len(list(self._configspace.values()))\n    if n_configs is not None:\n        logger.info(\"Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\")\n        self._n_configs = n_configs\n    elif n_configs_per_hyperparameter is not None:\n        self._n_configs = n_configs_per_hyperparameter * n_params\n    else:\n        raise ValueError(\n            \"Need to provide either argument `n_configs` or \"\n            \"`n_configs_per_hyperparameter` but provided none of them.\"\n        )\n\n    # If the number of configurations is too large, we reduce it\n    _n_configs = int(max(1, min(self._n_configs, (max_ratio * scenario.n_trials))))\n    if self._n_configs != _n_configs:\n        logger.info(\n            f\"Reducing the number of initial configurations from {self._n_configs} to \"\n            f\"{_n_configs} (max_ratio == {max_ratio}).\"\n        )\n        self._n_configs = _n_configs\n\n    # We allow no configs if we have additional configs\n    if n_configs is not None and n_configs == 0 and len(additional_configs) &gt; 0:\n        self._n_configs = 0\n\n    if self._n_configs + len(additional_configs) &gt; scenario.n_trials:\n        raise ValueError(\n            f\"Initial budget {self._n_configs} cannot be higher than the number of trials {scenario.n_trials}.\"\n        )\n</code></pre>"},{"location":"api/smac/initial_design/factorial_design/#smac.initial_design.factorial_design.FactorialInitialDesign.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/initial_design/factorial_design/#smac.initial_design.factorial_design.FactorialInitialDesign.select_configurations","title":"select_configurations","text":"<pre><code>select_configurations() -&gt; list[Configuration]\n</code></pre> <p>Selects the initial configurations. Internally, <code>_select_configurations</code> is called, which has to be implemented by the child class.</p>"},{"location":"api/smac/initial_design/factorial_design/#smac.initial_design.factorial_design.FactorialInitialDesign.select_configurations--returns","title":"Returns","text":"<p>configs : list[Configuration]     Configurations from the child class.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def select_configurations(self) -&gt; list[Configuration]:\n    \"\"\"Selects the initial configurations. Internally, `_select_configurations` is called,\n    which has to be implemented by the child class.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        Configurations from the child class.\n    \"\"\"\n    configs: list[Configuration] = []\n\n    if self._n_configs == 0:\n        logger.info(\"No initial configurations are used.\")\n    else:\n        configs += self._select_configurations()\n\n    # Adding additional configs\n    configs += self._additional_configs\n\n    for config in configs:\n        if config.origin is None:\n            config.origin = \"Initial design\"\n\n    # Removing duplicates\n    # (Reference: https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists)\n    configs = list(OrderedDict.fromkeys(configs))\n    logger.info(\n        f\"Using {len(configs) - len(self._additional_configs)} initial design configurations \"\n        f\"and {len(self._additional_configs)} additional configurations.\"\n    )\n\n    return configs\n</code></pre>"},{"location":"api/smac/initial_design/latin_hypercube_design/","title":"Latin hypercube design","text":""},{"location":"api/smac/initial_design/latin_hypercube_design/#smac.initial_design.latin_hypercube_design","title":"smac.initial_design.latin_hypercube_design","text":""},{"location":"api/smac/initial_design/latin_hypercube_design/#smac.initial_design.latin_hypercube_design.LatinHypercubeInitialDesign","title":"LatinHypercubeInitialDesign","text":"<pre><code>LatinHypercubeInitialDesign(\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractInitialDesign</code></p> <p>Latin Hypercube initial design. See docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.LatinHypercube.html for further information.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n):\n    self._configspace = scenario.configspace\n\n    if seed is None:\n        seed = scenario.seed\n\n    self.use_default_config = scenario.use_default_config\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._n_configs_per_hyperparameter = n_configs_per_hyperparameter\n\n    # make sure that additional configs is not a mutable default value\n    # this avoids issues\n    if additional_configs is None:\n        additional_configs = []\n\n    if self.use_default_config:\n        default_config = self._configspace.get_default_configuration()\n        default_config.origin = \"Initial Design: Default configuration\"\n        additional_configs.append(default_config)\n\n    self._additional_configs = additional_configs\n\n    n_params = len(list(self._configspace.values()))\n    if n_configs is not None:\n        logger.info(\"Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\")\n        self._n_configs = n_configs\n    elif n_configs_per_hyperparameter is not None:\n        self._n_configs = n_configs_per_hyperparameter * n_params\n    else:\n        raise ValueError(\n            \"Need to provide either argument `n_configs` or \"\n            \"`n_configs_per_hyperparameter` but provided none of them.\"\n        )\n\n    # If the number of configurations is too large, we reduce it\n    _n_configs = int(max(1, min(self._n_configs, (max_ratio * scenario.n_trials))))\n    if self._n_configs != _n_configs:\n        logger.info(\n            f\"Reducing the number of initial configurations from {self._n_configs} to \"\n            f\"{_n_configs} (max_ratio == {max_ratio}).\"\n        )\n        self._n_configs = _n_configs\n\n    # We allow no configs if we have additional configs\n    if n_configs is not None and n_configs == 0 and len(additional_configs) &gt; 0:\n        self._n_configs = 0\n\n    if self._n_configs + len(additional_configs) &gt; scenario.n_trials:\n        raise ValueError(\n            f\"Initial budget {self._n_configs} cannot be higher than the number of trials {scenario.n_trials}.\"\n        )\n</code></pre>"},{"location":"api/smac/initial_design/latin_hypercube_design/#smac.initial_design.latin_hypercube_design.LatinHypercubeInitialDesign.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/initial_design/latin_hypercube_design/#smac.initial_design.latin_hypercube_design.LatinHypercubeInitialDesign.select_configurations","title":"select_configurations","text":"<pre><code>select_configurations() -&gt; list[Configuration]\n</code></pre> <p>Selects the initial configurations. Internally, <code>_select_configurations</code> is called, which has to be implemented by the child class.</p>"},{"location":"api/smac/initial_design/latin_hypercube_design/#smac.initial_design.latin_hypercube_design.LatinHypercubeInitialDesign.select_configurations--returns","title":"Returns","text":"<p>configs : list[Configuration]     Configurations from the child class.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def select_configurations(self) -&gt; list[Configuration]:\n    \"\"\"Selects the initial configurations. Internally, `_select_configurations` is called,\n    which has to be implemented by the child class.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        Configurations from the child class.\n    \"\"\"\n    configs: list[Configuration] = []\n\n    if self._n_configs == 0:\n        logger.info(\"No initial configurations are used.\")\n    else:\n        configs += self._select_configurations()\n\n    # Adding additional configs\n    configs += self._additional_configs\n\n    for config in configs:\n        if config.origin is None:\n            config.origin = \"Initial design\"\n\n    # Removing duplicates\n    # (Reference: https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists)\n    configs = list(OrderedDict.fromkeys(configs))\n    logger.info(\n        f\"Using {len(configs) - len(self._additional_configs)} initial design configurations \"\n        f\"and {len(self._additional_configs)} additional configurations.\"\n    )\n\n    return configs\n</code></pre>"},{"location":"api/smac/initial_design/random_design/","title":"Random design","text":""},{"location":"api/smac/initial_design/random_design/#smac.initial_design.random_design","title":"smac.initial_design.random_design","text":""},{"location":"api/smac/initial_design/random_design/#smac.initial_design.random_design.RandomInitialDesign","title":"RandomInitialDesign","text":"<pre><code>RandomInitialDesign(\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractInitialDesign</code></p> <p>Initial design that evaluates random configurations.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    n_configs: int | None = None,\n    n_configs_per_hyperparameter: int | None = 10,\n    max_ratio: float = 0.25,\n    additional_configs: list[Configuration] = None,\n    seed: int | None = None,\n):\n    self._configspace = scenario.configspace\n\n    if seed is None:\n        seed = scenario.seed\n\n    self.use_default_config = scenario.use_default_config\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._n_configs_per_hyperparameter = n_configs_per_hyperparameter\n\n    # make sure that additional configs is not a mutable default value\n    # this avoids issues\n    if additional_configs is None:\n        additional_configs = []\n\n    if self.use_default_config:\n        default_config = self._configspace.get_default_configuration()\n        default_config.origin = \"Initial Design: Default configuration\"\n        additional_configs.append(default_config)\n\n    self._additional_configs = additional_configs\n\n    n_params = len(list(self._configspace.values()))\n    if n_configs is not None:\n        logger.info(\"Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\")\n        self._n_configs = n_configs\n    elif n_configs_per_hyperparameter is not None:\n        self._n_configs = n_configs_per_hyperparameter * n_params\n    else:\n        raise ValueError(\n            \"Need to provide either argument `n_configs` or \"\n            \"`n_configs_per_hyperparameter` but provided none of them.\"\n        )\n\n    # If the number of configurations is too large, we reduce it\n    _n_configs = int(max(1, min(self._n_configs, (max_ratio * scenario.n_trials))))\n    if self._n_configs != _n_configs:\n        logger.info(\n            f\"Reducing the number of initial configurations from {self._n_configs} to \"\n            f\"{_n_configs} (max_ratio == {max_ratio}).\"\n        )\n        self._n_configs = _n_configs\n\n    # We allow no configs if we have additional configs\n    if n_configs is not None and n_configs == 0 and len(additional_configs) &gt; 0:\n        self._n_configs = 0\n\n    if self._n_configs + len(additional_configs) &gt; scenario.n_trials:\n        raise ValueError(\n            f\"Initial budget {self._n_configs} cannot be higher than the number of trials {scenario.n_trials}.\"\n        )\n</code></pre>"},{"location":"api/smac/initial_design/random_design/#smac.initial_design.random_design.RandomInitialDesign.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/initial_design/random_design/#smac.initial_design.random_design.RandomInitialDesign.select_configurations","title":"select_configurations","text":"<pre><code>select_configurations() -&gt; list[Configuration]\n</code></pre> <p>Selects the initial configurations. Internally, <code>_select_configurations</code> is called, which has to be implemented by the child class.</p>"},{"location":"api/smac/initial_design/random_design/#smac.initial_design.random_design.RandomInitialDesign.select_configurations--returns","title":"Returns","text":"<p>configs : list[Configuration]     Configurations from the child class.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def select_configurations(self) -&gt; list[Configuration]:\n    \"\"\"Selects the initial configurations. Internally, `_select_configurations` is called,\n    which has to be implemented by the child class.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        Configurations from the child class.\n    \"\"\"\n    configs: list[Configuration] = []\n\n    if self._n_configs == 0:\n        logger.info(\"No initial configurations are used.\")\n    else:\n        configs += self._select_configurations()\n\n    # Adding additional configs\n    configs += self._additional_configs\n\n    for config in configs:\n        if config.origin is None:\n            config.origin = \"Initial design\"\n\n    # Removing duplicates\n    # (Reference: https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists)\n    configs = list(OrderedDict.fromkeys(configs))\n    logger.info(\n        f\"Using {len(configs) - len(self._additional_configs)} initial design configurations \"\n        f\"and {len(self._additional_configs)} additional configurations.\"\n    )\n\n    return configs\n</code></pre>"},{"location":"api/smac/initial_design/sobol_design/","title":"Sobol design","text":""},{"location":"api/smac/initial_design/sobol_design/#smac.initial_design.sobol_design","title":"smac.initial_design.sobol_design","text":""},{"location":"api/smac/initial_design/sobol_design/#smac.initial_design.sobol_design.SobolInitialDesign","title":"SobolInitialDesign","text":"<pre><code>SobolInitialDesign(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>AbstractInitialDesign</code></p> <p>Sobol sequence design with a scrambled Sobol sequence. See scipy.github.io/devdocs/reference/generated/scipy.stats.qmc.Sobol.html for further information.</p> Source code in <code>smac/initial_design/sobol_design.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n\n    if len(list(self._configspace.values())) &gt; 21201:\n        raise ValueError(\n            \"The default initial design Sobol sequence can only handle up to 21201 dimensions. \"\n            \"Please use a different initial design, such as the Latin Hypercube design.\"\n        )\n</code></pre>"},{"location":"api/smac/initial_design/sobol_design/#smac.initial_design.sobol_design.SobolInitialDesign.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/initial_design/sobol_design/#smac.initial_design.sobol_design.SobolInitialDesign.select_configurations","title":"select_configurations","text":"<pre><code>select_configurations() -&gt; list[Configuration]\n</code></pre> <p>Selects the initial configurations. Internally, <code>_select_configurations</code> is called, which has to be implemented by the child class.</p>"},{"location":"api/smac/initial_design/sobol_design/#smac.initial_design.sobol_design.SobolInitialDesign.select_configurations--returns","title":"Returns","text":"<p>configs : list[Configuration]     Configurations from the child class.</p> Source code in <code>smac/initial_design/abstract_initial_design.py</code> <pre><code>def select_configurations(self) -&gt; list[Configuration]:\n    \"\"\"Selects the initial configurations. Internally, `_select_configurations` is called,\n    which has to be implemented by the child class.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        Configurations from the child class.\n    \"\"\"\n    configs: list[Configuration] = []\n\n    if self._n_configs == 0:\n        logger.info(\"No initial configurations are used.\")\n    else:\n        configs += self._select_configurations()\n\n    # Adding additional configs\n    configs += self._additional_configs\n\n    for config in configs:\n        if config.origin is None:\n            config.origin = \"Initial design\"\n\n    # Removing duplicates\n    # (Reference: https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists)\n    configs = list(OrderedDict.fromkeys(configs))\n    logger.info(\n        f\"Using {len(configs) - len(self._additional_configs)} initial design configurations \"\n        f\"and {len(self._additional_configs)} additional configurations.\"\n    )\n\n    return configs\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/","title":"Abstract intensifier","text":""},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier","title":"smac.intensifier.abstract_intensifier","text":""},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier","title":"AbstractIntensifier","text":"<pre><code>AbstractIntensifier(\n    scenario: Scenario,\n    n_seeds: int | None = None,\n    max_config_calls: int | None = None,\n    max_incumbents: int = 10,\n    seed: int | None = None,\n)\n</code></pre> <p>Abstract implementation of an intensifier supporting multi-fidelity, multi-objective, and multi-threading. The abstract intensifier keeps track of the incumbent, which is updated everytime the runhistory changes.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier--parameters","title":"Parameters","text":"<p>n_seeds : int | None, defaults to None     How many seeds to use for each instance. It is used in the abstract intensifier to determine validation trials. max_config_calls : int, defaults to None     Maximum number of configuration evaluations. Basically, how many instance-seed keys should be max evaluated     for a configuration. It is used in the abstract intensifier to determine validation trials. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective. seed : int, defaults to None     Internal seed used for random events like shuffle seeds.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    n_seeds: int | None = None,\n    max_config_calls: int | None = None,\n    max_incumbents: int = 10,\n    seed: int | None = None,\n):\n    self._scenario = scenario\n    self._config_selector: ConfigSelector | None = None\n    self._config_generator: Iterator[ConfigSelector] | None = None\n    self._runhistory: RunHistory | None = None\n\n    if seed is None:\n        seed = self._scenario.seed\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n\n    # Internal variables\n    self._n_seeds = n_seeds\n    self._max_config_calls = max_config_calls\n    self._max_incumbents = max_incumbents\n    self._used_walltime_func: Callable | None = None\n\n    # Reset everything\n    self.reset()\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.config_generator","title":"config_generator  <code>property</code>","text":"<pre><code>config_generator: Iterator[Configuration]\n</code></pre> <p>Based on the configuration selector, an iterator is returned that generates configurations.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.config_selector","title":"config_selector  <code>property</code> <code>writable</code>","text":"<pre><code>config_selector: ConfigSelector\n</code></pre> <p>The configuration selector for the intensifier.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.incumbents_changed","title":"incumbents_changed  <code>property</code>","text":"<pre><code>incumbents_changed: int\n</code></pre> <p>How often the incumbents have changed.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>Runhistory of the intensifier.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.trajectory","title":"trajectory  <code>property</code>","text":"<pre><code>trajectory: list[TrajectoryItem]\n</code></pre> <p>Returns the trajectory (changes of incumbents) of the optimization run.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.used_walltime","title":"used_walltime  <code>property</code> <code>writable</code>","text":"<pre><code>used_walltime: float\n</code></pre> <p>Returns used wallclock time.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.uses_budgets","title":"uses_budgets  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>uses_budgets: bool\n</code></pre> <p>If the intensifier needs to make use of budgets.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.uses_instances","title":"uses_instances  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>uses_instances: bool\n</code></pre> <p>If the intensifier needs to make use of instances.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.uses_seeds","title":"uses_seeds  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>uses_seeds: bool\n</code></pre> <p>If the intensifier needs to make use of seeds.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.__iter__","title":"__iter__  <code>abstractmethod</code>","text":"<pre><code>__iter__() -&gt; Iterator[TrialInfo]\n</code></pre> <p>Main loop of the intensifier. This method always returns a TrialInfo object, although the intensifier algorithm may need to wait for the result of the trial. Please refer to a specific intensifier to get more information.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>@abstractmethod\ndef __iter__(self) -&gt; Iterator[TrialInfo]:\n    \"\"\"Main loop of the intensifier. This method always returns a TrialInfo object, although the intensifier\n    algorithm may need to wait for the result of the trial. Please refer to a specific\n    intensifier to get more information.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Fills <code>self._tf_seeds</code> and <code>self._tf_instances</code>. Moreover, the incumbents are updated.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Fills ``self._tf_seeds`` and ``self._tf_instances``. Moreover, the incumbents are updated.\"\"\"\n    rh = self.runhistory\n\n    # Validate runhistory: Are seeds/instances/budgets used?\n    # Add seed/instance/budget to the cache\n    for k in rh.keys():\n        if self.uses_seeds:\n            if k.seed is None:\n                raise ValueError(\"Trial contains no seed information but intensifier expects seeds to be used.\")\n\n            if k.seed not in self._tf_seeds:\n                self._tf_seeds.append(k.seed)\n\n        if self.uses_instances:\n            if self._scenario.instances is None and k.instance is not None:\n                raise ValueError(\n                    \"Scenario does not specify any instances but found instance information in runhistory.\"\n                )\n\n            if self._scenario.instances is not None and k.instance not in self._scenario.instances:\n                raise ValueError(\n                    \"Instance information in runhistory is not part of the defined instances in scenario.\"\n                )\n\n            if k.instance not in self._tf_instances:\n                self._tf_instances.append(k.instance)\n\n        if self.uses_budgets:\n            if k.budget is None:\n                raise ValueError(\"Trial contains no budget information but intensifier expects budgets to be used.\")\n\n            if k.budget not in self._tf_budgets:\n                self._tf_budgets.append(k.budget)\n\n    # Add all other instances to ``_tf_instances``\n    # Behind idea: Prioritize instances that are found in the runhistory\n    if (instances := self._scenario.instances) is not None:\n        for inst in instances:\n            if inst not in self._tf_instances:\n                self._tf_instances.append(inst)\n\n    if len(self._tf_instances) == 0:\n        self._tf_instances = [None]\n\n    if len(self._tf_budgets) == 0:\n        self._tf_budgets = [None]\n\n    # Update our incumbents here\n    for config in rh.get_configs():\n        self.update_incumbents(config)\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_callback","title":"get_callback","text":"<pre><code>get_callback() -&gt; Callback\n</code></pre> <p>The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory (every time new information is available). Moreover, incorporating the callback here allows developers more options in the future.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_callback(self) -&gt; Callback:\n    \"\"\"The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory\n    (every time new information is available). Moreover, incorporating the callback here allows developers\n    more options in the future.\n    \"\"\"\n\n    class RunHistoryCallback(Callback):\n        def __init__(self, intensifier: AbstractIntensifier):\n            self.intensifier = intensifier\n\n        def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; None:\n            self.intensifier.update_incumbents(info.config)\n\n    return RunHistoryCallback(self)\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_incumbent","title":"get_incumbent","text":"<pre><code>get_incumbent() -&gt; Configuration | None\n</code></pre> <p>Returns the current incumbent in a single-objective setting.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent(self) -&gt; Configuration | None:\n    \"\"\"Returns the current incumbent in a single-objective setting.\"\"\"\n    if self._scenario.count_objectives() &gt; 1:\n        raise ValueError(\"Cannot get a single incumbent for multi-objective optimization.\")\n\n    if len(self._incumbents) == 0:\n        return None\n\n    assert len(self._incumbents) == 1\n    return self._incumbents[0]\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_incumbent_instance_seed_budget_key_differences","title":"get_incumbent_instance_seed_budget_key_differences","text":"<pre><code>get_incumbent_instance_seed_budget_key_differences(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>There are situations in which incumbents are evaluated on more trials than others. This method returns the instances that are not part of the lowest intersection of instances for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_key_differences(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"There are situations in which incumbents are evaluated on more trials than others. This method returns the\n    instances that are not part of the lowest intersection of instances for all incumbents.\n    \"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the differences so that we can evaluate the other incumbents on the same instances\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n\n        if len(incumbent_isb_keys) &lt;= 1:\n            return []\n\n        # Compute the actual differences\n        intersection_isb_keys = set.intersection(*map(set, incumbent_isb_keys))  # type: ignore\n        union_isb_keys = set.union(*map(set, incumbent_isb_keys))  # type: ignore\n        incumbent_isb_keys = list(union_isb_keys - intersection_isb_keys)  # type: ignore\n\n        if len(incumbent_isb_keys) == 0:\n            return []\n\n        return incumbent_isb_keys  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_incumbent_instance_seed_budget_keys","title":"get_incumbent_instance_seed_budget_keys","text":"<pre><code>get_incumbent_instance_seed_budget_keys(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Find the lowest intersection of instance-seed-budget keys for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_keys(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Find the lowest intersection of instance-seed-budget keys for all incumbents.\"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the smallest set of trials that is used by all incumbents\n        # Reason: We can not fairly compare otherwise\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n        instances = list(set.intersection(*map(set, incumbent_isb_keys)))  # type: ignore\n\n        return instances  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_incumbents","title":"get_incumbents","text":"<pre><code>get_incumbents(\n    sort_by: str | None = None,\n) -&gt; list[Configuration]\n</code></pre> <p>Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective optimization, only one incumbent (if is) is returned.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_incumbents--returns","title":"Returns","text":"<p>configs : list[Configuration]     The configs of the Pareto front. sort_by : str, defaults to None     Sort the trials by <code>cost</code> (lowest cost first) or <code>num_trials</code> (config with lowest number of trials     first).</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbents(self, sort_by: str | None = None) -&gt; list[Configuration]:\n    \"\"\"Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective\n    optimization, only one incumbent (if is) is returned.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        The configs of the Pareto front.\n    sort_by : str, defaults to None\n        Sort the trials by ``cost`` (lowest cost first) or ``num_trials`` (config with lowest number of trials\n        first).\n    \"\"\"\n    rh = self.runhistory\n\n    if sort_by == \"cost\":\n        return list(sorted(self._incumbents, key=lambda config: rh._cost_per_config[rh.get_config_id(config)]))\n    elif sort_by == \"num_trials\":\n        return list(sorted(self._incumbents, key=lambda config: len(rh.get_trials(config))))\n    elif sort_by is None:\n        return list(self._incumbents)\n    else:\n        raise ValueError(f\"Unknown sort_by value: {sort_by}.\")\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_instance_seed_budget_keys","title":"get_instance_seed_budget_keys","text":"<pre><code>get_instance_seed_budget_keys(\n    config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Returns the instance-seed-budget keys for a given configuration. This method is used for updating the incumbents and might differ for different intensifiers. For example, if incumbents should only be compared on the highest observed budgets.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_instance_seed_budget_keys(\n    self, config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Returns the instance-seed-budget keys for a given configuration. This method is *used for\n    updating the incumbents* and might differ for different intensifiers. For example, if incumbents should only\n    be compared on the highest observed budgets.\n    \"\"\"\n    return self.runhistory.get_instance_seed_budget_keys(config, highest_observed_budget_only=False)\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_instance_seed_keys_of_interest","title":"get_instance_seed_keys_of_interest","text":"<pre><code>get_instance_seed_keys_of_interest(\n    *, validate: bool = False, seed: int | None = None\n) -&gt; list[InstanceSeedKey]\n</code></pre> <p>Returns a list of instance-seed keys. Considers seeds and instances from the runhistory (<code>self._tf_seeds</code> and <code>self._tf_instances</code>). If no seeds or instances were found, new seeds and instances are generated based on the global intensifier seed.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_instance_seed_keys_of_interest--warning","title":"Warning","text":"<p>The passed seed is only used for validation. For training, the global intensifier seed is used.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_instance_seed_keys_of_interest--parameters","title":"Parameters","text":"<p>validate : bool, defaults to False     Whether to get validation trials or training trials. The only difference lies in different seeds. seed : int | None, defaults to None     The seed used for the validation trials.</p>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_instance_seed_keys_of_interest--returns","title":"Returns","text":"<p>instance_seed_keys : list[InstanceSeedKey]     Instance-seed keys of interest.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_instance_seed_keys_of_interest(\n    self,\n    *,\n    validate: bool = False,\n    seed: int | None = None,\n) -&gt; list[InstanceSeedKey]:\n    \"\"\"Returns a list of instance-seed keys. Considers seeds and instances from the\n    runhistory (``self._tf_seeds`` and ``self._tf_instances``). If no seeds or instances were found, new\n    seeds and instances are generated based on the global intensifier seed.\n\n    Warning\n    -------\n    The passed seed is only used for validation. For training, the global intensifier seed is used.\n\n    Parameters\n    ----------\n    validate : bool, defaults to False\n        Whether to get validation trials or training trials. The only difference lies in different seeds.\n    seed : int | None, defaults to None\n        The seed used for the validation trials.\n\n    Returns\n    -------\n    instance_seed_keys : list[InstanceSeedKey]\n        Instance-seed keys of interest.\n    \"\"\"\n    if self._runhistory is None:\n        raise RuntimeError(\"Please set the runhistory before calling this method.\")\n\n    if len(self._tf_instances) == 0:\n        raise RuntimeError(\"Please call __post_init__ before calling this method.\")\n\n    if seed is None:\n        seed = 0\n\n    # We cache the instance-seed keys for efficiency and consistency reasons\n    if (self._instance_seed_keys is None and not validate) or (\n        self._instance_seed_keys_validation is None and validate\n    ):\n        instance_seed_keys: list[InstanceSeedKey] = []\n        if validate:\n            rng = np.random.RandomState(seed)\n        else:\n            rng = self._rng\n\n        i = 0\n        while True:\n            found_enough_configs = (\n                self._max_config_calls is not None and len(instance_seed_keys) &gt;= self._max_config_calls\n            )\n            used_enough_seeds = self._n_seeds is not None and i &gt;= self._n_seeds\n\n            if found_enough_configs or used_enough_seeds:\n                break\n\n            if validate:\n                next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n            else:\n                try:\n                    next_seed = self._tf_seeds[i]\n                    logger.info(f\"Added existing seed {next_seed} from runhistory to the intensifier.\")\n                except IndexError:\n                    # Use global random generator for a new seed and mark it so it will be reused for another config\n                    next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n\n                    # This line here is really important because we don't want to add the same seed twice\n                    if next_seed in self._tf_seeds:\n                        continue\n\n                    self._tf_seeds.append(next_seed)\n                    logger.debug(f\"Added a new random seed {next_seed} to the intensifier.\")\n\n            # If no instances are used, tf_instances includes None\n            for instance in self._tf_instances:\n                instance_seed_keys.append(InstanceSeedKey(instance, next_seed))\n\n            # Only use one seed in deterministic case\n            if self._scenario.deterministic:\n                logger.info(\"Using only one seed for deterministic scenario.\")\n                break\n\n            # Seed counter\n            i += 1\n\n        # Now we cut so that we only have max_config_calls instance_seed_keys\n        # We favor instances over seeds here: That makes sure we always work with the same instance/seed pairs\n        if self._max_config_calls is not None:\n            if len(instance_seed_keys) &gt; self._max_config_calls:\n                instance_seed_keys = instance_seed_keys[: self._max_config_calls]\n                logger.info(f\"Cut instance-seed keys to {self._max_config_calls} entries.\")\n\n        # Set it globally\n        if not validate:\n            self._instance_seed_keys = instance_seed_keys\n        else:\n            self._instance_seed_keys_validation = instance_seed_keys\n\n    if not validate:\n        assert self._instance_seed_keys is not None\n        instance_seed_keys = self._instance_seed_keys\n    else:\n        assert self._instance_seed_keys_validation is not None\n        instance_seed_keys = self._instance_seed_keys_validation\n\n    return instance_seed_keys.copy()\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_rejected_configs","title":"get_rejected_configs","text":"<pre><code>get_rejected_configs() -&gt; list[Configuration]\n</code></pre> <p>Returns rejected configurations when racing against the incumbent failed.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_rejected_configs(self) -&gt; list[Configuration]:\n    \"\"\"Returns rejected configurations when racing against the incumbent failed.\"\"\"\n    configs = []\n    for rejected_config_id in self._rejected_config_ids:\n        configs.append(self.runhistory._ids_config[rejected_config_id])\n\n    return configs\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_state","title":"get_state","text":"<pre><code>get_state() -&gt; dict[str, Any]\n</code></pre> <p>The current state of the intensifier. Used to restore the state of the intensifier when continuing a run.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_state(self) -&gt; dict[str, Any]:\n    \"\"\"The current state of the intensifier. Used to restore the state of the intensifier when continuing a run.\"\"\"\n    return {}\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.get_trials_of_interest","title":"get_trials_of_interest","text":"<pre><code>get_trials_of_interest(\n    config: Configuration,\n    *,\n    validate: bool = False,\n    seed: int | None = None\n) -&gt; list[TrialInfo]\n</code></pre> <p>Returns the trials of interest for a given configuration. Expands the keys from <code>get_instance_seed_keys_of_interest</code> with the config.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_trials_of_interest(\n    self,\n    config: Configuration,\n    *,\n    validate: bool = False,\n    seed: int | None = None,\n) -&gt; list[TrialInfo]:\n    \"\"\"Returns the trials of interest for a given configuration.\n    Expands the keys from ``get_instance_seed_keys_of_interest`` with the config.\n    \"\"\"\n    is_keys = self.get_instance_seed_keys_of_interest(validate=validate, seed=seed)\n\n    trials = []\n    for key in is_keys:\n        trials.append(TrialInfo(config=config, instance=key.instance, seed=key.seed))\n\n    return trials\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.load","title":"load","text":"<pre><code>load(filename: str | Path) -&gt; None\n</code></pre> <p>Loads the latest state of the intensifier including the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def load(self, filename: str | Path) -&gt; None:\n    \"\"\"Loads the latest state of the intensifier including the incumbents and trajectory.\"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    try:\n        with open(filename) as fp:\n            data = json.load(fp)\n    except Exception as e:\n        logger.warning(\n            f\"Encountered exception {e} while reading runhistory from {filename}. Not adding any trials!\"\n        )\n        return\n\n    # We reset the intensifier and then reset the runhistory\n    self.reset()\n    if self._runhistory is not None:\n        self.runhistory = self._runhistory\n\n    self._incumbents = [self.runhistory.get_config(config_id) for config_id in data[\"incumbent_ids\"]]\n    self._incumbents_changed = data[\"incumbents_changed\"]\n    self._rejected_config_ids = data[\"rejected_config_ids\"]\n    self._trajectory = [TrajectoryItem(**item) for item in data[\"trajectory\"]]\n    self.set_state(data[\"state\"])\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the internal variables of the intensifier.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the internal variables of the intensifier.\"\"\"\n    self._tf_seeds: list[int] = []\n    self._tf_instances: list[str | None] = []\n    self._tf_budgets: list[float | None] = []\n    self._instance_seed_keys: list[InstanceSeedKey] | None = None\n    self._instance_seed_keys_validation: list[InstanceSeedKey] | None = None\n\n    # Incumbent variables\n    self._incumbents: list[Configuration] = []\n    self._incumbents_changed = 0\n    self._rejected_config_ids: list[int] = []\n    self._trajectory: list[TrajectoryItem] = []\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.save","title":"save","text":"<pre><code>save(filename: str | Path) -&gt; None\n</code></pre> <p>Saves the current state of the intensifier. In addition to the state (retrieved by <code>get_state</code>), this method also saves the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def save(self, filename: str | Path) -&gt; None:\n    \"\"\"Saves the current state of the intensifier. In addition to the state (retrieved by ``get_state``), this\n    method also saves the incumbents and trajectory.\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    assert str(filename).endswith(\".json\")\n    filename.parent.mkdir(parents=True, exist_ok=True)\n\n    data = {\n        \"incumbent_ids\": [self.runhistory.get_config_id(config) for config in self._incumbents],\n        \"rejected_config_ids\": self._rejected_config_ids,\n        \"incumbents_changed\": self._incumbents_changed,\n        \"trajectory\": [dataclasses.asdict(item) for item in self._trajectory],\n        \"state\": self.get_state(),\n    }\n\n    with open(filename, \"w\") as fp:\n        json.dump(data, fp, indent=2, cls=NumpyEncoder)\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.set_state","title":"set_state","text":"<pre><code>set_state(state: dict[str, Any]) -&gt; None\n</code></pre> <p>Sets the state of the intensifier. Used to restore the state of the intensifier when continuing a run.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def set_state(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Sets the state of the intensifier. Used to restore the state of the intensifier when continuing a run.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/intensifier/abstract_intensifier/#smac.intensifier.abstract_intensifier.AbstractIntensifier.update_incumbents","title":"update_incumbents","text":"<pre><code>update_incumbents(config: Configuration) -&gt; None\n</code></pre> <p>Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only the affected config and the current incumbents are used, this method is very efficient. Furthermore, a configuration is only considered incumbent if it has a better performance on all incumbent instances.</p> <p>Crucially, if there is no incumbent (at the start) then, the first configuration assumes incumbent status. For the next configuration, we need to check if the configuration is better on all instances that have been evaluated for the incumbent. If this is the case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to obtain the missing instance-seed-budget combination or b) mark this configuration as inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().</p> <p>Notably, this method is written to support both multi-fidelity and multi-objective optimization. While the get_instance_seed_budget_keys() method and self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour, calculate_pareto_front() is used as a hard coded way to support multi-objective optimization, including the single objective as special case. calculate_pareto_front() is called on the set of all (in case of MO) incumbents amended with the challenger configuration, provided it has a sufficient overlap in seed-instance-budget combinations.</p> <p>Lastly, if we have a self._max_incumbents and the pareto front provides more than this specified amount, we cut the incumbents using crowding distance.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def update_incumbents(self, config: Configuration) -&gt; None:\n    \"\"\"Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only\n    the affected config and the current incumbents are used, this method is very efficient. Furthermore, a\n    configuration is only considered incumbent if it has a better performance on all incumbent instances.\n\n    Crucially, if there is no incumbent (at the start) then, the first configuration assumes\n    incumbent status. For the next configuration, we need to check if the configuration\n    is better on all instances that have been evaluated for the incumbent. If this is the\n    case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to\n    obtain the missing instance-seed-budget combination or b) mark this configuration as\n    inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by\n    self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().\n\n    Notably, this method is written to support both multi-fidelity and multi-objective\n    optimization. While the get_instance_seed_budget_keys() method and\n    self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour,\n    calculate_pareto_front() is used as a hard coded way to support multi-objective\n    optimization, including the single objective as special case. calculate_pareto_front()\n    is called on the set of all (in case of MO) incumbents amended with the challenger\n    configuration, provided it has a sufficient overlap in seed-instance-budget combinations.\n\n    Lastly, if we have a self._max_incumbents and the pareto front provides more than this\n    specified amount, we cut the incumbents using crowding distance.\n    \"\"\"\n    rh = self.runhistory\n\n    # What happens if a config was rejected, but it appears again? Give it another try even if it\n    # has already been evaluated? Yes!\n\n    # Associated trials and id\n    config_isb_keys = self.get_instance_seed_budget_keys(config)\n    config_id = rh.get_config_id(config)\n    config_hash = get_config_hash(config)\n\n    # We skip updating incumbents if no instances are available\n    # Note: This is especially the case if trials of a config are still running\n    # because if trials are running, the runhistory does not update the trials in the fast data structure\n    if len(config_isb_keys) == 0:\n        logger.debug(f\"No relevant instances evaluated for config {config_hash}. Updating incumbents is skipped.\")\n        return\n\n    # Now we get the incumbents and see which trials have been used\n    incumbents = self.get_incumbents()\n    incumbent_ids = [rh.get_config_id(c) for c in incumbents]\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    incumbent_isb_keys = self.get_incumbent_instance_seed_budget_keys()\n\n    # Save for later\n    previous_incumbents = incumbents.copy()\n    previous_incumbent_ids = incumbent_ids.copy()\n\n    # Little sanity check here for consistency\n    if len(incumbents) &gt; 0:\n        assert incumbent_isb_keys is not None\n        assert len(incumbent_isb_keys) &gt; 0\n\n    # If there are no incumbents at all, we just use the new config as new incumbent\n    # Problem: We can add running incumbents\n    if len(incumbents) == 0:  # incumbent_isb_keys is None and len(incumbents) == 0:\n        logger.info(f\"Added config {config_hash} as new incumbent because there are no incumbents yet.\")\n        self._update_trajectory([config])\n\n        # Nothing else to do\n        return\n\n    # Comparison keys\n    # This one is a bit tricky: We would have problems if we compare with budgets because we might have different\n    # scenarios (depending on the incumbent selection specified in Successive Halving).\n    # 1) Any budget/highest observed budget: We want to get rid of the budgets because if we know it is calculated\n    # on the same instance-seed already then we are ready to go. Imagine we would check for the same budgets,\n    # then the configs can not be compared although the user does not care on which budgets configurations have\n    # been evaluated.\n    # 2) Highest budget: We only want to compare the configs if they are evaluated on the highest budget.\n    # Here we do actually care about the budgets. Please see the ``get_instance_seed_budget_keys`` method from\n    # Successive Halving to get more information.\n    # Noitce: compare=True only takes effect when subclass implemented it. -- e.g. in SH it\n    # will remove the budgets from the keys.\n    config_isb_comparison_keys = self.get_instance_seed_budget_keys(config, compare=True)\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    config_incumbent_isb_comparison_keys = self.get_incumbent_instance_seed_budget_keys(compare=True)\n\n    # Now we have to check if the new config has been evaluated on the same keys as the incumbents\n    if not all([key in config_isb_comparison_keys for key in config_incumbent_isb_comparison_keys]):\n        # We can not tell if the new config is better/worse than the incumbents because it has not been\n        # evaluated on the necessary trials\n        logger.debug(\n            f\"Could not compare config {config_hash} with incumbents because it's evaluated on \"\n            f\"different trials.\"\n        )\n\n        # The config has to go to a queue now as it is a challenger and a potential incumbent\n        return\n    else:\n        # If all instances are available and the config is incumbent and even evaluated on more trials\n        # then there's nothing we can do\n        if config in incumbents and len(config_isb_keys) &gt; len(incumbent_isb_keys):\n            logger.debug(\n                \"Config is already an incumbent but can not be compared to other incumbents because \"\n                \"the others are missing trials.\"\n            )\n            return\n\n    # Add config to incumbents so that we compare only the new config and existing incumbents\n    if config not in incumbents:\n        incumbents.append(config)\n        incumbent_ids.append(config_id)\n\n    # Now we get all instance-seed-budget keys for each incumbent (they might be different when using budgets)\n    all_incumbent_isb_keys = []\n    for incumbent in incumbents:\n        all_incumbent_isb_keys.append(self.get_instance_seed_budget_keys(incumbent))\n\n    # We compare the incumbents now and only return the ones on the pareto front\n    new_incumbents = calculate_pareto_front(rh, incumbents, all_incumbent_isb_keys)\n    new_incumbent_ids = [rh.get_config_id(c) for c in new_incumbents]\n\n    if len(previous_incumbents) == len(new_incumbents):\n        if previous_incumbents == new_incumbents:\n            # No changes in the incumbents, we need this clause because we can't use set difference then\n            if config_id in new_incumbent_ids:\n                self._remove_rejected_config(config_id)\n            else:\n                # config worse than incumbents and thus rejected\n                self._add_rejected_config(config_id)\n            return\n        else:\n            # In this case, we have to determine which config replaced which incumbent and reject it\n            removed_incumbent_id = list(set(previous_incumbent_ids) - set(new_incumbent_ids))[0]\n            removed_incumbent_hash = get_config_hash(rh.get_config(removed_incumbent_id))\n            self._add_rejected_config(removed_incumbent_id)\n\n            if removed_incumbent_id == config_id:\n                logger.debug(\n                    f\"Rejected config {config_hash} because it is not better than the incumbents on \"\n                    f\"{len(config_isb_keys)} instances.\"\n                )\n            else:\n                self._remove_rejected_config(config_id)\n                logger.info(\n                    f\"Added config {config_hash} and rejected config {removed_incumbent_hash} as incumbent because \"\n                    f\"it is not better than the incumbents on {len(config_isb_keys)} instances: \"\n                )\n                print_config_changes(rh.get_config(removed_incumbent_id), config, logger=logger)\n    elif len(previous_incumbents) &lt; len(new_incumbents):\n        # Config becomes a new incumbent; nothing is rejected in this case\n        self._remove_rejected_config(config_id)\n        logger.info(\n            f\"Config {config_hash} is a new incumbent. \" f\"Total number of incumbents: {len(new_incumbents)}.\"\n        )\n    else:\n        # There might be situations that the incumbents might be removed because of updated cost information of\n        # config\n        for incumbent in previous_incumbents:\n            if incumbent not in new_incumbents:\n                self._add_rejected_config(incumbent)\n                logger.debug(\n                    f\"Removed incumbent {get_config_hash(incumbent)} because of the updated costs from config \"\n                    f\"{config_hash}.\"\n                )\n\n    # Cut incumbents: We only want to keep a specific number of incumbents\n    # We use the crowding distance for that\n    if len(new_incumbents) &gt; self._max_incumbents:\n        new_incumbents = sort_by_crowding_distance(rh, new_incumbents, all_incumbent_isb_keys)\n        new_incumbents = new_incumbents[: self._max_incumbents]\n\n        # or random?\n        # idx = self._rng.randint(0, len(new_incumbents))\n        # del new_incumbents[idx]\n        # del new_incumbent_ids[idx]\n\n        logger.info(\n            f\"Removed one incumbent using crowding distance because more than {self._max_incumbents} are \"\n            \"available.\"\n        )\n\n    self._update_trajectory(new_incumbents)\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/","title":"Hyperband","text":""},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband","title":"smac.intensifier.hyperband","text":""},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband","title":"Hyperband","text":"<pre><code>Hyperband(\n    scenario: Scenario,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\",\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>SuccessiveHalving</code></p> <p>See <code>SuccessiveHalving</code> for documentation.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\",\n    seed: int | None = None,\n):\n    super().__init__(\n        scenario=scenario,\n        n_seeds=n_seeds,\n        max_incumbents=max_incumbents,\n        seed=seed,\n    )\n\n    self._eta = eta\n    self._instance_seed_order = instance_seed_order\n    self._incumbent_selection = incumbent_selection\n    self._highest_observed_budget_only = False if incumbent_selection == \"any_budget\" else True\n\n    # Global variables derived from scenario\n    self._min_budget = self._scenario.min_budget\n    self._max_budget = self._scenario.max_budget\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.config_generator","title":"config_generator  <code>property</code>","text":"<pre><code>config_generator: Iterator[Configuration]\n</code></pre> <p>Based on the configuration selector, an iterator is returned that generates configurations.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.config_selector","title":"config_selector  <code>property</code> <code>writable</code>","text":"<pre><code>config_selector: ConfigSelector\n</code></pre> <p>The configuration selector for the intensifier.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.incumbents_changed","title":"incumbents_changed  <code>property</code>","text":"<pre><code>incumbents_changed: int\n</code></pre> <p>How often the incumbents have changed.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>Runhistory of the intensifier.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.trajectory","title":"trajectory  <code>property</code>","text":"<pre><code>trajectory: list[TrajectoryItem]\n</code></pre> <p>Returns the trajectory (changes of incumbents) of the optimization run.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.used_walltime","title":"used_walltime  <code>property</code> <code>writable</code>","text":"<pre><code>used_walltime: float\n</code></pre> <p>Returns used wallclock time.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_callback","title":"get_callback","text":"<pre><code>get_callback() -&gt; Callback\n</code></pre> <p>The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory (every time new information is available). Moreover, incorporating the callback here allows developers more options in the future.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_callback(self) -&gt; Callback:\n    \"\"\"The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory\n    (every time new information is available). Moreover, incorporating the callback here allows developers\n    more options in the future.\n    \"\"\"\n\n    class RunHistoryCallback(Callback):\n        def __init__(self, intensifier: AbstractIntensifier):\n            self.intensifier = intensifier\n\n        def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; None:\n            self.intensifier.update_incumbents(info.config)\n\n    return RunHistoryCallback(self)\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_incumbent","title":"get_incumbent","text":"<pre><code>get_incumbent() -&gt; Configuration | None\n</code></pre> <p>Returns the current incumbent in a single-objective setting.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent(self) -&gt; Configuration | None:\n    \"\"\"Returns the current incumbent in a single-objective setting.\"\"\"\n    if self._scenario.count_objectives() &gt; 1:\n        raise ValueError(\"Cannot get a single incumbent for multi-objective optimization.\")\n\n    if len(self._incumbents) == 0:\n        return None\n\n    assert len(self._incumbents) == 1\n    return self._incumbents[0]\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_incumbent_instance_seed_budget_key_differences","title":"get_incumbent_instance_seed_budget_key_differences","text":"<pre><code>get_incumbent_instance_seed_budget_key_differences(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>There are situations in which incumbents are evaluated on more trials than others. This method returns the instances that are not part of the lowest intersection of instances for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_key_differences(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"There are situations in which incumbents are evaluated on more trials than others. This method returns the\n    instances that are not part of the lowest intersection of instances for all incumbents.\n    \"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the differences so that we can evaluate the other incumbents on the same instances\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n\n        if len(incumbent_isb_keys) &lt;= 1:\n            return []\n\n        # Compute the actual differences\n        intersection_isb_keys = set.intersection(*map(set, incumbent_isb_keys))  # type: ignore\n        union_isb_keys = set.union(*map(set, incumbent_isb_keys))  # type: ignore\n        incumbent_isb_keys = list(union_isb_keys - intersection_isb_keys)  # type: ignore\n\n        if len(incumbent_isb_keys) == 0:\n            return []\n\n        return incumbent_isb_keys  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_incumbent_instance_seed_budget_keys","title":"get_incumbent_instance_seed_budget_keys","text":"<pre><code>get_incumbent_instance_seed_budget_keys(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Find the lowest intersection of instance-seed-budget keys for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_keys(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Find the lowest intersection of instance-seed-budget keys for all incumbents.\"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the smallest set of trials that is used by all incumbents\n        # Reason: We can not fairly compare otherwise\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n        instances = list(set.intersection(*map(set, incumbent_isb_keys)))  # type: ignore\n\n        return instances  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_incumbents","title":"get_incumbents","text":"<pre><code>get_incumbents(\n    sort_by: str | None = None,\n) -&gt; list[Configuration]\n</code></pre> <p>Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective optimization, only one incumbent (if is) is returned.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_incumbents--returns","title":"Returns","text":"<p>configs : list[Configuration]     The configs of the Pareto front. sort_by : str, defaults to None     Sort the trials by <code>cost</code> (lowest cost first) or <code>num_trials</code> (config with lowest number of trials     first).</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbents(self, sort_by: str | None = None) -&gt; list[Configuration]:\n    \"\"\"Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective\n    optimization, only one incumbent (if is) is returned.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        The configs of the Pareto front.\n    sort_by : str, defaults to None\n        Sort the trials by ``cost`` (lowest cost first) or ``num_trials`` (config with lowest number of trials\n        first).\n    \"\"\"\n    rh = self.runhistory\n\n    if sort_by == \"cost\":\n        return list(sorted(self._incumbents, key=lambda config: rh._cost_per_config[rh.get_config_id(config)]))\n    elif sort_by == \"num_trials\":\n        return list(sorted(self._incumbents, key=lambda config: len(rh.get_trials(config))))\n    elif sort_by is None:\n        return list(self._incumbents)\n    else:\n        raise ValueError(f\"Unknown sort_by value: {sort_by}.\")\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_instance_seed_budget_keys","title":"get_instance_seed_budget_keys","text":"<pre><code>get_instance_seed_budget_keys(\n    config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Returns the instance-seed-budget keys for a given configuration. This method supports <code>highest_budget</code>, which only returns the instance-seed-budget keys for the highest budget (if specified). In this case, the incumbents in <code>update_incumbents</code> are only changed if the costs on the highest budget are lower.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_instance_seed_budget_keys--parameters","title":"Parameters","text":"<p>config: Configuration     The Configuration to be queried compare : bool, defaults to False     Get rid of the budget information for comparing if the configuration was evaluated on the same     instance-seed keys.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def get_instance_seed_budget_keys(\n    self, config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Returns the instance-seed-budget keys for a given configuration. This method supports ``highest_budget``,\n    which only returns the instance-seed-budget keys for the highest budget (if specified). In this case, the\n    incumbents in ``update_incumbents`` are only changed if the costs on the highest budget are lower.\n\n    Parameters\n    ----------\n    config: Configuration\n        The Configuration to be queried\n    compare : bool, defaults to False\n        Get rid of the budget information for comparing if the configuration was evaluated on the same\n        instance-seed keys.\n    \"\"\"\n    isb_keys = self.runhistory.get_instance_seed_budget_keys(\n        config, highest_observed_budget_only=self._highest_observed_budget_only\n    )\n\n    # If incumbent should only be changed on the highest budget, we have to kick out all budgets below the highest\n    if self.uses_budgets and self._incumbent_selection == \"highest_budget\":\n        isb_keys = [key for key in isb_keys if key.budget == self._max_budget]\n\n    if compare:\n        # Get rid of duplicates\n        isb_keys = list(\n            set([InstanceSeedBudgetKey(instance=key.instance, seed=key.seed, budget=None) for key in isb_keys])\n        )\n\n    return isb_keys\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_instance_seed_keys_of_interest","title":"get_instance_seed_keys_of_interest","text":"<pre><code>get_instance_seed_keys_of_interest(\n    *, validate: bool = False, seed: int | None = None\n) -&gt; list[InstanceSeedKey]\n</code></pre> <p>Returns a list of instance-seed keys. Considers seeds and instances from the runhistory (<code>self._tf_seeds</code> and <code>self._tf_instances</code>). If no seeds or instances were found, new seeds and instances are generated based on the global intensifier seed.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_instance_seed_keys_of_interest--warning","title":"Warning","text":"<p>The passed seed is only used for validation. For training, the global intensifier seed is used.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_instance_seed_keys_of_interest--parameters","title":"Parameters","text":"<p>validate : bool, defaults to False     Whether to get validation trials or training trials. The only difference lies in different seeds. seed : int | None, defaults to None     The seed used for the validation trials.</p>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_instance_seed_keys_of_interest--returns","title":"Returns","text":"<p>instance_seed_keys : list[InstanceSeedKey]     Instance-seed keys of interest.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_instance_seed_keys_of_interest(\n    self,\n    *,\n    validate: bool = False,\n    seed: int | None = None,\n) -&gt; list[InstanceSeedKey]:\n    \"\"\"Returns a list of instance-seed keys. Considers seeds and instances from the\n    runhistory (``self._tf_seeds`` and ``self._tf_instances``). If no seeds or instances were found, new\n    seeds and instances are generated based on the global intensifier seed.\n\n    Warning\n    -------\n    The passed seed is only used for validation. For training, the global intensifier seed is used.\n\n    Parameters\n    ----------\n    validate : bool, defaults to False\n        Whether to get validation trials or training trials. The only difference lies in different seeds.\n    seed : int | None, defaults to None\n        The seed used for the validation trials.\n\n    Returns\n    -------\n    instance_seed_keys : list[InstanceSeedKey]\n        Instance-seed keys of interest.\n    \"\"\"\n    if self._runhistory is None:\n        raise RuntimeError(\"Please set the runhistory before calling this method.\")\n\n    if len(self._tf_instances) == 0:\n        raise RuntimeError(\"Please call __post_init__ before calling this method.\")\n\n    if seed is None:\n        seed = 0\n\n    # We cache the instance-seed keys for efficiency and consistency reasons\n    if (self._instance_seed_keys is None and not validate) or (\n        self._instance_seed_keys_validation is None and validate\n    ):\n        instance_seed_keys: list[InstanceSeedKey] = []\n        if validate:\n            rng = np.random.RandomState(seed)\n        else:\n            rng = self._rng\n\n        i = 0\n        while True:\n            found_enough_configs = (\n                self._max_config_calls is not None and len(instance_seed_keys) &gt;= self._max_config_calls\n            )\n            used_enough_seeds = self._n_seeds is not None and i &gt;= self._n_seeds\n\n            if found_enough_configs or used_enough_seeds:\n                break\n\n            if validate:\n                next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n            else:\n                try:\n                    next_seed = self._tf_seeds[i]\n                    logger.info(f\"Added existing seed {next_seed} from runhistory to the intensifier.\")\n                except IndexError:\n                    # Use global random generator for a new seed and mark it so it will be reused for another config\n                    next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n\n                    # This line here is really important because we don't want to add the same seed twice\n                    if next_seed in self._tf_seeds:\n                        continue\n\n                    self._tf_seeds.append(next_seed)\n                    logger.debug(f\"Added a new random seed {next_seed} to the intensifier.\")\n\n            # If no instances are used, tf_instances includes None\n            for instance in self._tf_instances:\n                instance_seed_keys.append(InstanceSeedKey(instance, next_seed))\n\n            # Only use one seed in deterministic case\n            if self._scenario.deterministic:\n                logger.info(\"Using only one seed for deterministic scenario.\")\n                break\n\n            # Seed counter\n            i += 1\n\n        # Now we cut so that we only have max_config_calls instance_seed_keys\n        # We favor instances over seeds here: That makes sure we always work with the same instance/seed pairs\n        if self._max_config_calls is not None:\n            if len(instance_seed_keys) &gt; self._max_config_calls:\n                instance_seed_keys = instance_seed_keys[: self._max_config_calls]\n                logger.info(f\"Cut instance-seed keys to {self._max_config_calls} entries.\")\n\n        # Set it globally\n        if not validate:\n            self._instance_seed_keys = instance_seed_keys\n        else:\n            self._instance_seed_keys_validation = instance_seed_keys\n\n    if not validate:\n        assert self._instance_seed_keys is not None\n        instance_seed_keys = self._instance_seed_keys\n    else:\n        assert self._instance_seed_keys_validation is not None\n        instance_seed_keys = self._instance_seed_keys_validation\n\n    return instance_seed_keys.copy()\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.get_rejected_configs","title":"get_rejected_configs","text":"<pre><code>get_rejected_configs() -&gt; list[Configuration]\n</code></pre> <p>Returns rejected configurations when racing against the incumbent failed.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_rejected_configs(self) -&gt; list[Configuration]:\n    \"\"\"Returns rejected configurations when racing against the incumbent failed.\"\"\"\n    configs = []\n    for rejected_config_id in self._rejected_config_ids:\n        configs.append(self.runhistory._ids_config[rejected_config_id])\n\n    return configs\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.load","title":"load","text":"<pre><code>load(filename: str | Path) -&gt; None\n</code></pre> <p>Loads the latest state of the intensifier including the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def load(self, filename: str | Path) -&gt; None:\n    \"\"\"Loads the latest state of the intensifier including the incumbents and trajectory.\"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    try:\n        with open(filename) as fp:\n            data = json.load(fp)\n    except Exception as e:\n        logger.warning(\n            f\"Encountered exception {e} while reading runhistory from {filename}. Not adding any trials!\"\n        )\n        return\n\n    # We reset the intensifier and then reset the runhistory\n    self.reset()\n    if self._runhistory is not None:\n        self.runhistory = self._runhistory\n\n    self._incumbents = [self.runhistory.get_config(config_id) for config_id in data[\"incumbent_ids\"]]\n    self._incumbents_changed = data[\"incumbents_changed\"]\n    self._rejected_config_ids = data[\"rejected_config_ids\"]\n    self._trajectory = [TrajectoryItem(**item) for item in data[\"trajectory\"]]\n    self.set_state(data[\"state\"])\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.print_tracker","title":"print_tracker","text":"<pre><code>print_tracker() -&gt; None\n</code></pre> <p>Prints the number of configurations in each bracket/stage.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def print_tracker(self) -&gt; None:\n    \"\"\"Prints the number of configurations in each bracket/stage.\"\"\"\n    messages = []\n    for (bracket, stage), others in self._tracker.items():\n        counter = 0\n        for _, config_ids in others:\n            counter += len(config_ids)\n\n        if counter &gt; 0:\n            messages.append(f\"--- Bracket {bracket} / Stage {stage}: {counter} configs\")\n\n    if len(messages) &gt; 0:\n        logger.debug(f\"{self.__class__.__name__} statistics:\")\n\n    for message in messages:\n        logger.debug(message)\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Resets the internal variables of the intensifier, including the tracker and the next bracket.</p> Source code in <code>smac/intensifier/hyperband.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Resets the internal variables of the intensifier, including the tracker and the next bracket.\"\"\"\n    super().reset()\n\n    # Reset current bracket\n    self._next_bracket: int = 0\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.save","title":"save","text":"<pre><code>save(filename: str | Path) -&gt; None\n</code></pre> <p>Saves the current state of the intensifier. In addition to the state (retrieved by <code>get_state</code>), this method also saves the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def save(self, filename: str | Path) -&gt; None:\n    \"\"\"Saves the current state of the intensifier. In addition to the state (retrieved by ``get_state``), this\n    method also saves the incumbents and trajectory.\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    assert str(filename).endswith(\".json\")\n    filename.parent.mkdir(parents=True, exist_ok=True)\n\n    data = {\n        \"incumbent_ids\": [self.runhistory.get_config_id(config) for config in self._incumbents],\n        \"rejected_config_ids\": self._rejected_config_ids,\n        \"incumbents_changed\": self._incumbents_changed,\n        \"trajectory\": [dataclasses.asdict(item) for item in self._trajectory],\n        \"state\": self.get_state(),\n    }\n\n    with open(filename, \"w\") as fp:\n        json.dump(data, fp, indent=2, cls=NumpyEncoder)\n</code></pre>"},{"location":"api/smac/intensifier/hyperband/#smac.intensifier.hyperband.Hyperband.update_incumbents","title":"update_incumbents","text":"<pre><code>update_incumbents(config: Configuration) -&gt; None\n</code></pre> <p>Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only the affected config and the current incumbents are used, this method is very efficient. Furthermore, a configuration is only considered incumbent if it has a better performance on all incumbent instances.</p> <p>Crucially, if there is no incumbent (at the start) then, the first configuration assumes incumbent status. For the next configuration, we need to check if the configuration is better on all instances that have been evaluated for the incumbent. If this is the case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to obtain the missing instance-seed-budget combination or b) mark this configuration as inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().</p> <p>Notably, this method is written to support both multi-fidelity and multi-objective optimization. While the get_instance_seed_budget_keys() method and self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour, calculate_pareto_front() is used as a hard coded way to support multi-objective optimization, including the single objective as special case. calculate_pareto_front() is called on the set of all (in case of MO) incumbents amended with the challenger configuration, provided it has a sufficient overlap in seed-instance-budget combinations.</p> <p>Lastly, if we have a self._max_incumbents and the pareto front provides more than this specified amount, we cut the incumbents using crowding distance.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def update_incumbents(self, config: Configuration) -&gt; None:\n    \"\"\"Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only\n    the affected config and the current incumbents are used, this method is very efficient. Furthermore, a\n    configuration is only considered incumbent if it has a better performance on all incumbent instances.\n\n    Crucially, if there is no incumbent (at the start) then, the first configuration assumes\n    incumbent status. For the next configuration, we need to check if the configuration\n    is better on all instances that have been evaluated for the incumbent. If this is the\n    case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to\n    obtain the missing instance-seed-budget combination or b) mark this configuration as\n    inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by\n    self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().\n\n    Notably, this method is written to support both multi-fidelity and multi-objective\n    optimization. While the get_instance_seed_budget_keys() method and\n    self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour,\n    calculate_pareto_front() is used as a hard coded way to support multi-objective\n    optimization, including the single objective as special case. calculate_pareto_front()\n    is called on the set of all (in case of MO) incumbents amended with the challenger\n    configuration, provided it has a sufficient overlap in seed-instance-budget combinations.\n\n    Lastly, if we have a self._max_incumbents and the pareto front provides more than this\n    specified amount, we cut the incumbents using crowding distance.\n    \"\"\"\n    rh = self.runhistory\n\n    # What happens if a config was rejected, but it appears again? Give it another try even if it\n    # has already been evaluated? Yes!\n\n    # Associated trials and id\n    config_isb_keys = self.get_instance_seed_budget_keys(config)\n    config_id = rh.get_config_id(config)\n    config_hash = get_config_hash(config)\n\n    # We skip updating incumbents if no instances are available\n    # Note: This is especially the case if trials of a config are still running\n    # because if trials are running, the runhistory does not update the trials in the fast data structure\n    if len(config_isb_keys) == 0:\n        logger.debug(f\"No relevant instances evaluated for config {config_hash}. Updating incumbents is skipped.\")\n        return\n\n    # Now we get the incumbents and see which trials have been used\n    incumbents = self.get_incumbents()\n    incumbent_ids = [rh.get_config_id(c) for c in incumbents]\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    incumbent_isb_keys = self.get_incumbent_instance_seed_budget_keys()\n\n    # Save for later\n    previous_incumbents = incumbents.copy()\n    previous_incumbent_ids = incumbent_ids.copy()\n\n    # Little sanity check here for consistency\n    if len(incumbents) &gt; 0:\n        assert incumbent_isb_keys is not None\n        assert len(incumbent_isb_keys) &gt; 0\n\n    # If there are no incumbents at all, we just use the new config as new incumbent\n    # Problem: We can add running incumbents\n    if len(incumbents) == 0:  # incumbent_isb_keys is None and len(incumbents) == 0:\n        logger.info(f\"Added config {config_hash} as new incumbent because there are no incumbents yet.\")\n        self._update_trajectory([config])\n\n        # Nothing else to do\n        return\n\n    # Comparison keys\n    # This one is a bit tricky: We would have problems if we compare with budgets because we might have different\n    # scenarios (depending on the incumbent selection specified in Successive Halving).\n    # 1) Any budget/highest observed budget: We want to get rid of the budgets because if we know it is calculated\n    # on the same instance-seed already then we are ready to go. Imagine we would check for the same budgets,\n    # then the configs can not be compared although the user does not care on which budgets configurations have\n    # been evaluated.\n    # 2) Highest budget: We only want to compare the configs if they are evaluated on the highest budget.\n    # Here we do actually care about the budgets. Please see the ``get_instance_seed_budget_keys`` method from\n    # Successive Halving to get more information.\n    # Noitce: compare=True only takes effect when subclass implemented it. -- e.g. in SH it\n    # will remove the budgets from the keys.\n    config_isb_comparison_keys = self.get_instance_seed_budget_keys(config, compare=True)\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    config_incumbent_isb_comparison_keys = self.get_incumbent_instance_seed_budget_keys(compare=True)\n\n    # Now we have to check if the new config has been evaluated on the same keys as the incumbents\n    if not all([key in config_isb_comparison_keys for key in config_incumbent_isb_comparison_keys]):\n        # We can not tell if the new config is better/worse than the incumbents because it has not been\n        # evaluated on the necessary trials\n        logger.debug(\n            f\"Could not compare config {config_hash} with incumbents because it's evaluated on \"\n            f\"different trials.\"\n        )\n\n        # The config has to go to a queue now as it is a challenger and a potential incumbent\n        return\n    else:\n        # If all instances are available and the config is incumbent and even evaluated on more trials\n        # then there's nothing we can do\n        if config in incumbents and len(config_isb_keys) &gt; len(incumbent_isb_keys):\n            logger.debug(\n                \"Config is already an incumbent but can not be compared to other incumbents because \"\n                \"the others are missing trials.\"\n            )\n            return\n\n    # Add config to incumbents so that we compare only the new config and existing incumbents\n    if config not in incumbents:\n        incumbents.append(config)\n        incumbent_ids.append(config_id)\n\n    # Now we get all instance-seed-budget keys for each incumbent (they might be different when using budgets)\n    all_incumbent_isb_keys = []\n    for incumbent in incumbents:\n        all_incumbent_isb_keys.append(self.get_instance_seed_budget_keys(incumbent))\n\n    # We compare the incumbents now and only return the ones on the pareto front\n    new_incumbents = calculate_pareto_front(rh, incumbents, all_incumbent_isb_keys)\n    new_incumbent_ids = [rh.get_config_id(c) for c in new_incumbents]\n\n    if len(previous_incumbents) == len(new_incumbents):\n        if previous_incumbents == new_incumbents:\n            # No changes in the incumbents, we need this clause because we can't use set difference then\n            if config_id in new_incumbent_ids:\n                self._remove_rejected_config(config_id)\n            else:\n                # config worse than incumbents and thus rejected\n                self._add_rejected_config(config_id)\n            return\n        else:\n            # In this case, we have to determine which config replaced which incumbent and reject it\n            removed_incumbent_id = list(set(previous_incumbent_ids) - set(new_incumbent_ids))[0]\n            removed_incumbent_hash = get_config_hash(rh.get_config(removed_incumbent_id))\n            self._add_rejected_config(removed_incumbent_id)\n\n            if removed_incumbent_id == config_id:\n                logger.debug(\n                    f\"Rejected config {config_hash} because it is not better than the incumbents on \"\n                    f\"{len(config_isb_keys)} instances.\"\n                )\n            else:\n                self._remove_rejected_config(config_id)\n                logger.info(\n                    f\"Added config {config_hash} and rejected config {removed_incumbent_hash} as incumbent because \"\n                    f\"it is not better than the incumbents on {len(config_isb_keys)} instances: \"\n                )\n                print_config_changes(rh.get_config(removed_incumbent_id), config, logger=logger)\n    elif len(previous_incumbents) &lt; len(new_incumbents):\n        # Config becomes a new incumbent; nothing is rejected in this case\n        self._remove_rejected_config(config_id)\n        logger.info(\n            f\"Config {config_hash} is a new incumbent. \" f\"Total number of incumbents: {len(new_incumbents)}.\"\n        )\n    else:\n        # There might be situations that the incumbents might be removed because of updated cost information of\n        # config\n        for incumbent in previous_incumbents:\n            if incumbent not in new_incumbents:\n                self._add_rejected_config(incumbent)\n                logger.debug(\n                    f\"Removed incumbent {get_config_hash(incumbent)} because of the updated costs from config \"\n                    f\"{config_hash}.\"\n                )\n\n    # Cut incumbents: We only want to keep a specific number of incumbents\n    # We use the crowding distance for that\n    if len(new_incumbents) &gt; self._max_incumbents:\n        new_incumbents = sort_by_crowding_distance(rh, new_incumbents, all_incumbent_isb_keys)\n        new_incumbents = new_incumbents[: self._max_incumbents]\n\n        # or random?\n        # idx = self._rng.randint(0, len(new_incumbents))\n        # del new_incumbents[idx]\n        # del new_incumbent_ids[idx]\n\n        logger.info(\n            f\"Removed one incumbent using crowding distance because more than {self._max_incumbents} are \"\n            \"available.\"\n        )\n\n    self._update_trajectory(new_incumbents)\n</code></pre>"},{"location":"api/smac/intensifier/hyperband_utils/","title":"Hyperband utils","text":""},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils","title":"smac.intensifier.hyperband_utils","text":""},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.determine_HB","title":"determine_HB","text":"<pre><code>determine_HB(\n    min_budget: float, max_budget: float, eta: int = 3\n) -&gt; dict\n</code></pre> <p>Determine one Hyperband round</p>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.determine_HB--parameters","title":"Parameters","text":"<p>min_budget : float     Minimum budget per trial in fidelity units max_budget : float     Maximum budget per trial in fidelity units eta : int, defaults to 3     Input that controls the proportion of configurations discarded in each round of Successive Halving.</p>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.determine_HB--returns","title":"Returns","text":"<p>dict     Info about the Hyperband round         \"max_iterations\"         \"n_configs_in_stage\"         \"budgets_in_stage\"         \"trials_used\"         \"budget_used\"         \"number_of_brackets\"</p> Source code in <code>smac/intensifier/hyperband_utils.py</code> <pre><code>def determine_HB(min_budget: float, max_budget: float, eta: int = 3) -&gt; dict:\n    \"\"\"Determine one Hyperband round\n\n    Parameters\n    ----------\n    min_budget : float\n        Minimum budget per trial in fidelity units\n    max_budget : float\n        Maximum budget per trial in fidelity units\n    eta : int, defaults to 3\n        Input that controls the proportion of configurations discarded in each round of Successive Halving.\n\n    Returns\n    -------\n    dict\n        Info about the Hyperband round\n            \"max_iterations\"\n            \"n_configs_in_stage\"\n            \"budgets_in_stage\"\n            \"trials_used\"\n            \"budget_used\"\n            \"number_of_brackets\"\n\n    \"\"\"\n    _s_max = SuccessiveHalving._get_max_iterations(eta, max_budget, min_budget)\n\n    _max_iterations: dict[int, int] = {}\n    _n_configs_in_stage: dict[int, list] = {}\n    _budgets_in_stage: dict[int, list] = {}\n\n    for i in range(_s_max + 1):\n        max_iter = _s_max - i\n\n        _budgets_in_stage[i], _n_configs_in_stage[i] = SuccessiveHalving._compute_configs_and_budgets_for_stages(\n            eta, max_budget, max_iter, _s_max\n        )\n        _max_iterations[i] = max_iter + 1\n\n    total_trials = np.sum([np.sum(v) for v in _n_configs_in_stage.values()])\n    total_budget = np.sum([np.sum(v) for v in _budgets_in_stage.values()])\n\n    return {\n        \"max_iterations\": _max_iterations,\n        \"n_configs_in_stage\": _n_configs_in_stage,\n        \"budgets_in_stage\": _budgets_in_stage,\n        \"trials_used\": total_trials,\n        \"budget_used\": total_budget,\n        \"number_of_brackets\": len(_max_iterations),\n    }\n</code></pre>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.determine_hyperband_for_multifidelity","title":"determine_hyperband_for_multifidelity","text":"<pre><code>determine_hyperband_for_multifidelity(\n    total_budget: float,\n    min_budget: float,\n    max_budget: float,\n    eta: int = 3,\n) -&gt; dict\n</code></pre> <p>Determine how many Hyperband rounds should happen based on a total budget</p>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.determine_hyperband_for_multifidelity--parameters","title":"Parameters","text":"<p>total_budget : float     Total budget for the complete optimization in fidelity units min_budget : float     Minimum budget per trial in fidelity units max_budget : float     Maximum budget per trial in fidelity units eta : int, defaults to 3     Input that controls the proportion of configurations discarded in each round of Successive Halving.</p>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.determine_hyperband_for_multifidelity--returns","title":"Returns","text":"<p>dict     Info about one Hyperband round         \"max_iterations\"         \"n_configs_in_stage\"         \"budgets_in_stage\"         \"trials_used\"         \"budget_used\"         \"number_of_brackets\"     Info about whole optimization         \"n_trials\"         \"total_budget\"         \"eta\"         \"min_budget\"         \"max_budget\"</p> Source code in <code>smac/intensifier/hyperband_utils.py</code> <pre><code>def determine_hyperband_for_multifidelity(\n    total_budget: float, min_budget: float, max_budget: float, eta: int = 3\n) -&gt; dict:\n    \"\"\"Determine how many Hyperband rounds should happen based on a total budget\n\n    Parameters\n    ----------\n    total_budget : float\n        Total budget for the complete optimization in fidelity units\n    min_budget : float\n        Minimum budget per trial in fidelity units\n    max_budget : float\n        Maximum budget per trial in fidelity units\n    eta : int, defaults to 3\n        Input that controls the proportion of configurations discarded in each round of Successive Halving.\n\n    Returns\n    -------\n    dict\n        Info about one Hyperband round\n            \"max_iterations\"\n            \"n_configs_in_stage\"\n            \"budgets_in_stage\"\n            \"trials_used\"\n            \"budget_used\"\n            \"number_of_brackets\"\n        Info about whole optimization\n            \"n_trials\"\n            \"total_budget\"\n            \"eta\"\n            \"min_budget\"\n            \"max_budget\"\n\n    \"\"\"\n    # Determine the HB\n    hyperband_round = determine_HB(eta=eta, min_budget=min_budget, max_budget=max_budget)\n\n    # Calculate how many HB rounds we can have\n    budget_used_per_hyperband_round = hyperband_round[\"budget_used\"]\n    number_of_full_hb_rounds = int(np.floor(total_budget / budget_used_per_hyperband_round))\n    remaining_budget = total_budget % budget_used_per_hyperband_round\n    trials_used_per_hb_round = hyperband_round[\"trials_used\"]\n    n_configs_in_stage = hyperband_round[\"n_configs_in_stage\"]\n    budgets_in_stage = hyperband_round[\"budgets_in_stage\"]\n\n    remaining_trials = 0\n    for stage in n_configs_in_stage.keys():\n        B = budgets_in_stage[stage]\n        C = n_configs_in_stage[stage]\n        for b, c in zip(B, C):\n            # How many trials are left?\n            # If b * c is lower than remaining budget, we can add full c\n            # otherwise we need to find out how many trials we can do with this budget\n            remaining_trials += min(c, int(np.floor(remaining_budget / b)))\n            # We cannot go lower than 0\n            # If we are in the case of b*c &gt; remaining_budget, we will not have any\n            # budget left. We can not add full c but the number of trials that still fit\n            remaining_budget = max(0, remaining_budget - b * c)\n\n    n_trials = int(number_of_full_hb_rounds * trials_used_per_hb_round + remaining_trials)\n\n    hyperband_info = hyperband_round\n    hyperband_info[\"n_trials\"] = n_trials\n    hyperband_info[\"total_budget\"] = total_budget\n    hyperband_info[\"eta\"] = eta\n    hyperband_info[\"min_budget\"] = min_budget\n    hyperband_info[\"max_budget\"] = max_budget\n\n    return hyperband_info\n</code></pre>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.get_n_trials_for_hyperband_multifidelity","title":"get_n_trials_for_hyperband_multifidelity","text":"<pre><code>get_n_trials_for_hyperband_multifidelity(\n    total_budget: float,\n    min_budget: float,\n    max_budget: float,\n    eta: int = 3,\n    print_summary: bool = True,\n) -&gt; int\n</code></pre> <p>Calculate the number of trials needed for multi-fidelity optimization</p> <p>Specify the total budget and find out how many trials that equals.</p>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.get_n_trials_for_hyperband_multifidelity--parameters","title":"Parameters","text":"<p>total_budget : float     Total budget for the complete optimization in fidelity units.     A fidelity unit can be one epoch or a fraction of a dataset size. min_budget : float     Minimum budget per trial in fidelity units max_budget : float     Maximum budget per trial in fidelity units eta : int, defaults to 3     Input that controls the proportion of configurations discarded in each round of Successive Halving.</p>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.get_n_trials_for_hyperband_multifidelity--returns","title":"Returns","text":"<p>int     Number of trials needed for the specified total budgets</p> Source code in <code>smac/intensifier/hyperband_utils.py</code> <pre><code>def get_n_trials_for_hyperband_multifidelity(\n    total_budget: float, min_budget: float, max_budget: float, eta: int = 3, print_summary: bool = True\n) -&gt; int:\n    \"\"\"Calculate the number of trials needed for multi-fidelity optimization\n\n    Specify the total budget and find out how many trials that equals.\n\n    Parameters\n    ----------\n    total_budget : float\n        Total budget for the complete optimization in fidelity units.\n        A fidelity unit can be one epoch or a fraction of a dataset size.\n    min_budget : float\n        Minimum budget per trial in fidelity units\n    max_budget : float\n        Maximum budget per trial in fidelity units\n    eta : int, defaults to 3\n        Input that controls the proportion of configurations discarded in each round of Successive Halving.\n\n    Returns\n    -------\n    int\n        Number of trials needed for the specified total budgets\n    \"\"\"\n    hyperband_info = determine_hyperband_for_multifidelity(\n        total_budget=total_budget, eta=eta, min_budget=min_budget, max_budget=max_budget\n    )\n    if print_summary:\n        print_hyperband_summary(hyperband_info=hyperband_info)\n    return hyperband_info[\"n_trials\"]\n</code></pre>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.print_hyperband_summary","title":"print_hyperband_summary","text":"<pre><code>print_hyperband_summary(hyperband_info: dict) -&gt; None\n</code></pre> <p>Print summary about Hyperband as used in the MultiFidelityFacade</p>"},{"location":"api/smac/intensifier/hyperband_utils/#smac.intensifier.hyperband_utils.print_hyperband_summary--parameters","title":"Parameters","text":"<p>hyperband_info : dict     Info dict about Hyperband</p> Source code in <code>smac/intensifier/hyperband_utils.py</code> <pre><code>def print_hyperband_summary(hyperband_info: dict) -&gt; None:\n    \"\"\"Print summary about Hyperband as used in the MultiFidelityFacade\n\n    Parameters\n    ----------\n    hyperband_info : dict\n        Info dict about Hyperband\n    \"\"\"\n    print(\"-\" * 30, \"HYPERBAND IN MULTI-FIDELITY\", \"-\" * 30)\n    print(\"total budget:\\t\\t\", hyperband_info[\"total_budget\"])\n    print(\"total number of trials:\\t\", hyperband_info[\"n_trials\"])\n    print(\"number of HB rounds:\\t\", hyperband_info[\"total_budget\"] / hyperband_info[\"budget_used\"])\n    print()\n\n    print(\"\\t~~~~~~~~~~~~HYPERBAND ROUND\")\n    print(\"\\teta:\\t\\t\\t\\t\\t\", hyperband_info[\"eta\"])\n    print(\"\\tmin budget per trial:\\t\\t\\t\", hyperband_info[\"min_budget\"])\n    print(\"\\tmax budget per trial:\\t\\t\\t\", hyperband_info[\"max_budget\"])\n    print(\"\\ttotal number of trials per HB round:\\t\", hyperband_info[\"trials_used\"])\n    print(\"\\tbudget used per HB round:\\t\\t\", hyperband_info[\"budget_used\"])\n    print(\"\\tnumber of brackets:\\t\\t\\t\", hyperband_info[\"number_of_brackets\"])\n    print(\"\\tbudgets per stage:\\t\\t\\t\", hyperband_info[\"budgets_in_stage\"])\n    print(\"\\tn configs per stage:\\t\\t\\t\", hyperband_info[\"n_configs_in_stage\"])\n    print(\"-\" * (2 * 30 + len(\"HYPERBAND IN MULTI-FIDELITY\") + 2))\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/","title":"Intensifier","text":""},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier","title":"smac.intensifier.intensifier","text":""},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier","title":"Intensifier","text":"<pre><code>Intensifier(\n    scenario: Scenario,\n    max_config_calls: int = 3,\n    max_incumbents: int = 10,\n    retries: int = 16,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractIntensifier</code></p> <p>Implementation of an intensifier supporting multi-fidelity, multi-objective, and multi-processing. Races challengers against current incumbents.</p> <p>The behaviour of this intensifier is as follows:</p> <ul> <li>First, adds configs from the runhistory to the queue with N=1 (they will be ignored if they are already   evaluated).</li> <li> <p>While loop:</p> </li> <li> <p>If queue is empty: Intensifies exactly one more instance of one incumbent and samples a new configuration     afterwards.</p> </li> <li>If queue is not empty: Configs in the queue are evaluated on N=(N*2) instances if they might be better     than the incumbents. If not, they are removed from the queue and rejected forever.</li> </ul>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier--parameters","title":"Parameters","text":"<p>max_config_calls : int, defaults to 3     Maximum number of configuration evaluations. Basically, how many instance-seed keys should be maxed evaluated     for a configuration. max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective. retries : int, defaults to 16     How many more iterations should be done in case no new trial is found. seed : int, defaults to None     Internal seed used for random events, like shuffle seeds.</p> Source code in <code>smac/intensifier/intensifier.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    max_config_calls: int = 3,\n    max_incumbents: int = 10,\n    retries: int = 16,\n    seed: int | None = None,\n):\n    super().__init__(scenario=scenario, max_config_calls=max_config_calls, max_incumbents=max_incumbents, seed=seed)\n    self._retries = retries\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.config_generator","title":"config_generator  <code>property</code>","text":"<pre><code>config_generator: Iterator[Configuration]\n</code></pre> <p>Based on the configuration selector, an iterator is returned that generates configurations.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.config_selector","title":"config_selector  <code>property</code> <code>writable</code>","text":"<pre><code>config_selector: ConfigSelector\n</code></pre> <p>The configuration selector for the intensifier.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.incumbents_changed","title":"incumbents_changed  <code>property</code>","text":"<pre><code>incumbents_changed: int\n</code></pre> <p>How often the incumbents have changed.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>Runhistory of the intensifier.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.trajectory","title":"trajectory  <code>property</code>","text":"<pre><code>trajectory: list[TrajectoryItem]\n</code></pre> <p>Returns the trajectory (changes of incumbents) of the optimization run.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.used_walltime","title":"used_walltime  <code>property</code> <code>writable</code>","text":"<pre><code>used_walltime: float\n</code></pre> <p>Returns used wallclock time.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[TrialInfo]\n</code></pre> <p>This iter method holds the logic for the intensification loop. Some facts about the loop:</p> <ul> <li>Adds existing configurations from the runhistory to the queue (that means it supports user-inputs).</li> <li>Everytime an incumbent (with the lowest amount of trials) is intensified, a new challenger is added to the   queue.</li> <li>If all incumbents are evaluated on the same trials, a new trial is added to one of the incumbents.</li> <li>Only challengers which are not rejected/running/incumbent are intensified by N*2.</li> </ul>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.__iter__--returns","title":"Returns","text":"<p>trials : Iterator[TrialInfo]     Iterator over the trials.</p> Source code in <code>smac/intensifier/intensifier.py</code> <pre><code>def __iter__(self) -&gt; Iterator[TrialInfo]:\n    \"\"\"This iter method holds the logic for the intensification loop.\n    Some facts about the loop:\n\n    - Adds existing configurations from the runhistory to the queue (that means it supports user-inputs).\n    - Everytime an incumbent (with the lowest amount of trials) is intensified, a new challenger is added to the\n      queue.\n    - If all incumbents are evaluated on the same trials, a new trial is added to one of the incumbents.\n    - Only challengers which are not rejected/running/incumbent are intensified by N*2.\n\n    Returns\n    -------\n    trials : Iterator[TrialInfo]\n        Iterator over the trials.\n    \"\"\"\n    self.__post_init__()\n\n    rh = self.runhistory\n    assert self._max_config_calls is not None\n\n    # What if there are already trials in the runhistory? Should we queue them up?\n    # Because they are part of the runhistory, they might be selected as incumbents. However, they are not\n    # intensified because they are not part of the queue. We could add them here to incorporate them in the\n    # intensification process.\n    # Idea: Add all configs to queue (if it is an incumbent it is removed automatically later on)\n    # N=1 is enough here as it will increase automatically in the iterations if the configuration is worthy\n    # Note: The incumbents are updated once the runhistory is set (see abstract intensifier)\n    # Note 2: If the queue was restored, we don't want to go in here (queue is restored)\n    if len(self._queue) == 0:\n        for config in rh.get_configs():\n            hash = get_config_hash(config)\n            self._queue.append((config, 1))\n            logger.info(f\"Added config {hash} from runhistory to the intensifier queue.\")\n\n    fails = -1\n    while True:\n        fails += 1\n\n        # Some criteria to stop the intensification if nothing can be intensified anymore\n        if fails &gt; self._retries:\n            logger.error(\"Intensifier could not find any new trials.\")\n            return\n\n        # Some configs from the runhistory\n        running_configs = rh.get_running_configs()\n        rejected_configs = self.get_rejected_configs()\n\n        # Now we get the incumbents sorted by number of trials\n        # Also, incorporate ``get_incumbent_instance_seed_budget_keys`` here because challengers are only allowed to\n        # sample from the incumbent's instances\n        incumbents = self.get_incumbents(sort_by=\"num_trials\")\n        incumbent_isb_keys = self.get_incumbent_instance_seed_budget_keys()\n\n        # Check if configs in queue are still running\n        all_configs_running = True\n        for config, _ in self._queue:\n            if config not in running_configs:\n                all_configs_running = False\n                break\n\n        if len(self._queue) == 0 or all_configs_running:\n            if len(self._queue) == 0:\n                logger.debug(\"Queue is empty:\")\n            else:\n                logger.debug(\"All configs in the queue are running:\")\n\n            if len(incumbents) == 0:\n                logger.debug(\"--- No incumbent to intensify.\")\n\n            for incumbent in incumbents:\n                # Instances of this particular incumbent\n                individual_incumbent_isb_keys = rh.get_instance_seed_budget_keys(incumbent)\n                incumbent_hash = get_config_hash(incumbent)\n\n                # We don't want to intensify an incumbent which is either still running or rejected\n                if incumbent in running_configs:\n                    logger.debug(\n                        f\"--- Skipping intensifying incumbent {incumbent_hash} because it has trials pending.\"\n                    )\n                    continue\n\n                if incumbent in rejected_configs:\n                    # This should actually not happen because if a config is rejected the incumbent should\n                    # have changed\n                    # However, we just keep it here as sanity check\n                    logger.debug(f\"--- Skipping intensifying incumbent {incumbent_hash} because it was rejected.\")\n                    continue\n\n                # If incumbent was evaluated on all incumbent instance intersections but was not evaluated on\n                # the differences, we have to add it here\n                incumbent_isb_key_differences = self.get_incumbent_instance_seed_budget_key_differences()\n\n                # We set shuffle to false because we first want to evaluate the incumbent instances, then the\n                # differences (to make the instance-seed keys for the incumbents equal again)\n                trials = self._get_next_trials(\n                    incumbent,\n                    from_keys=incumbent_isb_keys + incumbent_isb_key_differences,\n                    shuffle=False,\n                )\n\n                # If we don't receive any trials, then we try it randomly with any other because we want to\n                # intensify for sure\n                if len(trials) == 0:\n                    logger.debug(\n                        f\"--- Incumbent {incumbent_hash} was already evaluated on all incumbent instances \"\n                        \"and incumbent instance differences so far. Looking for new instances...\"\n                    )\n                    trials = self._get_next_trials(incumbent)\n                    logger.debug(f\"--- Randomly found {len(trials)} new trials.\")\n\n                if len(trials) &gt; 0:\n                    fails = -1\n                    logger.debug(\n                        f\"--- Yielding trial {len(individual_incumbent_isb_keys)+1} of \"\n                        f\"{self._max_config_calls} from incumbent {incumbent_hash}...\"\n                    )\n                    yield trials[0]\n                    logger.debug(f\"--- Finished yielding for config {incumbent_hash}.\")\n\n                    # We break here because we only want to intensify one more trial of one incumbent\n                    break\n                else:\n                    # assert len(incumbent_isb_keys) == self._max_config_calls\n                    logger.debug(\n                        f\"--- Skipped intensifying incumbent {incumbent_hash} because no new trials have \"\n                        \"been found. Evaluated \"\n                        f\"{len(individual_incumbent_isb_keys)}/{self._max_config_calls} trials.\"\n                    )\n\n            # For each intensification of the incumbent, we also want to intensify the next configuration\n            # We simply add it to the queue and intensify it in the next iteration\n            try:\n                config = next(self.config_generator)\n                config_hash = get_config_hash(config)\n                self._queue.append((config, 1))\n                logger.debug(f\"--- Added a new config {config_hash} to the queue.\")\n\n                # If we added a new config, then we did something in this iteration\n                fails = -1\n            except StopIteration:\n                # We stop if we don't find any configuration anymore\n                logger.warning(\n                    \"If you assume your configspace was not yet exhausted, try to \"\n                    \"increase the number of retries in the config selector.\"\n                )\n                return\n        else:\n            logger.debug(\"Start finding a new challenger in the queue:\")\n            for i, (config, N) in enumerate(self._queue.copy()):\n                config_hash = get_config_hash(config)\n\n                # If the config is still running, we ignore it and head to the next config\n                if config in running_configs:\n                    logger.debug(f\"--- Config {config_hash} is still running. Skipping this config in the queue...\")\n                    continue\n\n                # We want to get rid of configs in the queue which are rejected\n                if config in rejected_configs:\n                    logger.debug(f\"--- Config {config_hash} was removed from the queue because it was rejected.\")\n                    self._queue.remove((config, N))\n                    continue\n\n                # We don't want to intensify an incumbent here\n                if config in incumbents:\n                    logger.debug(f\"--- Config {config_hash} was removed from the queue because it is an incumbent.\")\n                    self._queue.remove((config, N))\n                    continue\n\n                # And then we yield as many trials as we specified N\n                # However, only the same instances as the incumbents are used\n                isk_keys: list[InstanceSeedBudgetKey] | None = None\n                if len(incumbent_isb_keys) &gt; 0:\n                    isk_keys = incumbent_isb_keys\n\n                # TODO: What to do if there are no incumbent instances? (Use-case: call multiple asks)\n\n                trials = self._get_next_trials(config, N=N, from_keys=isk_keys)\n                logger.debug(f\"--- Yielding {len(trials)} trials to evaluate config {config_hash}...\")\n                for trial in trials:\n                    fails = -1\n                    yield trial\n\n                logger.debug(f\"--- Finished yielding for config {config_hash}.\")\n\n                # Now we have to remove the config\n                self._queue.remove((config, N))\n                logger.debug(f\"--- Removed config {config_hash} with N={N} from queue.\")\n\n                # Finally, we add the same config to the queue with a higher N\n                # If the config was rejected by the runhistory, then it's been removed in the next iteration\n                if N &lt; self._max_config_calls:\n                    new_pair = (config, N * 2)\n                    if new_pair not in self._queue:\n                        logger.debug(\n                            f\"--- Doubled trials of config {config_hash} to N={N*2} and added it to the queue \"\n                            \"again.\"\n                        )\n                        self._queue.append((config, N * 2))\n\n                        # Also reset fails here\n                        fails = -1\n                    else:\n                        logger.debug(f\"--- Config {config_hash} with N={N*2} is already in the queue.\")\n\n                # If we are at this point, it really is important to break because otherwise, we would intensify\n                # all configs in the queue in one iteration\n                break\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Fills <code>self._tf_seeds</code> and <code>self._tf_instances</code>. Moreover, the incumbents are updated.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Fills ``self._tf_seeds`` and ``self._tf_instances``. Moreover, the incumbents are updated.\"\"\"\n    rh = self.runhistory\n\n    # Validate runhistory: Are seeds/instances/budgets used?\n    # Add seed/instance/budget to the cache\n    for k in rh.keys():\n        if self.uses_seeds:\n            if k.seed is None:\n                raise ValueError(\"Trial contains no seed information but intensifier expects seeds to be used.\")\n\n            if k.seed not in self._tf_seeds:\n                self._tf_seeds.append(k.seed)\n\n        if self.uses_instances:\n            if self._scenario.instances is None and k.instance is not None:\n                raise ValueError(\n                    \"Scenario does not specify any instances but found instance information in runhistory.\"\n                )\n\n            if self._scenario.instances is not None and k.instance not in self._scenario.instances:\n                raise ValueError(\n                    \"Instance information in runhistory is not part of the defined instances in scenario.\"\n                )\n\n            if k.instance not in self._tf_instances:\n                self._tf_instances.append(k.instance)\n\n        if self.uses_budgets:\n            if k.budget is None:\n                raise ValueError(\"Trial contains no budget information but intensifier expects budgets to be used.\")\n\n            if k.budget not in self._tf_budgets:\n                self._tf_budgets.append(k.budget)\n\n    # Add all other instances to ``_tf_instances``\n    # Behind idea: Prioritize instances that are found in the runhistory\n    if (instances := self._scenario.instances) is not None:\n        for inst in instances:\n            if inst not in self._tf_instances:\n                self._tf_instances.append(inst)\n\n    if len(self._tf_instances) == 0:\n        self._tf_instances = [None]\n\n    if len(self._tf_budgets) == 0:\n        self._tf_budgets = [None]\n\n    # Update our incumbents here\n    for config in rh.get_configs():\n        self.update_incumbents(config)\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_callback","title":"get_callback","text":"<pre><code>get_callback() -&gt; Callback\n</code></pre> <p>The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory (every time new information is available). Moreover, incorporating the callback here allows developers more options in the future.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_callback(self) -&gt; Callback:\n    \"\"\"The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory\n    (every time new information is available). Moreover, incorporating the callback here allows developers\n    more options in the future.\n    \"\"\"\n\n    class RunHistoryCallback(Callback):\n        def __init__(self, intensifier: AbstractIntensifier):\n            self.intensifier = intensifier\n\n        def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; None:\n            self.intensifier.update_incumbents(info.config)\n\n    return RunHistoryCallback(self)\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_incumbent","title":"get_incumbent","text":"<pre><code>get_incumbent() -&gt; Configuration | None\n</code></pre> <p>Returns the current incumbent in a single-objective setting.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent(self) -&gt; Configuration | None:\n    \"\"\"Returns the current incumbent in a single-objective setting.\"\"\"\n    if self._scenario.count_objectives() &gt; 1:\n        raise ValueError(\"Cannot get a single incumbent for multi-objective optimization.\")\n\n    if len(self._incumbents) == 0:\n        return None\n\n    assert len(self._incumbents) == 1\n    return self._incumbents[0]\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_incumbent_instance_seed_budget_key_differences","title":"get_incumbent_instance_seed_budget_key_differences","text":"<pre><code>get_incumbent_instance_seed_budget_key_differences(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>There are situations in which incumbents are evaluated on more trials than others. This method returns the instances that are not part of the lowest intersection of instances for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_key_differences(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"There are situations in which incumbents are evaluated on more trials than others. This method returns the\n    instances that are not part of the lowest intersection of instances for all incumbents.\n    \"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the differences so that we can evaluate the other incumbents on the same instances\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n\n        if len(incumbent_isb_keys) &lt;= 1:\n            return []\n\n        # Compute the actual differences\n        intersection_isb_keys = set.intersection(*map(set, incumbent_isb_keys))  # type: ignore\n        union_isb_keys = set.union(*map(set, incumbent_isb_keys))  # type: ignore\n        incumbent_isb_keys = list(union_isb_keys - intersection_isb_keys)  # type: ignore\n\n        if len(incumbent_isb_keys) == 0:\n            return []\n\n        return incumbent_isb_keys  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_incumbent_instance_seed_budget_keys","title":"get_incumbent_instance_seed_budget_keys","text":"<pre><code>get_incumbent_instance_seed_budget_keys(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Find the lowest intersection of instance-seed-budget keys for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_keys(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Find the lowest intersection of instance-seed-budget keys for all incumbents.\"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the smallest set of trials that is used by all incumbents\n        # Reason: We can not fairly compare otherwise\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n        instances = list(set.intersection(*map(set, incumbent_isb_keys)))  # type: ignore\n\n        return instances  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_incumbents","title":"get_incumbents","text":"<pre><code>get_incumbents(\n    sort_by: str | None = None,\n) -&gt; list[Configuration]\n</code></pre> <p>Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective optimization, only one incumbent (if is) is returned.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_incumbents--returns","title":"Returns","text":"<p>configs : list[Configuration]     The configs of the Pareto front. sort_by : str, defaults to None     Sort the trials by <code>cost</code> (lowest cost first) or <code>num_trials</code> (config with lowest number of trials     first).</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbents(self, sort_by: str | None = None) -&gt; list[Configuration]:\n    \"\"\"Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective\n    optimization, only one incumbent (if is) is returned.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        The configs of the Pareto front.\n    sort_by : str, defaults to None\n        Sort the trials by ``cost`` (lowest cost first) or ``num_trials`` (config with lowest number of trials\n        first).\n    \"\"\"\n    rh = self.runhistory\n\n    if sort_by == \"cost\":\n        return list(sorted(self._incumbents, key=lambda config: rh._cost_per_config[rh.get_config_id(config)]))\n    elif sort_by == \"num_trials\":\n        return list(sorted(self._incumbents, key=lambda config: len(rh.get_trials(config))))\n    elif sort_by is None:\n        return list(self._incumbents)\n    else:\n        raise ValueError(f\"Unknown sort_by value: {sort_by}.\")\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_instance_seed_budget_keys","title":"get_instance_seed_budget_keys","text":"<pre><code>get_instance_seed_budget_keys(\n    config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Returns the instance-seed-budget keys for a given configuration. This method is used for updating the incumbents and might differ for different intensifiers. For example, if incumbents should only be compared on the highest observed budgets.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_instance_seed_budget_keys(\n    self, config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Returns the instance-seed-budget keys for a given configuration. This method is *used for\n    updating the incumbents* and might differ for different intensifiers. For example, if incumbents should only\n    be compared on the highest observed budgets.\n    \"\"\"\n    return self.runhistory.get_instance_seed_budget_keys(config, highest_observed_budget_only=False)\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_instance_seed_keys_of_interest","title":"get_instance_seed_keys_of_interest","text":"<pre><code>get_instance_seed_keys_of_interest(\n    *, validate: bool = False, seed: int | None = None\n) -&gt; list[InstanceSeedKey]\n</code></pre> <p>Returns a list of instance-seed keys. Considers seeds and instances from the runhistory (<code>self._tf_seeds</code> and <code>self._tf_instances</code>). If no seeds or instances were found, new seeds and instances are generated based on the global intensifier seed.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_instance_seed_keys_of_interest--warning","title":"Warning","text":"<p>The passed seed is only used for validation. For training, the global intensifier seed is used.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_instance_seed_keys_of_interest--parameters","title":"Parameters","text":"<p>validate : bool, defaults to False     Whether to get validation trials or training trials. The only difference lies in different seeds. seed : int | None, defaults to None     The seed used for the validation trials.</p>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_instance_seed_keys_of_interest--returns","title":"Returns","text":"<p>instance_seed_keys : list[InstanceSeedKey]     Instance-seed keys of interest.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_instance_seed_keys_of_interest(\n    self,\n    *,\n    validate: bool = False,\n    seed: int | None = None,\n) -&gt; list[InstanceSeedKey]:\n    \"\"\"Returns a list of instance-seed keys. Considers seeds and instances from the\n    runhistory (``self._tf_seeds`` and ``self._tf_instances``). If no seeds or instances were found, new\n    seeds and instances are generated based on the global intensifier seed.\n\n    Warning\n    -------\n    The passed seed is only used for validation. For training, the global intensifier seed is used.\n\n    Parameters\n    ----------\n    validate : bool, defaults to False\n        Whether to get validation trials or training trials. The only difference lies in different seeds.\n    seed : int | None, defaults to None\n        The seed used for the validation trials.\n\n    Returns\n    -------\n    instance_seed_keys : list[InstanceSeedKey]\n        Instance-seed keys of interest.\n    \"\"\"\n    if self._runhistory is None:\n        raise RuntimeError(\"Please set the runhistory before calling this method.\")\n\n    if len(self._tf_instances) == 0:\n        raise RuntimeError(\"Please call __post_init__ before calling this method.\")\n\n    if seed is None:\n        seed = 0\n\n    # We cache the instance-seed keys for efficiency and consistency reasons\n    if (self._instance_seed_keys is None and not validate) or (\n        self._instance_seed_keys_validation is None and validate\n    ):\n        instance_seed_keys: list[InstanceSeedKey] = []\n        if validate:\n            rng = np.random.RandomState(seed)\n        else:\n            rng = self._rng\n\n        i = 0\n        while True:\n            found_enough_configs = (\n                self._max_config_calls is not None and len(instance_seed_keys) &gt;= self._max_config_calls\n            )\n            used_enough_seeds = self._n_seeds is not None and i &gt;= self._n_seeds\n\n            if found_enough_configs or used_enough_seeds:\n                break\n\n            if validate:\n                next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n            else:\n                try:\n                    next_seed = self._tf_seeds[i]\n                    logger.info(f\"Added existing seed {next_seed} from runhistory to the intensifier.\")\n                except IndexError:\n                    # Use global random generator for a new seed and mark it so it will be reused for another config\n                    next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n\n                    # This line here is really important because we don't want to add the same seed twice\n                    if next_seed in self._tf_seeds:\n                        continue\n\n                    self._tf_seeds.append(next_seed)\n                    logger.debug(f\"Added a new random seed {next_seed} to the intensifier.\")\n\n            # If no instances are used, tf_instances includes None\n            for instance in self._tf_instances:\n                instance_seed_keys.append(InstanceSeedKey(instance, next_seed))\n\n            # Only use one seed in deterministic case\n            if self._scenario.deterministic:\n                logger.info(\"Using only one seed for deterministic scenario.\")\n                break\n\n            # Seed counter\n            i += 1\n\n        # Now we cut so that we only have max_config_calls instance_seed_keys\n        # We favor instances over seeds here: That makes sure we always work with the same instance/seed pairs\n        if self._max_config_calls is not None:\n            if len(instance_seed_keys) &gt; self._max_config_calls:\n                instance_seed_keys = instance_seed_keys[: self._max_config_calls]\n                logger.info(f\"Cut instance-seed keys to {self._max_config_calls} entries.\")\n\n        # Set it globally\n        if not validate:\n            self._instance_seed_keys = instance_seed_keys\n        else:\n            self._instance_seed_keys_validation = instance_seed_keys\n\n    if not validate:\n        assert self._instance_seed_keys is not None\n        instance_seed_keys = self._instance_seed_keys\n    else:\n        assert self._instance_seed_keys_validation is not None\n        instance_seed_keys = self._instance_seed_keys_validation\n\n    return instance_seed_keys.copy()\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_rejected_configs","title":"get_rejected_configs","text":"<pre><code>get_rejected_configs() -&gt; list[Configuration]\n</code></pre> <p>Returns rejected configurations when racing against the incumbent failed.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_rejected_configs(self) -&gt; list[Configuration]:\n    \"\"\"Returns rejected configurations when racing against the incumbent failed.\"\"\"\n    configs = []\n    for rejected_config_id in self._rejected_config_ids:\n        configs.append(self.runhistory._ids_config[rejected_config_id])\n\n    return configs\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.get_trials_of_interest","title":"get_trials_of_interest","text":"<pre><code>get_trials_of_interest(\n    config: Configuration,\n    *,\n    validate: bool = False,\n    seed: int | None = None\n) -&gt; list[TrialInfo]\n</code></pre> <p>Returns the trials of interest for a given configuration. Expands the keys from <code>get_instance_seed_keys_of_interest</code> with the config.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_trials_of_interest(\n    self,\n    config: Configuration,\n    *,\n    validate: bool = False,\n    seed: int | None = None,\n) -&gt; list[TrialInfo]:\n    \"\"\"Returns the trials of interest for a given configuration.\n    Expands the keys from ``get_instance_seed_keys_of_interest`` with the config.\n    \"\"\"\n    is_keys = self.get_instance_seed_keys_of_interest(validate=validate, seed=seed)\n\n    trials = []\n    for key in is_keys:\n        trials.append(TrialInfo(config=config, instance=key.instance, seed=key.seed))\n\n    return trials\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.load","title":"load","text":"<pre><code>load(filename: str | Path) -&gt; None\n</code></pre> <p>Loads the latest state of the intensifier including the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def load(self, filename: str | Path) -&gt; None:\n    \"\"\"Loads the latest state of the intensifier including the incumbents and trajectory.\"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    try:\n        with open(filename) as fp:\n            data = json.load(fp)\n    except Exception as e:\n        logger.warning(\n            f\"Encountered exception {e} while reading runhistory from {filename}. Not adding any trials!\"\n        )\n        return\n\n    # We reset the intensifier and then reset the runhistory\n    self.reset()\n    if self._runhistory is not None:\n        self.runhistory = self._runhistory\n\n    self._incumbents = [self.runhistory.get_config(config_id) for config_id in data[\"incumbent_ids\"]]\n    self._incumbents_changed = data[\"incumbents_changed\"]\n    self._rejected_config_ids = data[\"rejected_config_ids\"]\n    self._trajectory = [TrajectoryItem(**item) for item in data[\"trajectory\"]]\n    self.set_state(data[\"state\"])\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Resets the internal variables of the intensifier including the queue.</p> Source code in <code>smac/intensifier/intensifier.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Resets the internal variables of the intensifier including the queue.\"\"\"\n    super().reset()\n\n    # Queue to keep track of the challengers\n    # (config, N=how many trials should be sampled)\n    self._queue: list[tuple[Configuration, int]] = []\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.save","title":"save","text":"<pre><code>save(filename: str | Path) -&gt; None\n</code></pre> <p>Saves the current state of the intensifier. In addition to the state (retrieved by <code>get_state</code>), this method also saves the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def save(self, filename: str | Path) -&gt; None:\n    \"\"\"Saves the current state of the intensifier. In addition to the state (retrieved by ``get_state``), this\n    method also saves the incumbents and trajectory.\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    assert str(filename).endswith(\".json\")\n    filename.parent.mkdir(parents=True, exist_ok=True)\n\n    data = {\n        \"incumbent_ids\": [self.runhistory.get_config_id(config) for config in self._incumbents],\n        \"rejected_config_ids\": self._rejected_config_ids,\n        \"incumbents_changed\": self._incumbents_changed,\n        \"trajectory\": [dataclasses.asdict(item) for item in self._trajectory],\n        \"state\": self.get_state(),\n    }\n\n    with open(filename, \"w\") as fp:\n        json.dump(data, fp, indent=2, cls=NumpyEncoder)\n</code></pre>"},{"location":"api/smac/intensifier/intensifier/#smac.intensifier.intensifier.Intensifier.update_incumbents","title":"update_incumbents","text":"<pre><code>update_incumbents(config: Configuration) -&gt; None\n</code></pre> <p>Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only the affected config and the current incumbents are used, this method is very efficient. Furthermore, a configuration is only considered incumbent if it has a better performance on all incumbent instances.</p> <p>Crucially, if there is no incumbent (at the start) then, the first configuration assumes incumbent status. For the next configuration, we need to check if the configuration is better on all instances that have been evaluated for the incumbent. If this is the case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to obtain the missing instance-seed-budget combination or b) mark this configuration as inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().</p> <p>Notably, this method is written to support both multi-fidelity and multi-objective optimization. While the get_instance_seed_budget_keys() method and self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour, calculate_pareto_front() is used as a hard coded way to support multi-objective optimization, including the single objective as special case. calculate_pareto_front() is called on the set of all (in case of MO) incumbents amended with the challenger configuration, provided it has a sufficient overlap in seed-instance-budget combinations.</p> <p>Lastly, if we have a self._max_incumbents and the pareto front provides more than this specified amount, we cut the incumbents using crowding distance.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def update_incumbents(self, config: Configuration) -&gt; None:\n    \"\"\"Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only\n    the affected config and the current incumbents are used, this method is very efficient. Furthermore, a\n    configuration is only considered incumbent if it has a better performance on all incumbent instances.\n\n    Crucially, if there is no incumbent (at the start) then, the first configuration assumes\n    incumbent status. For the next configuration, we need to check if the configuration\n    is better on all instances that have been evaluated for the incumbent. If this is the\n    case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to\n    obtain the missing instance-seed-budget combination or b) mark this configuration as\n    inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by\n    self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().\n\n    Notably, this method is written to support both multi-fidelity and multi-objective\n    optimization. While the get_instance_seed_budget_keys() method and\n    self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour,\n    calculate_pareto_front() is used as a hard coded way to support multi-objective\n    optimization, including the single objective as special case. calculate_pareto_front()\n    is called on the set of all (in case of MO) incumbents amended with the challenger\n    configuration, provided it has a sufficient overlap in seed-instance-budget combinations.\n\n    Lastly, if we have a self._max_incumbents and the pareto front provides more than this\n    specified amount, we cut the incumbents using crowding distance.\n    \"\"\"\n    rh = self.runhistory\n\n    # What happens if a config was rejected, but it appears again? Give it another try even if it\n    # has already been evaluated? Yes!\n\n    # Associated trials and id\n    config_isb_keys = self.get_instance_seed_budget_keys(config)\n    config_id = rh.get_config_id(config)\n    config_hash = get_config_hash(config)\n\n    # We skip updating incumbents if no instances are available\n    # Note: This is especially the case if trials of a config are still running\n    # because if trials are running, the runhistory does not update the trials in the fast data structure\n    if len(config_isb_keys) == 0:\n        logger.debug(f\"No relevant instances evaluated for config {config_hash}. Updating incumbents is skipped.\")\n        return\n\n    # Now we get the incumbents and see which trials have been used\n    incumbents = self.get_incumbents()\n    incumbent_ids = [rh.get_config_id(c) for c in incumbents]\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    incumbent_isb_keys = self.get_incumbent_instance_seed_budget_keys()\n\n    # Save for later\n    previous_incumbents = incumbents.copy()\n    previous_incumbent_ids = incumbent_ids.copy()\n\n    # Little sanity check here for consistency\n    if len(incumbents) &gt; 0:\n        assert incumbent_isb_keys is not None\n        assert len(incumbent_isb_keys) &gt; 0\n\n    # If there are no incumbents at all, we just use the new config as new incumbent\n    # Problem: We can add running incumbents\n    if len(incumbents) == 0:  # incumbent_isb_keys is None and len(incumbents) == 0:\n        logger.info(f\"Added config {config_hash} as new incumbent because there are no incumbents yet.\")\n        self._update_trajectory([config])\n\n        # Nothing else to do\n        return\n\n    # Comparison keys\n    # This one is a bit tricky: We would have problems if we compare with budgets because we might have different\n    # scenarios (depending on the incumbent selection specified in Successive Halving).\n    # 1) Any budget/highest observed budget: We want to get rid of the budgets because if we know it is calculated\n    # on the same instance-seed already then we are ready to go. Imagine we would check for the same budgets,\n    # then the configs can not be compared although the user does not care on which budgets configurations have\n    # been evaluated.\n    # 2) Highest budget: We only want to compare the configs if they are evaluated on the highest budget.\n    # Here we do actually care about the budgets. Please see the ``get_instance_seed_budget_keys`` method from\n    # Successive Halving to get more information.\n    # Noitce: compare=True only takes effect when subclass implemented it. -- e.g. in SH it\n    # will remove the budgets from the keys.\n    config_isb_comparison_keys = self.get_instance_seed_budget_keys(config, compare=True)\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    config_incumbent_isb_comparison_keys = self.get_incumbent_instance_seed_budget_keys(compare=True)\n\n    # Now we have to check if the new config has been evaluated on the same keys as the incumbents\n    if not all([key in config_isb_comparison_keys for key in config_incumbent_isb_comparison_keys]):\n        # We can not tell if the new config is better/worse than the incumbents because it has not been\n        # evaluated on the necessary trials\n        logger.debug(\n            f\"Could not compare config {config_hash} with incumbents because it's evaluated on \"\n            f\"different trials.\"\n        )\n\n        # The config has to go to a queue now as it is a challenger and a potential incumbent\n        return\n    else:\n        # If all instances are available and the config is incumbent and even evaluated on more trials\n        # then there's nothing we can do\n        if config in incumbents and len(config_isb_keys) &gt; len(incumbent_isb_keys):\n            logger.debug(\n                \"Config is already an incumbent but can not be compared to other incumbents because \"\n                \"the others are missing trials.\"\n            )\n            return\n\n    # Add config to incumbents so that we compare only the new config and existing incumbents\n    if config not in incumbents:\n        incumbents.append(config)\n        incumbent_ids.append(config_id)\n\n    # Now we get all instance-seed-budget keys for each incumbent (they might be different when using budgets)\n    all_incumbent_isb_keys = []\n    for incumbent in incumbents:\n        all_incumbent_isb_keys.append(self.get_instance_seed_budget_keys(incumbent))\n\n    # We compare the incumbents now and only return the ones on the pareto front\n    new_incumbents = calculate_pareto_front(rh, incumbents, all_incumbent_isb_keys)\n    new_incumbent_ids = [rh.get_config_id(c) for c in new_incumbents]\n\n    if len(previous_incumbents) == len(new_incumbents):\n        if previous_incumbents == new_incumbents:\n            # No changes in the incumbents, we need this clause because we can't use set difference then\n            if config_id in new_incumbent_ids:\n                self._remove_rejected_config(config_id)\n            else:\n                # config worse than incumbents and thus rejected\n                self._add_rejected_config(config_id)\n            return\n        else:\n            # In this case, we have to determine which config replaced which incumbent and reject it\n            removed_incumbent_id = list(set(previous_incumbent_ids) - set(new_incumbent_ids))[0]\n            removed_incumbent_hash = get_config_hash(rh.get_config(removed_incumbent_id))\n            self._add_rejected_config(removed_incumbent_id)\n\n            if removed_incumbent_id == config_id:\n                logger.debug(\n                    f\"Rejected config {config_hash} because it is not better than the incumbents on \"\n                    f\"{len(config_isb_keys)} instances.\"\n                )\n            else:\n                self._remove_rejected_config(config_id)\n                logger.info(\n                    f\"Added config {config_hash} and rejected config {removed_incumbent_hash} as incumbent because \"\n                    f\"it is not better than the incumbents on {len(config_isb_keys)} instances: \"\n                )\n                print_config_changes(rh.get_config(removed_incumbent_id), config, logger=logger)\n    elif len(previous_incumbents) &lt; len(new_incumbents):\n        # Config becomes a new incumbent; nothing is rejected in this case\n        self._remove_rejected_config(config_id)\n        logger.info(\n            f\"Config {config_hash} is a new incumbent. \" f\"Total number of incumbents: {len(new_incumbents)}.\"\n        )\n    else:\n        # There might be situations that the incumbents might be removed because of updated cost information of\n        # config\n        for incumbent in previous_incumbents:\n            if incumbent not in new_incumbents:\n                self._add_rejected_config(incumbent)\n                logger.debug(\n                    f\"Removed incumbent {get_config_hash(incumbent)} because of the updated costs from config \"\n                    f\"{config_hash}.\"\n                )\n\n    # Cut incumbents: We only want to keep a specific number of incumbents\n    # We use the crowding distance for that\n    if len(new_incumbents) &gt; self._max_incumbents:\n        new_incumbents = sort_by_crowding_distance(rh, new_incumbents, all_incumbent_isb_keys)\n        new_incumbents = new_incumbents[: self._max_incumbents]\n\n        # or random?\n        # idx = self._rng.randint(0, len(new_incumbents))\n        # del new_incumbents[idx]\n        # del new_incumbent_ids[idx]\n\n        logger.info(\n            f\"Removed one incumbent using crowding distance because more than {self._max_incumbents} are \"\n            \"available.\"\n        )\n\n    self._update_trajectory(new_incumbents)\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/","title":"Successive halving","text":""},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving","title":"smac.intensifier.successive_halving","text":""},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving","title":"SuccessiveHalving","text":"<pre><code>SuccessiveHalving(\n    scenario: Scenario,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\",\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractIntensifier</code></p> <p>Implementation of Succesive Halving supporting multi-fidelity, multi-objective, and multi-processing. Internally, a tracker keeps track of configurations and their bracket and stage.</p> <p>The behaviour of this intensifier is as follows:</p> <ul> <li>First, adds configurations from the runhistory to the tracker. The first stage is always filled-up. For example,   the user provided 4 configs with the tell-method but the first stage requires 8 configs: 4 new configs are   sampled and added together with the provided configs as a group to the tracker.</li> <li> <p>While loop:</p> </li> <li> <p>If a trial in the tracker has not been yielded yet, yield it.</p> </li> <li>If we are running out of trials, we simply add a new batch of configurations to the first stage.</li> </ul>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving--note","title":"Note","text":"<p>The implementation natively supports brackets from Hyperband. However, in the case of Successive Halving, only one bracket is used.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving--parameters","title":"Parameters","text":"<p>eta : int, defaults to 3     Input that controls the proportion of configurations discarded in each round of Successive Halving. n_seeds : int, defaults to 1     How many seeds to use for each instance. instance_seed_order : str, defaults to \"shuffle_once\"     How to order the instance-seed pairs. Can be set to:</p> <pre><code>- `None`: No shuffling at all and use the instance-seed order provided by the user.\n- `shuffle_once`: Shuffle the instance-seed keys once and use the same order across all runs.\n- `shuffle`: Shuffles the instance-seed keys for each bracket individually.\n</code></pre> <p>incumbent_selection : str, defaults to \"highest_observed_budget\"     How to select the incumbent when using budgets. Can be set to:</p> <pre><code>- `any_budget`: Incumbent is the best on any budget i.e., best performance regardless of budget.\n- `highest_observed_budget`: Incumbent is the best in the highest budget run so far.\n- `highest_budget`: Incumbent is selected only based on the highest budget.\n</code></pre> <p>max_incumbents : int, defaults to 10     How many incumbents to keep track of in the case of multi-objective. seed : int, defaults to None     Internal seed used for random events like shuffle seeds.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    eta: int = 3,\n    n_seeds: int = 1,\n    instance_seed_order: str | None = \"shuffle_once\",\n    max_incumbents: int = 10,\n    incumbent_selection: str = \"highest_observed_budget\",\n    seed: int | None = None,\n):\n    super().__init__(\n        scenario=scenario,\n        n_seeds=n_seeds,\n        max_incumbents=max_incumbents,\n        seed=seed,\n    )\n\n    self._eta = eta\n    self._instance_seed_order = instance_seed_order\n    self._incumbent_selection = incumbent_selection\n    self._highest_observed_budget_only = False if incumbent_selection == \"any_budget\" else True\n\n    # Global variables derived from scenario\n    self._min_budget = self._scenario.min_budget\n    self._max_budget = self._scenario.max_budget\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.config_generator","title":"config_generator  <code>property</code>","text":"<pre><code>config_generator: Iterator[Configuration]\n</code></pre> <p>Based on the configuration selector, an iterator is returned that generates configurations.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.config_selector","title":"config_selector  <code>property</code> <code>writable</code>","text":"<pre><code>config_selector: ConfigSelector\n</code></pre> <p>The configuration selector for the intensifier.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.incumbents_changed","title":"incumbents_changed  <code>property</code>","text":"<pre><code>incumbents_changed: int\n</code></pre> <p>How often the incumbents have changed.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>Runhistory of the intensifier.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.trajectory","title":"trajectory  <code>property</code>","text":"<pre><code>trajectory: list[TrajectoryItem]\n</code></pre> <p>Returns the trajectory (changes of incumbents) of the optimization run.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.used_walltime","title":"used_walltime  <code>property</code> <code>writable</code>","text":"<pre><code>used_walltime: float\n</code></pre> <p>Returns used wallclock time.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Post initialization steps after the runhistory has been set.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialization steps after the runhistory has been set.\"\"\"\n    super().__post_init__()\n\n    # We generate our instance seed pairs once\n    is_keys = self.get_instance_seed_keys_of_interest()\n\n    # Budgets, followed by lots of sanity-checking\n    eta = self._eta\n    min_budget = self._min_budget\n    max_budget = self._max_budget\n\n    if max_budget is not None and min_budget is not None and max_budget &lt; min_budget:\n        raise ValueError(\"Max budget has to be larger than min budget.\")\n\n    if self.uses_instances:\n        if isinstance(min_budget, float) or isinstance(max_budget, float):\n            raise ValueError(\"Successive Halving requires integer budgets when using instances.\")\n\n        min_budget = min_budget if min_budget is not None else 1\n        max_budget = max_budget if max_budget is not None else len(is_keys)\n\n        if max_budget &gt; len(is_keys):\n            raise ValueError(\n                f\"Max budget of {max_budget} can not be greater than the number of instance-seed \"\n                f\"keys ({len(is_keys)}).\"\n            )\n\n        if max_budget &lt; len(is_keys):\n            logger.warning(\n                f\"Max budget {max_budget} does not include all instance seed  \" f\"pairs ({len(is_keys)}).\"\n            )\n    else:\n        if min_budget is None or max_budget is None:\n            raise ValueError(\n                \"Successive Halving requires the parameters min_budget and max_budget defined in the scenario.\"\n            )\n\n        if len(is_keys) != 1:\n            raise ValueError(\"Successive Halving supports only one seed when using budgets.\")\n\n    if min_budget is None or min_budget &lt;= 0:\n        raise ValueError(\"Min budget has to be larger than 0.\")\n\n    budget_type = \"INSTANCES\" if self.uses_instances else \"BUDGETS\"\n    logger.info(\n        f\"Successive Halving uses budget type {budget_type} with eta {eta}, \"\n        f\"min budget {min_budget}, and max budget {max_budget}.\"\n    )\n\n    # Pre-computing Successive Halving variables\n    max_iter = self._get_max_iterations(eta, max_budget, min_budget)\n    budgets, n_configs = self._compute_configs_and_budgets_for_stages(eta, max_budget, max_iter)\n\n    # Global variables\n    self._min_budget = min_budget\n    self._max_budget = max_budget\n\n    # Stage variables, depending on the bracket (0 is the bracket here since SH only has one bracket)\n    self._max_iterations: dict[int, int] = {0: max_iter + 1}\n    self._n_configs_in_stage: dict[int, list] = {0: n_configs}\n    self._budgets_in_stage: dict[int, list] = {0: budgets}\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_callback","title":"get_callback","text":"<pre><code>get_callback() -&gt; Callback\n</code></pre> <p>The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory (every time new information is available). Moreover, incorporating the callback here allows developers more options in the future.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_callback(self) -&gt; Callback:\n    \"\"\"The intensifier makes use of a callback to efficiently update the incumbent based on the runhistory\n    (every time new information is available). Moreover, incorporating the callback here allows developers\n    more options in the future.\n    \"\"\"\n\n    class RunHistoryCallback(Callback):\n        def __init__(self, intensifier: AbstractIntensifier):\n            self.intensifier = intensifier\n\n        def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; None:\n            self.intensifier.update_incumbents(info.config)\n\n    return RunHistoryCallback(self)\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_incumbent","title":"get_incumbent","text":"<pre><code>get_incumbent() -&gt; Configuration | None\n</code></pre> <p>Returns the current incumbent in a single-objective setting.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent(self) -&gt; Configuration | None:\n    \"\"\"Returns the current incumbent in a single-objective setting.\"\"\"\n    if self._scenario.count_objectives() &gt; 1:\n        raise ValueError(\"Cannot get a single incumbent for multi-objective optimization.\")\n\n    if len(self._incumbents) == 0:\n        return None\n\n    assert len(self._incumbents) == 1\n    return self._incumbents[0]\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_incumbent_instance_seed_budget_key_differences","title":"get_incumbent_instance_seed_budget_key_differences","text":"<pre><code>get_incumbent_instance_seed_budget_key_differences(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>There are situations in which incumbents are evaluated on more trials than others. This method returns the instances that are not part of the lowest intersection of instances for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_key_differences(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"There are situations in which incumbents are evaluated on more trials than others. This method returns the\n    instances that are not part of the lowest intersection of instances for all incumbents.\n    \"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the differences so that we can evaluate the other incumbents on the same instances\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n\n        if len(incumbent_isb_keys) &lt;= 1:\n            return []\n\n        # Compute the actual differences\n        intersection_isb_keys = set.intersection(*map(set, incumbent_isb_keys))  # type: ignore\n        union_isb_keys = set.union(*map(set, incumbent_isb_keys))  # type: ignore\n        incumbent_isb_keys = list(union_isb_keys - intersection_isb_keys)  # type: ignore\n\n        if len(incumbent_isb_keys) == 0:\n            return []\n\n        return incumbent_isb_keys  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_incumbent_instance_seed_budget_keys","title":"get_incumbent_instance_seed_budget_keys","text":"<pre><code>get_incumbent_instance_seed_budget_keys(\n    compare: bool = False,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Find the lowest intersection of instance-seed-budget keys for all incumbents.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbent_instance_seed_budget_keys(self, compare: bool = False) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Find the lowest intersection of instance-seed-budget keys for all incumbents.\"\"\"\n    incumbents = self.get_incumbents()\n\n    if len(incumbents) &gt; 0:\n        # We want to calculate the smallest set of trials that is used by all incumbents\n        # Reason: We can not fairly compare otherwise\n        incumbent_isb_keys = [self.get_instance_seed_budget_keys(incumbent, compare) for incumbent in incumbents]\n        instances = list(set.intersection(*map(set, incumbent_isb_keys)))  # type: ignore\n\n        return instances  # type: ignore\n\n    return []\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_incumbents","title":"get_incumbents","text":"<pre><code>get_incumbents(\n    sort_by: str | None = None,\n) -&gt; list[Configuration]\n</code></pre> <p>Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective optimization, only one incumbent (if is) is returned.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_incumbents--returns","title":"Returns","text":"<p>configs : list[Configuration]     The configs of the Pareto front. sort_by : str, defaults to None     Sort the trials by <code>cost</code> (lowest cost first) or <code>num_trials</code> (config with lowest number of trials     first).</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_incumbents(self, sort_by: str | None = None) -&gt; list[Configuration]:\n    \"\"\"Returns the incumbents (points on the pareto front) of the runhistory as copy. In case of a single-objective\n    optimization, only one incumbent (if is) is returned.\n\n    Returns\n    -------\n    configs : list[Configuration]\n        The configs of the Pareto front.\n    sort_by : str, defaults to None\n        Sort the trials by ``cost`` (lowest cost first) or ``num_trials`` (config with lowest number of trials\n        first).\n    \"\"\"\n    rh = self.runhistory\n\n    if sort_by == \"cost\":\n        return list(sorted(self._incumbents, key=lambda config: rh._cost_per_config[rh.get_config_id(config)]))\n    elif sort_by == \"num_trials\":\n        return list(sorted(self._incumbents, key=lambda config: len(rh.get_trials(config))))\n    elif sort_by is None:\n        return list(self._incumbents)\n    else:\n        raise ValueError(f\"Unknown sort_by value: {sort_by}.\")\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_instance_seed_budget_keys","title":"get_instance_seed_budget_keys","text":"<pre><code>get_instance_seed_budget_keys(\n    config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Returns the instance-seed-budget keys for a given configuration. This method supports <code>highest_budget</code>, which only returns the instance-seed-budget keys for the highest budget (if specified). In this case, the incumbents in <code>update_incumbents</code> are only changed if the costs on the highest budget are lower.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_instance_seed_budget_keys--parameters","title":"Parameters","text":"<p>config: Configuration     The Configuration to be queried compare : bool, defaults to False     Get rid of the budget information for comparing if the configuration was evaluated on the same     instance-seed keys.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def get_instance_seed_budget_keys(\n    self, config: Configuration, compare: bool = False\n) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"Returns the instance-seed-budget keys for a given configuration. This method supports ``highest_budget``,\n    which only returns the instance-seed-budget keys for the highest budget (if specified). In this case, the\n    incumbents in ``update_incumbents`` are only changed if the costs on the highest budget are lower.\n\n    Parameters\n    ----------\n    config: Configuration\n        The Configuration to be queried\n    compare : bool, defaults to False\n        Get rid of the budget information for comparing if the configuration was evaluated on the same\n        instance-seed keys.\n    \"\"\"\n    isb_keys = self.runhistory.get_instance_seed_budget_keys(\n        config, highest_observed_budget_only=self._highest_observed_budget_only\n    )\n\n    # If incumbent should only be changed on the highest budget, we have to kick out all budgets below the highest\n    if self.uses_budgets and self._incumbent_selection == \"highest_budget\":\n        isb_keys = [key for key in isb_keys if key.budget == self._max_budget]\n\n    if compare:\n        # Get rid of duplicates\n        isb_keys = list(\n            set([InstanceSeedBudgetKey(instance=key.instance, seed=key.seed, budget=None) for key in isb_keys])\n        )\n\n    return isb_keys\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_instance_seed_keys_of_interest","title":"get_instance_seed_keys_of_interest","text":"<pre><code>get_instance_seed_keys_of_interest(\n    *, validate: bool = False, seed: int | None = None\n) -&gt; list[InstanceSeedKey]\n</code></pre> <p>Returns a list of instance-seed keys. Considers seeds and instances from the runhistory (<code>self._tf_seeds</code> and <code>self._tf_instances</code>). If no seeds or instances were found, new seeds and instances are generated based on the global intensifier seed.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_instance_seed_keys_of_interest--warning","title":"Warning","text":"<p>The passed seed is only used for validation. For training, the global intensifier seed is used.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_instance_seed_keys_of_interest--parameters","title":"Parameters","text":"<p>validate : bool, defaults to False     Whether to get validation trials or training trials. The only difference lies in different seeds. seed : int | None, defaults to None     The seed used for the validation trials.</p>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_instance_seed_keys_of_interest--returns","title":"Returns","text":"<p>instance_seed_keys : list[InstanceSeedKey]     Instance-seed keys of interest.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_instance_seed_keys_of_interest(\n    self,\n    *,\n    validate: bool = False,\n    seed: int | None = None,\n) -&gt; list[InstanceSeedKey]:\n    \"\"\"Returns a list of instance-seed keys. Considers seeds and instances from the\n    runhistory (``self._tf_seeds`` and ``self._tf_instances``). If no seeds or instances were found, new\n    seeds and instances are generated based on the global intensifier seed.\n\n    Warning\n    -------\n    The passed seed is only used for validation. For training, the global intensifier seed is used.\n\n    Parameters\n    ----------\n    validate : bool, defaults to False\n        Whether to get validation trials or training trials. The only difference lies in different seeds.\n    seed : int | None, defaults to None\n        The seed used for the validation trials.\n\n    Returns\n    -------\n    instance_seed_keys : list[InstanceSeedKey]\n        Instance-seed keys of interest.\n    \"\"\"\n    if self._runhistory is None:\n        raise RuntimeError(\"Please set the runhistory before calling this method.\")\n\n    if len(self._tf_instances) == 0:\n        raise RuntimeError(\"Please call __post_init__ before calling this method.\")\n\n    if seed is None:\n        seed = 0\n\n    # We cache the instance-seed keys for efficiency and consistency reasons\n    if (self._instance_seed_keys is None and not validate) or (\n        self._instance_seed_keys_validation is None and validate\n    ):\n        instance_seed_keys: list[InstanceSeedKey] = []\n        if validate:\n            rng = np.random.RandomState(seed)\n        else:\n            rng = self._rng\n\n        i = 0\n        while True:\n            found_enough_configs = (\n                self._max_config_calls is not None and len(instance_seed_keys) &gt;= self._max_config_calls\n            )\n            used_enough_seeds = self._n_seeds is not None and i &gt;= self._n_seeds\n\n            if found_enough_configs or used_enough_seeds:\n                break\n\n            if validate:\n                next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n            else:\n                try:\n                    next_seed = self._tf_seeds[i]\n                    logger.info(f\"Added existing seed {next_seed} from runhistory to the intensifier.\")\n                except IndexError:\n                    # Use global random generator for a new seed and mark it so it will be reused for another config\n                    next_seed = int(rng.randint(low=0, high=MAXINT, size=1)[0])\n\n                    # This line here is really important because we don't want to add the same seed twice\n                    if next_seed in self._tf_seeds:\n                        continue\n\n                    self._tf_seeds.append(next_seed)\n                    logger.debug(f\"Added a new random seed {next_seed} to the intensifier.\")\n\n            # If no instances are used, tf_instances includes None\n            for instance in self._tf_instances:\n                instance_seed_keys.append(InstanceSeedKey(instance, next_seed))\n\n            # Only use one seed in deterministic case\n            if self._scenario.deterministic:\n                logger.info(\"Using only one seed for deterministic scenario.\")\n                break\n\n            # Seed counter\n            i += 1\n\n        # Now we cut so that we only have max_config_calls instance_seed_keys\n        # We favor instances over seeds here: That makes sure we always work with the same instance/seed pairs\n        if self._max_config_calls is not None:\n            if len(instance_seed_keys) &gt; self._max_config_calls:\n                instance_seed_keys = instance_seed_keys[: self._max_config_calls]\n                logger.info(f\"Cut instance-seed keys to {self._max_config_calls} entries.\")\n\n        # Set it globally\n        if not validate:\n            self._instance_seed_keys = instance_seed_keys\n        else:\n            self._instance_seed_keys_validation = instance_seed_keys\n\n    if not validate:\n        assert self._instance_seed_keys is not None\n        instance_seed_keys = self._instance_seed_keys\n    else:\n        assert self._instance_seed_keys_validation is not None\n        instance_seed_keys = self._instance_seed_keys_validation\n\n    return instance_seed_keys.copy()\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.get_rejected_configs","title":"get_rejected_configs","text":"<pre><code>get_rejected_configs() -&gt; list[Configuration]\n</code></pre> <p>Returns rejected configurations when racing against the incumbent failed.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def get_rejected_configs(self) -&gt; list[Configuration]:\n    \"\"\"Returns rejected configurations when racing against the incumbent failed.\"\"\"\n    configs = []\n    for rejected_config_id in self._rejected_config_ids:\n        configs.append(self.runhistory._ids_config[rejected_config_id])\n\n    return configs\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.load","title":"load","text":"<pre><code>load(filename: str | Path) -&gt; None\n</code></pre> <p>Loads the latest state of the intensifier including the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def load(self, filename: str | Path) -&gt; None:\n    \"\"\"Loads the latest state of the intensifier including the incumbents and trajectory.\"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    try:\n        with open(filename) as fp:\n            data = json.load(fp)\n    except Exception as e:\n        logger.warning(\n            f\"Encountered exception {e} while reading runhistory from {filename}. Not adding any trials!\"\n        )\n        return\n\n    # We reset the intensifier and then reset the runhistory\n    self.reset()\n    if self._runhistory is not None:\n        self.runhistory = self._runhistory\n\n    self._incumbents = [self.runhistory.get_config(config_id) for config_id in data[\"incumbent_ids\"]]\n    self._incumbents_changed = data[\"incumbents_changed\"]\n    self._rejected_config_ids = data[\"rejected_config_ids\"]\n    self._trajectory = [TrajectoryItem(**item) for item in data[\"trajectory\"]]\n    self.set_state(data[\"state\"])\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.print_tracker","title":"print_tracker","text":"<pre><code>print_tracker() -&gt; None\n</code></pre> <p>Prints the number of configurations in each bracket/stage.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def print_tracker(self) -&gt; None:\n    \"\"\"Prints the number of configurations in each bracket/stage.\"\"\"\n    messages = []\n    for (bracket, stage), others in self._tracker.items():\n        counter = 0\n        for _, config_ids in others:\n            counter += len(config_ids)\n\n        if counter &gt; 0:\n            messages.append(f\"--- Bracket {bracket} / Stage {stage}: {counter} configs\")\n\n    if len(messages) &gt; 0:\n        logger.debug(f\"{self.__class__.__name__} statistics:\")\n\n    for message in messages:\n        logger.debug(message)\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the internal variables of the intensifier including the tracker.</p> Source code in <code>smac/intensifier/successive_halving.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the internal variables of the intensifier including the tracker.\"\"\"\n    super().reset()\n\n    # States\n    # dict[tuple[bracket, stage], list[tuple[seed to shuffle instance-seed keys, list[config_id]]]\n    self._tracker: dict[tuple[int, int], list[tuple[int | None, list[Configuration]]]] = defaultdict(list)\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.save","title":"save","text":"<pre><code>save(filename: str | Path) -&gt; None\n</code></pre> <p>Saves the current state of the intensifier. In addition to the state (retrieved by <code>get_state</code>), this method also saves the incumbents and trajectory.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def save(self, filename: str | Path) -&gt; None:\n    \"\"\"Saves the current state of the intensifier. In addition to the state (retrieved by ``get_state``), this\n    method also saves the incumbents and trajectory.\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    assert str(filename).endswith(\".json\")\n    filename.parent.mkdir(parents=True, exist_ok=True)\n\n    data = {\n        \"incumbent_ids\": [self.runhistory.get_config_id(config) for config in self._incumbents],\n        \"rejected_config_ids\": self._rejected_config_ids,\n        \"incumbents_changed\": self._incumbents_changed,\n        \"trajectory\": [dataclasses.asdict(item) for item in self._trajectory],\n        \"state\": self.get_state(),\n    }\n\n    with open(filename, \"w\") as fp:\n        json.dump(data, fp, indent=2, cls=NumpyEncoder)\n</code></pre>"},{"location":"api/smac/intensifier/successive_halving/#smac.intensifier.successive_halving.SuccessiveHalving.update_incumbents","title":"update_incumbents","text":"<pre><code>update_incumbents(config: Configuration) -&gt; None\n</code></pre> <p>Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only the affected config and the current incumbents are used, this method is very efficient. Furthermore, a configuration is only considered incumbent if it has a better performance on all incumbent instances.</p> <p>Crucially, if there is no incumbent (at the start) then, the first configuration assumes incumbent status. For the next configuration, we need to check if the configuration is better on all instances that have been evaluated for the incumbent. If this is the case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to obtain the missing instance-seed-budget combination or b) mark this configuration as inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().</p> <p>Notably, this method is written to support both multi-fidelity and multi-objective optimization. While the get_instance_seed_budget_keys() method and self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour, calculate_pareto_front() is used as a hard coded way to support multi-objective optimization, including the single objective as special case. calculate_pareto_front() is called on the set of all (in case of MO) incumbents amended with the challenger configuration, provided it has a sufficient overlap in seed-instance-budget combinations.</p> <p>Lastly, if we have a self._max_incumbents and the pareto front provides more than this specified amount, we cut the incumbents using crowding distance.</p> Source code in <code>smac/intensifier/abstract_intensifier.py</code> <pre><code>def update_incumbents(self, config: Configuration) -&gt; None:\n    \"\"\"Updates the incumbents. This method is called everytime a trial is added to the runhistory. Since only\n    the affected config and the current incumbents are used, this method is very efficient. Furthermore, a\n    configuration is only considered incumbent if it has a better performance on all incumbent instances.\n\n    Crucially, if there is no incumbent (at the start) then, the first configuration assumes\n    incumbent status. For the next configuration, we need to check if the configuration\n    is better on all instances that have been evaluated for the incumbent. If this is the\n    case, then we can replace the incumbent. Otherwise, a) we need to requeue the config to\n    obtain the missing instance-seed-budget combination or b) mark this configuration as\n    inferior (\"rejected\") to not consider it again. The comparison behaviour is controlled by\n    self.get_instance_seed_budget_keys() and self.get_incumbent_instance_seed_budget_keys().\n\n    Notably, this method is written to support both multi-fidelity and multi-objective\n    optimization. While the get_instance_seed_budget_keys() method and\n    self.get_incumbent_instance_seed_budget_keys() are used for the multi-fidelity behaviour,\n    calculate_pareto_front() is used as a hard coded way to support multi-objective\n    optimization, including the single objective as special case. calculate_pareto_front()\n    is called on the set of all (in case of MO) incumbents amended with the challenger\n    configuration, provided it has a sufficient overlap in seed-instance-budget combinations.\n\n    Lastly, if we have a self._max_incumbents and the pareto front provides more than this\n    specified amount, we cut the incumbents using crowding distance.\n    \"\"\"\n    rh = self.runhistory\n\n    # What happens if a config was rejected, but it appears again? Give it another try even if it\n    # has already been evaluated? Yes!\n\n    # Associated trials and id\n    config_isb_keys = self.get_instance_seed_budget_keys(config)\n    config_id = rh.get_config_id(config)\n    config_hash = get_config_hash(config)\n\n    # We skip updating incumbents if no instances are available\n    # Note: This is especially the case if trials of a config are still running\n    # because if trials are running, the runhistory does not update the trials in the fast data structure\n    if len(config_isb_keys) == 0:\n        logger.debug(f\"No relevant instances evaluated for config {config_hash}. Updating incumbents is skipped.\")\n        return\n\n    # Now we get the incumbents and see which trials have been used\n    incumbents = self.get_incumbents()\n    incumbent_ids = [rh.get_config_id(c) for c in incumbents]\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    incumbent_isb_keys = self.get_incumbent_instance_seed_budget_keys()\n\n    # Save for later\n    previous_incumbents = incumbents.copy()\n    previous_incumbent_ids = incumbent_ids.copy()\n\n    # Little sanity check here for consistency\n    if len(incumbents) &gt; 0:\n        assert incumbent_isb_keys is not None\n        assert len(incumbent_isb_keys) &gt; 0\n\n    # If there are no incumbents at all, we just use the new config as new incumbent\n    # Problem: We can add running incumbents\n    if len(incumbents) == 0:  # incumbent_isb_keys is None and len(incumbents) == 0:\n        logger.info(f\"Added config {config_hash} as new incumbent because there are no incumbents yet.\")\n        self._update_trajectory([config])\n\n        # Nothing else to do\n        return\n\n    # Comparison keys\n    # This one is a bit tricky: We would have problems if we compare with budgets because we might have different\n    # scenarios (depending on the incumbent selection specified in Successive Halving).\n    # 1) Any budget/highest observed budget: We want to get rid of the budgets because if we know it is calculated\n    # on the same instance-seed already then we are ready to go. Imagine we would check for the same budgets,\n    # then the configs can not be compared although the user does not care on which budgets configurations have\n    # been evaluated.\n    # 2) Highest budget: We only want to compare the configs if they are evaluated on the highest budget.\n    # Here we do actually care about the budgets. Please see the ``get_instance_seed_budget_keys`` method from\n    # Successive Halving to get more information.\n    # Noitce: compare=True only takes effect when subclass implemented it. -- e.g. in SH it\n    # will remove the budgets from the keys.\n    config_isb_comparison_keys = self.get_instance_seed_budget_keys(config, compare=True)\n    # Find the lowest intersection of instance-seed-budget keys for all incumbents.\n    config_incumbent_isb_comparison_keys = self.get_incumbent_instance_seed_budget_keys(compare=True)\n\n    # Now we have to check if the new config has been evaluated on the same keys as the incumbents\n    if not all([key in config_isb_comparison_keys for key in config_incumbent_isb_comparison_keys]):\n        # We can not tell if the new config is better/worse than the incumbents because it has not been\n        # evaluated on the necessary trials\n        logger.debug(\n            f\"Could not compare config {config_hash} with incumbents because it's evaluated on \"\n            f\"different trials.\"\n        )\n\n        # The config has to go to a queue now as it is a challenger and a potential incumbent\n        return\n    else:\n        # If all instances are available and the config is incumbent and even evaluated on more trials\n        # then there's nothing we can do\n        if config in incumbents and len(config_isb_keys) &gt; len(incumbent_isb_keys):\n            logger.debug(\n                \"Config is already an incumbent but can not be compared to other incumbents because \"\n                \"the others are missing trials.\"\n            )\n            return\n\n    # Add config to incumbents so that we compare only the new config and existing incumbents\n    if config not in incumbents:\n        incumbents.append(config)\n        incumbent_ids.append(config_id)\n\n    # Now we get all instance-seed-budget keys for each incumbent (they might be different when using budgets)\n    all_incumbent_isb_keys = []\n    for incumbent in incumbents:\n        all_incumbent_isb_keys.append(self.get_instance_seed_budget_keys(incumbent))\n\n    # We compare the incumbents now and only return the ones on the pareto front\n    new_incumbents = calculate_pareto_front(rh, incumbents, all_incumbent_isb_keys)\n    new_incumbent_ids = [rh.get_config_id(c) for c in new_incumbents]\n\n    if len(previous_incumbents) == len(new_incumbents):\n        if previous_incumbents == new_incumbents:\n            # No changes in the incumbents, we need this clause because we can't use set difference then\n            if config_id in new_incumbent_ids:\n                self._remove_rejected_config(config_id)\n            else:\n                # config worse than incumbents and thus rejected\n                self._add_rejected_config(config_id)\n            return\n        else:\n            # In this case, we have to determine which config replaced which incumbent and reject it\n            removed_incumbent_id = list(set(previous_incumbent_ids) - set(new_incumbent_ids))[0]\n            removed_incumbent_hash = get_config_hash(rh.get_config(removed_incumbent_id))\n            self._add_rejected_config(removed_incumbent_id)\n\n            if removed_incumbent_id == config_id:\n                logger.debug(\n                    f\"Rejected config {config_hash} because it is not better than the incumbents on \"\n                    f\"{len(config_isb_keys)} instances.\"\n                )\n            else:\n                self._remove_rejected_config(config_id)\n                logger.info(\n                    f\"Added config {config_hash} and rejected config {removed_incumbent_hash} as incumbent because \"\n                    f\"it is not better than the incumbents on {len(config_isb_keys)} instances: \"\n                )\n                print_config_changes(rh.get_config(removed_incumbent_id), config, logger=logger)\n    elif len(previous_incumbents) &lt; len(new_incumbents):\n        # Config becomes a new incumbent; nothing is rejected in this case\n        self._remove_rejected_config(config_id)\n        logger.info(\n            f\"Config {config_hash} is a new incumbent. \" f\"Total number of incumbents: {len(new_incumbents)}.\"\n        )\n    else:\n        # There might be situations that the incumbents might be removed because of updated cost information of\n        # config\n        for incumbent in previous_incumbents:\n            if incumbent not in new_incumbents:\n                self._add_rejected_config(incumbent)\n                logger.debug(\n                    f\"Removed incumbent {get_config_hash(incumbent)} because of the updated costs from config \"\n                    f\"{config_hash}.\"\n                )\n\n    # Cut incumbents: We only want to keep a specific number of incumbents\n    # We use the crowding distance for that\n    if len(new_incumbents) &gt; self._max_incumbents:\n        new_incumbents = sort_by_crowding_distance(rh, new_incumbents, all_incumbent_isb_keys)\n        new_incumbents = new_incumbents[: self._max_incumbents]\n\n        # or random?\n        # idx = self._rng.randint(0, len(new_incumbents))\n        # del new_incumbents[idx]\n        # del new_incumbent_ids[idx]\n\n        logger.info(\n            f\"Removed one incumbent using crowding distance because more than {self._max_incumbents} are \"\n            \"available.\"\n        )\n\n    self._update_trajectory(new_incumbents)\n</code></pre>"},{"location":"api/smac/main/config_selector/","title":"Config selector","text":""},{"location":"api/smac/main/config_selector/#smac.main.config_selector","title":"smac.main.config_selector","text":""},{"location":"api/smac/main/config_selector/#smac.main.config_selector.ConfigSelector","title":"ConfigSelector","text":"<pre><code>ConfigSelector(\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n    min_trials: int = 1\n)\n</code></pre> <p>The config selector handles the surrogate model and the acquisition function. Based on both components, the next configuration is selected.</p>"},{"location":"api/smac/main/config_selector/#smac.main.config_selector.ConfigSelector--parameters","title":"Parameters","text":"<p>retrain_after : int, defaults to 8     How many configurations should be returned before the surrogate model is retrained. retries : int, defaults to 8     How often to retry receiving a new configuration before giving up. min_trials: int, defaults to 1     How many samples are required to train the surrogate model. If budgets are involved,     the highest budgets are checked first. For example, if min_trials is three, but we find only     two trials in the runhistory for the highest budget, we will use trials of a lower budget     instead.</p> Source code in <code>smac/main/config_selector.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    *,\n    retrain_after: int = 8,\n    retries: int = 16,\n    min_trials: int = 1,\n) -&gt; None:\n    # Those are the configs sampled from the passed initial design\n    # Selecting configurations from initial design\n    self._initial_design_configs: list[Configuration] = []\n\n    # Set classes globally\n    self._scenario = scenario\n    self._runhistory: RunHistory | None = None\n    self._runhistory_encoder: AbstractRunHistoryEncoder | None = None\n    self._model: AbstractModel | None = None\n    self._acquisition_maximizer: AbstractAcquisitionMaximizer | None = None\n    self._acquisition_function: AbstractAcquisitionFunction | None = None\n    self._random_design: AbstractRandomDesign | None = None\n    self._callbacks: list[Callback] = []\n\n    # And other variables\n    self._retrain_after = retrain_after\n    self._previous_entries = -1\n    self._predict_x_best = True\n    self._min_trials = min_trials\n    self._considered_budgets: list[float | int | None] = [None]\n\n    # How often to retry receiving a new configuration\n    # (counter increases if the received config was already returned before)\n    self._retries = retries\n\n    # Processed configurations should be stored here; this is important to not return the same configuration twice\n    self._processed_configs: list[Configuration] = []\n</code></pre>"},{"location":"api/smac/main/config_selector/#smac.main.config_selector.ConfigSelector.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/main/config_selector/#smac.main.config_selector.ConfigSelector.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Configuration]\n</code></pre> <p>This method returns the next configuration to evaluate. It ignores already processed configurations, i.e., the configurations from the runhistory, if the runhistory is not empty. The method (after yielding the initial design configurations) trains the surrogate model, maximizes the acquisition function and yields <code>n</code> configurations. After the <code>n</code> configurations, the surrogate model is trained again, etc. The program stops if <code>retries</code> was reached within each iteration. A configuration is ignored, if it was used already before.</p>"},{"location":"api/smac/main/config_selector/#smac.main.config_selector.ConfigSelector.__iter__--note","title":"Note","text":"<p>When SMAC continues a run, processed configurations from the runhistory are ignored. For example, if the intitial design configurations already have been processed, they are ignored here. After the run is continued, however, the surrogate model is trained based on the runhistory in all cases.</p>"},{"location":"api/smac/main/config_selector/#smac.main.config_selector.ConfigSelector.__iter__--returns","title":"Returns","text":"<p>next_config : Iterator[Configuration]     The next configuration to evaluate.</p> Source code in <code>smac/main/config_selector.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Configuration]:\n    \"\"\"This method returns the next configuration to evaluate. It ignores already processed configurations, i.e.,\n    the configurations from the runhistory, if the runhistory is not empty.\n    The method (after yielding the initial design configurations) trains the surrogate model, maximizes the\n    acquisition function and yields ``n`` configurations. After the ``n`` configurations, the surrogate model is\n    trained again, etc. The program stops if ``retries`` was reached within each iteration. A configuration\n    is ignored, if it was used already before.\n\n    Note\n    ----\n    When SMAC continues a run, processed configurations from the runhistory are ignored. For example, if the\n    intitial design configurations already have been processed, they are ignored here. After the run is\n    continued, however, the surrogate model is trained based on the runhistory in all cases.\n\n    Returns\n    -------\n    next_config : Iterator[Configuration]\n        The next configuration to evaluate.\n    \"\"\"\n    assert self._runhistory is not None\n    assert self._runhistory_encoder is not None\n    assert self._model is not None\n    assert self._acquisition_maximizer is not None\n    assert self._acquisition_function is not None\n    assert self._random_design is not None\n\n    self._processed_configs = self._runhistory.get_configs()\n\n    # We add more retries because there could be a case in which the processed configs are sampled again\n    self._retries += len(self._processed_configs)\n\n    logger.debug(\"Search for the next configuration...\")\n    self._call_callbacks_on_start()\n\n    # First: We return the initial configurations\n    for config in self._initial_design_configs:\n        if config not in self._processed_configs:\n            self._processed_configs.append(config)\n            self._call_callbacks_on_end(config)\n            yield config\n            self._call_callbacks_on_start()\n\n    # We want to generate configurations endlessly\n    while True:\n        # Cost value of incumbent configuration (required for acquisition function).\n        # If not given, it will be inferred from runhistory or predicted.\n        # If not given and runhistory is empty, it will raise a ValueError.\n        incumbent_value: float | None = None\n\n        # Everytime we re-train the surrogate model, we also update our multi-objective algorithm\n        if (mo := self._runhistory_encoder.multi_objective_algorithm) is not None:\n            mo.update_on_iteration_start()\n\n        X, Y, X_configurations = self._collect_data()\n        previous_configs = self._runhistory.get_configs()\n\n        if X.shape[0] == 0:\n            # Only return a single point to avoid an overly high number of random search iterations.\n            # We got rid of random search here and replaced it with a simple configuration sampling from\n            # the configspace.\n            logger.debug(\"No data available to train the model. Sample a random configuration.\")\n\n            config = self._scenario.configspace.sample_configuration()\n            self._call_callbacks_on_end(config)\n            yield config\n            self._call_callbacks_on_start()\n\n            # Important to continue here because we still don't have data available\n            continue\n\n        # Check if X/Y differs from the last run, otherwise use cached results\n        if self._previous_entries != Y.shape[0]:\n            self._model.train(X, Y)\n\n            x_best_array: np.ndarray | None = None\n            if incumbent_value is not None:\n                best_observation = incumbent_value\n            else:\n                if self._runhistory.empty():\n                    raise ValueError(\"Runhistory is empty and the cost value of the incumbent is unknown.\")\n\n                x_best_array, best_observation = self._get_x_best(X_configurations)\n\n            self._acquisition_function.update(\n                model=self._model,\n                eta=best_observation,\n                incumbent_array=x_best_array,\n                num_data=len(self._get_evaluated_configs()),\n                X=X_configurations,\n            )\n\n        # We want to cache how many entries we used because if we have the same number of entries\n        # we don't need to train the next time\n        self._previous_entries = Y.shape[0]\n\n        # Now we maximize the acquisition function\n        challengers = self._acquisition_maximizer.maximize(\n            previous_configs,\n            random_design=self._random_design,\n        )\n\n        counter = 0\n        failed_counter = 0\n        for config in challengers:\n            if config not in self._processed_configs:\n                counter += 1\n                self._processed_configs.append(config)\n                self._call_callbacks_on_end(config)\n                yield config\n                retrain = counter == self._retrain_after\n                self._call_callbacks_on_start()\n\n                # We break to enforce a new iteration of the while loop (i.e. we retrain the surrogate model)\n                if retrain:\n                    logger.debug(\n                        f\"Yielded {counter} configurations. Start new iteration and retrain surrogate model.\"\n                    )\n                    break\n            else:\n                failed_counter += 1\n\n                # We exit the loop if we have tried to add the same configuration too often\n                if failed_counter == self._retries:\n                    logger.warning(f\"Could not return a new configuration after {self._retries} retries.\" \"\")\n                    return\n</code></pre>"},{"location":"api/smac/main/smbo/","title":"Smbo","text":""},{"location":"api/smac/main/smbo/#smac.main.smbo","title":"smac.main.smbo","text":""},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO","title":"SMBO","text":"<pre><code>SMBO(\n    scenario: Scenario,\n    runner: AbstractRunner,\n    runhistory: RunHistory,\n    intensifier: AbstractIntensifier,\n    overwrite: bool = False,\n)\n</code></pre> <p>Implementation that contains the main Bayesian optimization loop.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO--parameters","title":"Parameters","text":"<p>scenario : Scenario     The scenario object, holding all environmental information. runner : AbstractRunner     The runner (containing the target function) is called internally to judge a trial's performance. runhistory : Runhistory     The runhistory stores all trials. intensifier : AbstractIntensifier     The intensifier decides which trial (combination of configuration, seed, budget and instance) should be run     next. overwrite: bool, defaults to False     When True, overwrites the run results if a previous run is found that is     inconsistent in the meta data with the current setup. If <code>overwrite</code> is set to False, the user is asked     for the exact behaviour (overwrite completely, save old run, or use old results).</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO--warning","title":"Warning","text":"<p>This model should be initialized by a facade only.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    runner: AbstractRunner,\n    runhistory: RunHistory,\n    intensifier: AbstractIntensifier,\n    overwrite: bool = False,\n):\n    self._scenario = scenario\n    self._configspace = scenario.configspace\n    self._runhistory = runhistory\n    self._intensifier = intensifier\n    self._trial_generator = iter(intensifier)\n    self._runner = runner\n    self._overwrite = overwrite\n\n    # Internal variables\n    self._finished = False\n    self._stop = False  # Gracefully stop SMAC\n    self._callbacks: list[Callback] = []\n\n    # Stats variables\n    self._start_time: float | None = None\n    self._used_target_function_walltime = 0.0\n    self._used_target_function_cputime = 0.0\n\n    # Set walltime used method for intensifier\n    self._intensifier.used_walltime = lambda: self.used_walltime  # type: ignore\n\n    # We initialize the state based on previous data.\n    # If no previous data is found then we take care of the initial design.\n    self._initialize_state()\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.budget_exhausted","title":"budget_exhausted  <code>property</code>","text":"<pre><code>budget_exhausted: bool\n</code></pre> <p>Checks whether the the remaining walltime, cputime or trials was exceeded.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.intensifier","title":"intensifier  <code>property</code>","text":"<pre><code>intensifier: AbstractIntensifier\n</code></pre> <p>The run history, which is filled with all information during the optimization process.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.remaining_cputime","title":"remaining_cputime  <code>property</code>","text":"<pre><code>remaining_cputime: float\n</code></pre> <p>Subtracts the target function running budget with the used time.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.remaining_trials","title":"remaining_trials  <code>property</code>","text":"<pre><code>remaining_trials: int\n</code></pre> <p>Subtract the target function runs in the scenario with the used ta runs.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.remaining_walltime","title":"remaining_walltime  <code>property</code>","text":"<pre><code>remaining_walltime: float\n</code></pre> <p>Subtracts the runtime configuration budget with the used wallclock time.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.runhistory","title":"runhistory  <code>property</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The run history, which is filled with all information during the optimization process.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.used_target_function_cputime","title":"used_target_function_cputime  <code>property</code>","text":"<pre><code>used_target_function_cputime: float\n</code></pre> <p>Returns how much time the target function spend on the hardware so far.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.used_target_function_walltime","title":"used_target_function_walltime  <code>property</code>","text":"<pre><code>used_target_function_walltime: float\n</code></pre> <p>Returns how much walltime the target function spend so far.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.used_walltime","title":"used_walltime  <code>property</code>","text":"<pre><code>used_walltime: float\n</code></pre> <p>Returns used wallclock time.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.ask","title":"ask","text":"<pre><code>ask() -&gt; TrialInfo\n</code></pre> <p>Asks the intensifier for the next trial.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.ask--returns","title":"Returns","text":"<p>info : TrialInfo     Information about the trial (config, instance, seed, budget).</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def ask(self) -&gt; TrialInfo:\n    \"\"\"Asks the intensifier for the next trial.\n\n    Returns\n    -------\n    info : TrialInfo\n        Information about the trial (config, instance, seed, budget).\n    \"\"\"\n    logger.debug(\"Calling ask...\")\n\n    for callback in self._callbacks:\n        callback.on_ask_start(self)\n\n    # Now we use our generator to get the next trial info\n    trial_info = next(self._trial_generator)\n\n    # Track the fact that the trial was returned\n    # This is really important because otherwise the intensifier would most likly sample the same trial again\n    self._runhistory.add_running_trial(trial_info)\n\n    for callback in self._callbacks:\n        callback.on_ask_end(self, trial_info)\n\n    logger.debug(\"...and received a new trial.\")\n\n    return trial_info\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.exists","title":"exists","text":"<pre><code>exists(filename: str | Path) -&gt; bool\n</code></pre> <p>Checks if the files associated with the run already exist. Checks all files that are created by the optimizer.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.exists--parameters","title":"Parameters","text":"<p>filename : str | Path     The name of the folder of the SMAC run.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def exists(self, filename: str | Path) -&gt; bool:\n    \"\"\"Checks if the files associated with the run already exist.\n    Checks all files that are created by the optimizer.\n\n    Parameters\n    ----------\n    filename : str | Path\n        The name of the folder of the SMAC run.\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    optimization_fn = filename / \"optimization.json\"\n    runhistory_fn = filename / \"runhistory.json\"\n    intensifier_fn = filename / \"intensifier.json\"\n\n    if optimization_fn.exists() and runhistory_fn.exists() and intensifier_fn.exists():\n        return True\n\n    return False\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.load","title":"load","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Loads the optimizer, intensifier, and runhistory from the output directory specified in the scenario.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Loads the optimizer, intensifier, and runhistory from the output directory specified in the scenario.\"\"\"\n    filename = self._scenario.output_directory\n\n    optimization_fn = filename / \"optimization.json\"\n    runhistory_fn = filename / \"runhistory.json\"\n    intensifier_fn = filename / \"intensifier.json\"\n\n    if filename is not None:\n        with open(optimization_fn) as fp:\n            data = json.load(fp)\n\n        self._runhistory.load(runhistory_fn, configspace=self._scenario.configspace)\n        self._intensifier.load(intensifier_fn)\n\n        self._used_target_function_walltime = data[\"used_target_function_walltime\"]\n        self._used_target_function_cputime = data[\"used_target_function_cputime\"]\n        self._finished = data[\"finished\"]\n        self._start_time = time.time() - data[\"used_walltime\"]\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.optimize","title":"optimize","text":"<pre><code>optimize(\n    *, data_to_scatter: dict[str, Any] | None = None\n) -&gt; Configuration | list[Configuration]\n</code></pre> <p>Runs the Bayesian optimization loop.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.optimize--parameters","title":"Parameters","text":"<p>data_to_scatter: dict[str, Any] | None     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.optimize--returns","title":"Returns","text":"<p>incumbent : Configuration     The best found configuration.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def optimize(self, *, data_to_scatter: dict[str, Any] | None = None) -&gt; Configuration | list[Configuration]:\n    \"\"\"Runs the Bayesian optimization loop.\n\n    Parameters\n    ----------\n    data_to_scatter: dict[str, Any] | None\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    incumbent : Configuration\n        The best found configuration.\n    \"\"\"\n    # We return the incumbent if we already finished the a process (we don't want to allow to call\n    # optimize more than once).\n    if self._finished:\n        logger.info(\"Optimization process was already finished. Returning incumbent...\")\n        if self._scenario.count_objectives() == 1:\n            return self.intensifier.get_incumbent()\n        else:\n            return self.intensifier.get_incumbents()\n\n    # Start the timer before we do anything\n    # If we continue the optimization, the starting time is set by the load method\n    if self._start_time is None:\n        self._start_time = time.time()\n\n    for callback in self._callbacks:\n        callback.on_start(self)\n\n    dask_data_to_scatter = {}\n    if isinstance(self._runner, DaskParallelRunner) and data_to_scatter is not None:\n        dask_data_to_scatter = dict(data_to_scatter=self._runner._client.scatter(data_to_scatter, broadcast=True))\n    elif data_to_scatter is not None:\n        raise ValueError(\n            \"data_to_scatter is valid only for DaskParallelRunner, \"\n            f\"but {dask_data_to_scatter} was provided for {self._runner.__class__.__name__}\"\n        )\n\n    # Main BO loop\n    while True:\n        for callback in self._callbacks:\n            callback.on_iteration_start(self)\n\n        try:\n            # Sample next trial from the intensification\n            trial_info = self.ask()\n\n            # We submit the trial to the runner\n            # In multi-worker mode, SMAC waits till a new worker is available here\n            self._runner.submit_trial(trial_info=trial_info, **dask_data_to_scatter)\n        except StopIteration:\n            self._stop = True\n\n        # We add results from the runner if results are available\n        self._add_results()\n\n        # Some statistics\n        logger.debug(\n            f\"Remaining wallclock time: {self.remaining_walltime}; \"\n            f\"Remaining cpu time: {self.remaining_cputime}; \"\n            f\"Remaining trials: {self.remaining_trials}\"\n        )\n\n        if self.runhistory.finished % 50 == 0:\n            logger.info(f\"Finished {self.runhistory.finished} trials.\")\n\n        for callback in self._callbacks:\n            callback.on_iteration_end(self)\n\n        # Now we check whether we have to stop the optimization\n        if self.budget_exhausted or self._stop:\n            if self.budget_exhausted:\n                logger.info(\"Configuration budget is exhausted:\")\n                logger.info(f\"--- Remaining wallclock time: {self.remaining_walltime}\")\n                logger.info(f\"--- Remaining cpu time: {self.remaining_cputime}\")\n                logger.info(f\"--- Remaining trials: {self.remaining_trials}\")\n            else:\n                logger.info(\"Shutting down because the stop flag was set.\")\n\n            # Wait for the trials to finish\n            while self._runner.is_running():\n                self._runner.wait()\n                self._add_results()\n\n            # Break from the intensification loop, as there are no more resources\n            break\n\n    for callback in self._callbacks:\n        callback.on_end(self)\n\n    # We only set the finished flag if the budget really was exhausted\n    if self.budget_exhausted:\n        self._finished = True\n\n    if self._scenario.count_objectives() == 1:\n        return self.intensifier.get_incumbent()\n    else:\n        return self.intensifier.get_incumbents()\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.print_stats","title":"print_stats","text":"<pre><code>print_stats() -&gt; None\n</code></pre> <p>Prints all statistics.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def print_stats(self) -&gt; None:\n    \"\"\"Prints all statistics.\"\"\"\n    logger.info(\n        \"\\n\"\n        f\"--- STATISTICS -------------------------------------\\n\"\n        f\"--- Incumbent changed: {self.intensifier.incumbents_changed}\\n\"\n        f\"--- Submitted trials: {self.runhistory.submitted} / {self._scenario.n_trials}\\n\"\n        f\"--- Finished trials: {self.runhistory.finished} / {self._scenario.n_trials}\\n\"\n        f\"--- Configurations: {self.runhistory._n_id}\\n\"\n        f\"--- Used wallclock time: {round(self.used_walltime)} / {self._scenario.walltime_limit} sec\\n\"\n        \"--- Used target function runtime: \"\n        f\"{round(self.used_target_function_walltime, 2)} / {self._scenario.cputime_limit} sec\\n\"\n        \"--- Used target function CPU time: \"\n        f\"{round(self.used_target_function_cputime, 2)} / {self._scenario.cputime_limit} sec\\n\"\n        f\"----------------------------------------------------\"\n    )\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.register_callback","title":"register_callback","text":"<pre><code>register_callback(\n    callback: Callback, index: int | None = None\n) -&gt; None\n</code></pre> <p>Registers a callback to be called before, in between, and after the Bayesian optimization loop.</p> <p>Callback is appended to the list by default.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.register_callback--parameters","title":"Parameters","text":"<p>callback : Callback     The callback to be registered. index : int, optional     The index at which the callback should be registered. The default is None.     If it is None, append the callback to the list.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def register_callback(self, callback: Callback, index: int | None = None) -&gt; None:\n    \"\"\"\n    Registers a callback to be called before, in between, and after the Bayesian optimization loop.\n\n    Callback is appended to the list by default.\n\n    Parameters\n    ----------\n    callback : Callback\n        The callback to be registered.\n    index : int, optional\n        The index at which the callback should be registered. The default is None.\n        If it is None, append the callback to the list.\n    \"\"\"\n    if index is None:\n        index = len(self._callbacks)\n    self._callbacks.insert(index, callback)\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Resets the internal variables of the optimizer, intensifier, and runhistory.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Resets the internal variables of the optimizer, intensifier, and runhistory.\"\"\"\n    self._used_target_function_walltime = 0\n    self._used_target_function_cputime = 0\n    self._finished = False\n\n    # We also reset runhistory and intensifier here\n    self._runhistory.reset()\n    self._intensifier.reset()\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Saves the current stats, runhistory, and intensifier.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Saves the current stats, runhistory, and intensifier.\"\"\"\n    path = self._scenario.output_directory\n\n    if path is not None:\n        data = {\n            \"used_walltime\": self.used_walltime,\n            \"used_target_function_walltime\": self.used_target_function_walltime,\n            \"used_target_function_cputime\": self.used_target_function_cputime,\n            \"last_update\": time.time(),\n            \"finished\": self._finished,\n        }\n\n        # Save optimization data\n        with open(str(path / \"optimization.json\"), \"w\") as file:\n            json.dump(data, file, indent=2, cls=NumpyEncoder)\n\n        # And save runhistory and intensifier\n        self._runhistory.save(path / \"runhistory.json\")\n        self._intensifier.save(path / \"intensifier.json\")\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.tell","title":"tell","text":"<pre><code>tell(\n    info: TrialInfo, value: TrialValue, save: bool = True\n) -&gt; None\n</code></pre> <p>Adds the result of a trial to the runhistory and updates the stats object.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.tell--parameters","title":"Parameters","text":"<p>info : TrialInfo     Describes the trial from which to process the results. value : TrialValue     Contains relevant information regarding the execution of a trial. save : bool, optional to True     Whether the runhistory should be saved.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def tell(\n    self,\n    info: TrialInfo,\n    value: TrialValue,\n    save: bool = True,\n) -&gt; None:\n    \"\"\"Adds the result of a trial to the runhistory and updates the stats object.\n\n    Parameters\n    ----------\n    info : TrialInfo\n        Describes the trial from which to process the results.\n    value : TrialValue\n        Contains relevant information regarding the execution of a trial.\n    save : bool, optional to True\n        Whether the runhistory should be saved.\n    \"\"\"\n    if info.config.origin is None:\n        info.config.origin = \"Custom\"\n\n    for callback in self._callbacks:\n        response = callback.on_tell_start(self, info, value)\n\n        # If a callback returns False, the optimization loop should be interrupted\n        # the other callbacks are still being called.\n        if response is False:\n            logger.info(\"A callback returned False. Abort is requested.\")\n            self._stop = True\n\n    # Some sanity checks here\n    if self._intensifier.uses_instances and info.instance is None:\n        raise ValueError(\"Passed instance is None but intensifier requires instances.\")\n\n    if self._intensifier.uses_budgets and info.budget is None:\n        raise ValueError(\"Passed budget is None but intensifier requires budgets.\")\n\n    self._runhistory.add(\n        config=info.config,\n        cost=value.cost,\n        time=value.time,\n        cpu_time=value.cpu_time,\n        status=value.status,\n        instance=info.instance,\n        seed=info.seed,\n        budget=info.budget,\n        starttime=value.starttime,\n        endtime=value.endtime,\n        additional_info=value.additional_info,\n        force_update=True,  # Important to overwrite the status RUNNING\n    )\n\n    logger.debug(f\"Tell method was called with cost {value.cost} ({StatusType(value.status).name}).\")\n\n    for callback in self._callbacks:\n        response = callback.on_tell_end(self, info, value)\n\n        # If a callback returns False, the optimization loop should be interrupted\n        # the other callbacks are still being called.\n        if response is False:\n            logger.info(\"A callback returned False. Abort is requested.\")\n            self._stop = True\n\n    if save:\n        self.save()\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.update_acquisition_function","title":"update_acquisition_function","text":"<pre><code>update_acquisition_function(\n    acquisition_function: AbstractAcquisitionFunction,\n) -&gt; None\n</code></pre> <p>Updates the acquisition function including the associated model and the acquisition optimizer.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def update_acquisition_function(self, acquisition_function: AbstractAcquisitionFunction) -&gt; None:\n    \"\"\"Updates the acquisition function including the associated model and the acquisition\n    optimizer.\n    \"\"\"\n    if (config_selector := self._intensifier._config_selector) is not None:\n        config_selector._acquisition_function = acquisition_function\n        config_selector._acquisition_function.model = config_selector._model\n\n        assert config_selector._acquisition_maximizer is not None\n        config_selector._acquisition_maximizer.acquisition_function = acquisition_function\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.update_model","title":"update_model","text":"<pre><code>update_model(model: AbstractModel) -&gt; None\n</code></pre> <p>Updates the model and updates the acquisition function.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def update_model(self, model: AbstractModel) -&gt; None:\n    \"\"\"Updates the model and updates the acquisition function.\"\"\"\n    if (config_selector := self._intensifier._config_selector) is not None:\n        config_selector._model = model\n\n        assert config_selector._acquisition_function is not None\n        config_selector._acquisition_function.model = model\n</code></pre>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.validate","title":"validate","text":"<pre><code>validate(\n    config: Configuration, *, seed: int | None = None\n) -&gt; float | ndarray[float]\n</code></pre> <p>Validates a configuration on other seeds than the ones used in the optimization process and on the highest budget (if budget type is real-valued). Does not exceed the maximum number of config calls or seeds as defined in the scenario.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.validate--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to validate     In case that the budget type is real-valued budget, this argument is ignored. seed : int | None, defaults to None     If None, the seed from the scenario is used.</p>"},{"location":"api/smac/main/smbo/#smac.main.smbo.SMBO.validate--returns","title":"Returns","text":"<p>cost : float | ndarray[float]     The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is     averaged.</p> Source code in <code>smac/main/smbo.py</code> <pre><code>def validate(\n    self,\n    config: Configuration,\n    *,\n    seed: int | None = None,\n) -&gt; float | ndarray[float]:\n    \"\"\"Validates a configuration on other seeds than the ones used in the optimization process and on the highest\n    budget (if budget type is real-valued). Does not exceed the maximum number of config calls or seeds as defined\n    in the scenario.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to validate\n        In case that the budget type is real-valued budget, this argument is ignored.\n    seed : int | None, defaults to None\n        If None, the seed from the scenario is used.\n\n    Returns\n    -------\n    cost : float | ndarray[float]\n        The averaged cost of the configuration. In case of multi-fidelity, the cost of each objective is\n        averaged.\n    \"\"\"\n    if seed is None:\n        seed = self._scenario.seed\n\n    costs = []\n    for trial in self._intensifier.get_trials_of_interest(config, validate=True, seed=seed):\n        kwargs: dict[str, Any] = {}\n        if trial.seed is not None:\n            kwargs[\"seed\"] = trial.seed\n        if trial.budget is not None:\n            kwargs[\"budget\"] = trial.budget\n        if trial.instance is not None:\n            kwargs[\"instance\"] = trial.instance\n\n        # TODO: Use submit run for faster evaluation\n        # self._runner.submit_trial(trial_info=trial)\n        _, cost, _, _, _ = self._runner.run(config, **kwargs)\n        costs += [cost]\n\n    np_costs = np.array(costs)\n    return np.mean(np_costs, axis=0)\n</code></pre>"},{"location":"api/smac/model/abstract_model/","title":"Abstract model","text":""},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model","title":"smac.model.abstract_model","text":""},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel","title":"AbstractModel","text":"<pre><code>AbstractModel(\n    configspace: ConfigurationSpace,\n    instance_features: (\n        dict[str, list[int | float]] | None\n    ) = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n)\n</code></pre> <p>Abstract implementation of the surrogate model.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel--note","title":"Note","text":"<p>The input dimensionality of Y for training and the output dimensions of all predictions depend on the concrete implementation of this abstract class.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace instance_features : dict[str, list[int | float]] | None, defaults to None     Features (list of int or floats) of the instances (str). The features are incorporated into the X data,     on which the model is trained on. pca_components : float, defaults to 7     Number of components to keep when using PCA to reduce dimensionality of instance features. seed : int</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    instance_features: dict[str, list[int | float]] | None = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n) -&gt; None:\n    self._configspace = configspace\n    self._seed = seed\n    self._rng = np.random.RandomState(self._seed)\n    self._instance_features = instance_features\n    self._pca_components = pca_components\n\n    n_features = 0\n    if self._instance_features is not None:\n        for v in self._instance_features.values():\n            if n_features == 0:\n                n_features = len(v)\n            else:\n                if len(v) != n_features:\n                    raise RuntimeError(\"Instances must have the same number of features.\")\n\n    self._n_features = n_features\n    self._n_hps = len(list(self._configspace.values()))\n\n    self._pca = PCA(n_components=self._pca_components)\n    self._scaler = MinMaxScaler()\n    self._apply_pca = False\n\n    # Never use a lower variance than this.\n    # If estimated variance &lt; var_threshold, set to var_threshold\n    self._var_threshold = VERY_SMALL_NUMBER\n    self._types, self._bounds = get_types(configspace, instance_features)\n\n    # Initial types array which is used to reset the type array at every call to `self.train()`\n    self._initial_types = copy.deepcopy(self._types)\n</code></pre>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.predict_marginalized","title":"predict_marginalized","text":"<pre><code>predict_marginalized(X: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Predicts mean and variance marginalized over all instances.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.predict_marginalized--warning","title":"Warning","text":"<p>The input data must not include any features.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.predict_marginalized--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters]     Input data points.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.predict_marginalized--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, 1]     The predictive mean. vars : np.ndarray [#samples, 1]     The predictive variance.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict_marginalized(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predicts mean and variance marginalized over all instances.\n\n    Warning\n    -------\n    The input data must not include any features.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters]\n        Input data points.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, 1]\n        The predictive mean.\n    vars : np.ndarray [#samples, 1]\n        The predictive variance.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters (and no features) for this method, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._instance_features is None:\n        mean, var = self.predict(X)\n        assert var is not None\n\n        var[var &lt; self._var_threshold] = self._var_threshold\n        var[np.isnan(var)] = self._var_threshold\n\n        return mean, var\n    else:\n        n_instances = len(self._instance_features)\n\n        mean = np.zeros(X.shape[0])\n        var = np.zeros(X.shape[0])\n        for i, x in enumerate(X):\n            features = np.array(list(self._instance_features.values()))\n            x_tiled = np.tile(x, (n_instances, 1))\n            X_ = np.hstack((x_tiled, features))\n\n            means, vars = self.predict(X_)\n            assert vars is not None\n\n            # VAR[1/n (X_1 + ... + X_n)] =\n            # 1/n^2 * ( VAR(X_1) + ... + VAR(X_n))\n            # for independent X_1 ... X_n\n            var_x = np.sum(vars) / (len(vars) ** 2)\n            if var_x &lt; self._var_threshold:\n                var_x = self._var_threshold\n\n            var[i] = var_x\n            mean[i] = np.mean(means)\n\n        if len(mean.shape) == 1:\n            mean = mean.reshape((-1, 1))\n\n        if len(var.shape) == 1:\n            var = var.reshape((-1, 1))\n\n        return mean, var\n</code></pre>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/abstract_model/#smac.model.abstract_model.AbstractModel.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/model/multi_objective_model/","title":"Multi objective model","text":""},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model","title":"smac.model.multi_objective_model","text":""},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel","title":"MultiObjectiveModel","text":"<pre><code>MultiObjectiveModel(\n    models: AbstractModel | list[AbstractModel],\n    objectives: list[str],\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractModel</code></p> <p>Wrapper for the surrogate model to predict multiple objectives.</p>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel--parameters","title":"Parameters","text":"<p>models : AbstractModel | list[AbstractModel]     Which model should be used. If it is a list, then it must provide as many models as objectives.     If it is a single model only, the model is used for all objectives. objectives : list[str]     Which objectives should be used. seed : int</p> Source code in <code>smac/model/multi_objective_model.py</code> <pre><code>def __init__(\n    self,\n    models: AbstractModel | list[AbstractModel],\n    objectives: list[str],\n    seed: int = 0,\n) -&gt; None:\n    self._n_objectives = len(objectives)\n    if isinstance(models, list):\n        assert len(models) == len(objectives)\n\n        # Make sure the configspace is the same\n        configspace = models[0]._configspace\n        for m in models:\n            assert configspace == m._configspace\n\n        self._models = models\n    else:\n        configspace = models._configspace\n        self._models = [models for _ in range(self._n_objectives)]\n\n    super().__init__(\n        configspace=configspace,\n        instance_features=None,\n        pca_components=None,\n        seed=seed,\n    )\n</code></pre>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.models","title":"models  <code>property</code>","text":"<pre><code>models: list[AbstractModel]\n</code></pre> <p>The internally used surrogate models.</p>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/multi_objective_model/#smac.model.multi_objective_model.MultiObjectiveModel.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/model/random_model/","title":"Random model","text":""},{"location":"api/smac/model/random_model/#smac.model.random_model","title":"smac.model.random_model","text":""},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel","title":"RandomModel","text":"<pre><code>RandomModel(\n    configspace: ConfigurationSpace,\n    instance_features: (\n        dict[str, list[int | float]] | None\n    ) = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractModel</code></p> <p>AbstractModel which returns random values on a call to <code>fit</code>.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    instance_features: dict[str, list[int | float]] | None = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n) -&gt; None:\n    self._configspace = configspace\n    self._seed = seed\n    self._rng = np.random.RandomState(self._seed)\n    self._instance_features = instance_features\n    self._pca_components = pca_components\n\n    n_features = 0\n    if self._instance_features is not None:\n        for v in self._instance_features.values():\n            if n_features == 0:\n                n_features = len(v)\n            else:\n                if len(v) != n_features:\n                    raise RuntimeError(\"Instances must have the same number of features.\")\n\n    self._n_features = n_features\n    self._n_hps = len(list(self._configspace.values()))\n\n    self._pca = PCA(n_components=self._pca_components)\n    self._scaler = MinMaxScaler()\n    self._apply_pca = False\n\n    # Never use a lower variance than this.\n    # If estimated variance &lt; var_threshold, set to var_threshold\n    self._var_threshold = VERY_SMALL_NUMBER\n    self._types, self._bounds = get_types(configspace, instance_features)\n\n    # Initial types array which is used to reset the type array at every call to `self.train()`\n    self._initial_types = copy.deepcopy(self._types)\n</code></pre>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.predict_marginalized","title":"predict_marginalized","text":"<pre><code>predict_marginalized(X: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Predicts mean and variance marginalized over all instances.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.predict_marginalized--warning","title":"Warning","text":"<p>The input data must not include any features.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.predict_marginalized--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters]     Input data points.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.predict_marginalized--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, 1]     The predictive mean. vars : np.ndarray [#samples, 1]     The predictive variance.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict_marginalized(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predicts mean and variance marginalized over all instances.\n\n    Warning\n    -------\n    The input data must not include any features.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters]\n        Input data points.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, 1]\n        The predictive mean.\n    vars : np.ndarray [#samples, 1]\n        The predictive variance.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters (and no features) for this method, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._instance_features is None:\n        mean, var = self.predict(X)\n        assert var is not None\n\n        var[var &lt; self._var_threshold] = self._var_threshold\n        var[np.isnan(var)] = self._var_threshold\n\n        return mean, var\n    else:\n        n_instances = len(self._instance_features)\n\n        mean = np.zeros(X.shape[0])\n        var = np.zeros(X.shape[0])\n        for i, x in enumerate(X):\n            features = np.array(list(self._instance_features.values()))\n            x_tiled = np.tile(x, (n_instances, 1))\n            X_ = np.hstack((x_tiled, features))\n\n            means, vars = self.predict(X_)\n            assert vars is not None\n\n            # VAR[1/n (X_1 + ... + X_n)] =\n            # 1/n^2 * ( VAR(X_1) + ... + VAR(X_n))\n            # for independent X_1 ... X_n\n            var_x = np.sum(vars) / (len(vars) ** 2)\n            if var_x &lt; self._var_threshold:\n                var_x = self._var_threshold\n\n            var[i] = var_x\n            mean[i] = np.mean(means)\n\n        if len(mean.shape) == 1:\n            mean = mean.reshape((-1, 1))\n\n        if len(var.shape) == 1:\n            var = var.reshape((-1, 1))\n\n        return mean, var\n</code></pre>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/random_model/#smac.model.random_model.RandomModel.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/","title":"Abstract gaussian process","text":""},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process","title":"smac.model.gaussian_process.abstract_gaussian_process","text":""},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess","title":"AbstractGaussianProcess","text":"<pre><code>AbstractGaussianProcess(\n    configspace: ConfigurationSpace,\n    kernel: Kernel,\n    instance_features: (\n        dict[str, list[int | float]] | None\n    ) = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractModel</code></p> <p>Abstract base class for all Gaussian process models.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace kernel : Kernel     Kernel which is used for the Gaussian process. instance_features : dict[str, list[int | float]] | None, defaults to None     Features (list of int or floats) of the instances (str). The features are incorporated into the X data,     on which the model is trained on. pca_components : float, defaults to 7     Number of components to keep when using PCA to reduce dimensionality of instance features. seed : int</p> Source code in <code>smac/model/gaussian_process/abstract_gaussian_process.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    kernel: Kernel,\n    instance_features: dict[str, list[int | float]] | None = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n):\n    super().__init__(\n        configspace=configspace,\n        instance_features=instance_features,\n        pca_components=pca_components,\n        seed=seed,\n    )\n\n    self._kernel = kernel\n    self._gp = self._get_gaussian_process()\n</code></pre>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.predict_marginalized","title":"predict_marginalized","text":"<pre><code>predict_marginalized(X: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Predicts mean and variance marginalized over all instances.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.predict_marginalized--warning","title":"Warning","text":"<p>The input data must not include any features.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.predict_marginalized--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters]     Input data points.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.predict_marginalized--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, 1]     The predictive mean. vars : np.ndarray [#samples, 1]     The predictive variance.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict_marginalized(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predicts mean and variance marginalized over all instances.\n\n    Warning\n    -------\n    The input data must not include any features.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters]\n        Input data points.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, 1]\n        The predictive mean.\n    vars : np.ndarray [#samples, 1]\n        The predictive variance.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters (and no features) for this method, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._instance_features is None:\n        mean, var = self.predict(X)\n        assert var is not None\n\n        var[var &lt; self._var_threshold] = self._var_threshold\n        var[np.isnan(var)] = self._var_threshold\n\n        return mean, var\n    else:\n        n_instances = len(self._instance_features)\n\n        mean = np.zeros(X.shape[0])\n        var = np.zeros(X.shape[0])\n        for i, x in enumerate(X):\n            features = np.array(list(self._instance_features.values()))\n            x_tiled = np.tile(x, (n_instances, 1))\n            X_ = np.hstack((x_tiled, features))\n\n            means, vars = self.predict(X_)\n            assert vars is not None\n\n            # VAR[1/n (X_1 + ... + X_n)] =\n            # 1/n^2 * ( VAR(X_1) + ... + VAR(X_n))\n            # for independent X_1 ... X_n\n            var_x = np.sum(vars) / (len(vars) ** 2)\n            if var_x &lt; self._var_threshold:\n                var_x = self._var_threshold\n\n            var[i] = var_x\n            mean[i] = np.mean(means)\n\n        if len(mean.shape) == 1:\n            mean = mean.reshape((-1, 1))\n\n        if len(var.shape) == 1:\n            var = var.reshape((-1, 1))\n\n        return mean, var\n</code></pre>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/gaussian_process/abstract_gaussian_process/#smac.model.gaussian_process.abstract_gaussian_process.AbstractGaussianProcess.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/gaussian_process/","title":"Gaussian process","text":""},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process","title":"smac.model.gaussian_process.gaussian_process","text":""},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess","title":"GaussianProcess","text":"<pre><code>GaussianProcess(\n    configspace: ConfigurationSpace,\n    kernel: Kernel,\n    n_restarts: int = 10,\n    normalize_y: bool = True,\n    instance_features: (\n        dict[str, list[int | float]] | None\n    ) = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractGaussianProcess</code></p> <p>Implementation of Gaussian process model. The Gaussian process hyperparameters are obtained by optimizing the marginal log likelihood.</p> <p>This code is based on the implementation of RoBO: Klein, A. and Falkner, S. and Mansur, N. and Hutter, F. RoBO: A Flexible and Robust Bayesian Optimization Framework in Python In: NIPS 2017 Bayesian Optimization Workshop</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace kernel : Kernel     Kernel which is used for the Gaussian process. n_restarts : int, defaults to 10     Number of restarts for the Gaussian process hyperparameter optimization. normalize_y : bool, defaults to True     Zero mean unit variance normalization of the output values. instance_features : dict[str, list[int | float]] | None, defaults to None     Features (list of int or floats) of the instances (str). The features are incorporated into the X data,     on which the model is trained on. pca_components : float, defaults to 7     Number of components to keep when using PCA to reduce dimensionality of instance features. seed : int</p> Source code in <code>smac/model/gaussian_process/gaussian_process.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    kernel: Kernel,\n    n_restarts: int = 10,\n    normalize_y: bool = True,\n    instance_features: dict[str, list[int | float]] | None = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n):\n    super().__init__(\n        configspace=configspace,\n        seed=seed,\n        kernel=kernel,\n        instance_features=instance_features,\n        pca_components=pca_components,\n    )\n\n    self._normalize_y = normalize_y\n    self._n_restarts = n_restarts\n\n    # Internal variables\n    self._hypers = np.empty((0,))\n    self._is_trained = False\n    self._n_ll_evals = 0\n\n    self._set_has_conditions()\n</code></pre>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.predict_marginalized","title":"predict_marginalized","text":"<pre><code>predict_marginalized(X: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Predicts mean and variance marginalized over all instances.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.predict_marginalized--warning","title":"Warning","text":"<p>The input data must not include any features.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.predict_marginalized--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters]     Input data points.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.predict_marginalized--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, 1]     The predictive mean. vars : np.ndarray [#samples, 1]     The predictive variance.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict_marginalized(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predicts mean and variance marginalized over all instances.\n\n    Warning\n    -------\n    The input data must not include any features.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters]\n        Input data points.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, 1]\n        The predictive mean.\n    vars : np.ndarray [#samples, 1]\n        The predictive variance.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters (and no features) for this method, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._instance_features is None:\n        mean, var = self.predict(X)\n        assert var is not None\n\n        var[var &lt; self._var_threshold] = self._var_threshold\n        var[np.isnan(var)] = self._var_threshold\n\n        return mean, var\n    else:\n        n_instances = len(self._instance_features)\n\n        mean = np.zeros(X.shape[0])\n        var = np.zeros(X.shape[0])\n        for i, x in enumerate(X):\n            features = np.array(list(self._instance_features.values()))\n            x_tiled = np.tile(x, (n_instances, 1))\n            X_ = np.hstack((x_tiled, features))\n\n            means, vars = self.predict(X_)\n            assert vars is not None\n\n            # VAR[1/n (X_1 + ... + X_n)] =\n            # 1/n^2 * ( VAR(X_1) + ... + VAR(X_n))\n            # for independent X_1 ... X_n\n            var_x = np.sum(vars) / (len(vars) ** 2)\n            if var_x &lt; self._var_threshold:\n                var_x = self._var_threshold\n\n            var[i] = var_x\n            mean[i] = np.mean(means)\n\n        if len(mean.shape) == 1:\n            mean = mean.reshape((-1, 1))\n\n        if len(var.shape) == 1:\n            var = var.reshape((-1, 1))\n\n        return mean, var\n</code></pre>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.sample_functions","title":"sample_functions","text":"<pre><code>sample_functions(\n    X_test: ndarray, n_funcs: int = 1\n) -&gt; ndarray\n</code></pre> <p>Samples F function values from the current posterior at the N specified test points.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.sample_functions--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. n_funcs: int     Number of function values that are drawn at each test point.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.sample_functions--returns","title":"Returns","text":"<p>function_samples : np.ndarray     The F function values drawn at the N test points.</p> Source code in <code>smac/model/gaussian_process/gaussian_process.py</code> <pre><code>def sample_functions(self, X_test: np.ndarray, n_funcs: int = 1) -&gt; np.ndarray:\n    \"\"\"Samples F function values from the current posterior at the N specified test points.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    n_funcs: int\n        Number of function values that are drawn at each test point.\n\n    Returns\n    -------\n    function_samples : np.ndarray\n        The F function values drawn at the N test points.\n    \"\"\"\n    if not self._is_trained:\n        raise Exception(\"Model has to be trained first.\")\n\n    X_test = self._impute_inactive(X_test)\n    funcs = self._gp.sample_y(X_test, n_samples=n_funcs, random_state=self._rng)\n\n    if self._normalize_y:\n        funcs = self._untransform_y(funcs)\n\n    if len(funcs.shape) == 1:\n        return funcs[None, :]\n    else:\n        return funcs\n</code></pre>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/gaussian_process/gaussian_process/#smac.model.gaussian_process.gaussian_process.GaussianProcess.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/gpytorch_gaussian_process/","title":"Gpytorch gaussian process","text":""},{"location":"api/smac/model/gaussian_process/gpytorch_gaussian_process/#smac.model.gaussian_process.gpytorch_gaussian_process","title":"smac.model.gaussian_process.gpytorch_gaussian_process","text":""},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/","title":"Mcmc gaussian process","text":""},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process","title":"smac.model.gaussian_process.mcmc_gaussian_process","text":""},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess","title":"MCMCGaussianProcess","text":"<pre><code>MCMCGaussianProcess(\n    configspace: ConfigurationSpace,\n    kernel: Kernel,\n    n_mcmc_walkers: int = 20,\n    chain_length: int = 50,\n    burning_steps: int = 50,\n    mcmc_sampler: str = \"emcee\",\n    average_samples: bool = False,\n    normalize_y: bool = True,\n    instance_features: (\n        dict[str, list[int | float]] | None\n    ) = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractGaussianProcess</code></p> <p>Implementation of a Gaussian process model which out-integrates its hyperparameters by Markow-Chain-Monte-Carlo (MCMC). If you use this class make sure that you also use an integrated acquisition function to integrate over the GP's hyperparameter as proposed by Snoek et al.</p> <p>This code is based on the implementation of RoBO:</p> <p>Klein, A. and Falkner, S. and Mansur, N. and Hutter, F. RoBO: A Flexible and Robust Bayesian Optimization Framework in Python In: NIPS 2017 Bayesian Optimization Workshop</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess--parameters","title":"Parameters","text":"<p>configspace : ConfigurationSpace kernel : Kernel     Kernel which is used for the Gaussian process. n_mcmc_walkers : int, defaults to 20     The number of hyperparameter samples. This also determines the number of walker for MCMC sampling as each     walker will return one hyperparameter sample. chain_length : int, defaults to 50     The length of the MCMC chain. We start <code>n_mcmc_walkers</code> walker for <code>chain_length</code> steps, and we use the last     sample in the chain as a hyperparameter sample. burning_steps : int, defaults to 50     The number of burning steps before the actual MCMC sampling starts. mcmc_sampler : str, defaults to \"emcee\"     Choose a self-tuning MCMC sampler. Can be either <code>emcee</code> or <code>nuts</code>. normalize_y : bool, defaults to True     Zero mean unit variance normalization of the output values. instance_features : dict[str, list[int | float]] | None, defaults to None     Features (list of int or floats) of the instances (str). The features are incorporated into the X data,     on which the model is trained on. pca_components : float, defaults to 7     Number of components to keep when using PCA to reduce dimensionality of instance features. seed : int</p> Source code in <code>smac/model/gaussian_process/mcmc_gaussian_process.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    kernel: Kernel,\n    n_mcmc_walkers: int = 20,\n    chain_length: int = 50,\n    burning_steps: int = 50,\n    mcmc_sampler: str = \"emcee\",\n    average_samples: bool = False,\n    normalize_y: bool = True,\n    instance_features: dict[str, list[int | float]] | None = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n):\n    if mcmc_sampler not in [\"emcee\", \"nuts\"]:\n        raise ValueError(f\"MCMC Gaussian process does not support the sampler `{mcmc_sampler}`.\")\n\n    super().__init__(\n        configspace=configspace,\n        kernel=kernel,\n        instance_features=instance_features,\n        pca_components=pca_components,\n        seed=seed,\n    )\n\n    self._n_mcmc_walkers = n_mcmc_walkers\n    self._chain_length = chain_length\n    self._burning_steps = burning_steps\n    self._models: list[GaussianProcess] = []\n    self._normalize_y = normalize_y\n    self._mcmc_sampler = mcmc_sampler\n    self._average_samples = average_samples\n    self._set_has_conditions()\n\n    # Internal statistics\n    self._n_ll_evals = 0\n    self._burned = False\n    self._is_trained = False\n    self._samples: np.ndarray | None = None\n</code></pre>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.models","title":"models  <code>property</code>","text":"<pre><code>models: list[GaussianProcess]\n</code></pre> <p>Returns the internally used gaussian processes.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.predict_marginalized","title":"predict_marginalized","text":"<pre><code>predict_marginalized(X: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Predicts mean and variance marginalized over all instances.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.predict_marginalized--warning","title":"Warning","text":"<p>The input data must not include any features.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.predict_marginalized--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters]     Input data points.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.predict_marginalized--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, 1]     The predictive mean. vars : np.ndarray [#samples, 1]     The predictive variance.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict_marginalized(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predicts mean and variance marginalized over all instances.\n\n    Warning\n    -------\n    The input data must not include any features.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters]\n        Input data points.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, 1]\n        The predictive mean.\n    vars : np.ndarray [#samples, 1]\n        The predictive variance.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters (and no features) for this method, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._instance_features is None:\n        mean, var = self.predict(X)\n        assert var is not None\n\n        var[var &lt; self._var_threshold] = self._var_threshold\n        var[np.isnan(var)] = self._var_threshold\n\n        return mean, var\n    else:\n        n_instances = len(self._instance_features)\n\n        mean = np.zeros(X.shape[0])\n        var = np.zeros(X.shape[0])\n        for i, x in enumerate(X):\n            features = np.array(list(self._instance_features.values()))\n            x_tiled = np.tile(x, (n_instances, 1))\n            X_ = np.hstack((x_tiled, features))\n\n            means, vars = self.predict(X_)\n            assert vars is not None\n\n            # VAR[1/n (X_1 + ... + X_n)] =\n            # 1/n^2 * ( VAR(X_1) + ... + VAR(X_n))\n            # for independent X_1 ... X_n\n            var_x = np.sum(vars) / (len(vars) ** 2)\n            if var_x &lt; self._var_threshold:\n                var_x = self._var_threshold\n\n            var[i] = var_x\n            mean[i] = np.mean(means)\n\n        if len(mean.shape) == 1:\n            mean = mean.reshape((-1, 1))\n\n        if len(var.shape) == 1:\n            var = var.reshape((-1, 1))\n\n        return mean, var\n</code></pre>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/gaussian_process/mcmc_gaussian_process/#smac.model.gaussian_process.mcmc_gaussian_process.MCMCGaussianProcess.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/","title":"Base kernels","text":""},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels","title":"smac.model.gaussian_process.kernels.base_kernels","text":""},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel","title":"AbstractKernel","text":"<pre><code>AbstractKernel(\n    *,\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>This is a mixin for a kernel to override functions of the kernel. Because it overrides functions of the kernel, it needs to be placed first in the inheritance hierarchy. For this reason it is not possible to subclass the Mixin from the kernel class because this will prevent it from being instantiatable. Therefore, mypy won't know about anything related to the superclass and some type:ignore statements has to be added when accessing a member that is declared in the superclass such as <code>self.has_conditions</code>, <code>self._call</code>, <code>super().get_params</code>, etc.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel--parameters","title":"Parameters","text":"<p>operate_on : np.ndarray, defaults to None     On which numpy array should be operated on. has_conditions : bool, defaults to False     Whether the kernel has conditions. prior : AbstractPrior, defaults to None     Which prior the kernel is using.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel--attributes","title":"Attributes","text":"<p>operate_on : np.ndarray, defaults to None     On which numpy array should be operated on. has_conditions : bool, defaults to False     Whether the kernel has conditions. Might be changed by the gaussian process. prior : AbstractPrior, defaults to None     Which prior the kernel is using. Primarily used by sklearn.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __init__(\n    self,\n    *,\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    self.operate_on = operate_on\n    self.has_conditions = has_conditions\n    self.prior = prior\n    self._set_active_dims(operate_on)\n\n    # Since this class is a mixin, we just pass all the other parameters to the next class.\n    super().__init__(**kwargs)\n\n    # Get variables from next class:\n    # We make it explicit here to make sure the next class really has this attributes.\n    self._hyperparameters: list[kernels.Hyperparameter] = super().hyperparameters  # type: ignore\n    self._n_dims: int = super().n_dims  # type: ignore\n    self._len_active: int | None\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object. This method calls the <code>get_params</code> method to collect the parameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Call the kernel function. Internally, <code>self._call</code> is called, which must be specified by a subclass.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Call the kernel function. Internally, `self._call` is called, which must be specified by a subclass.\"\"\"\n    if active is None and self.has_conditions:\n        if self.operate_on is None:\n            active = get_conditional_hyperparameters(X, Y)\n        else:\n            if Y is None:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], None)\n            else:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], Y[:, self.operate_on])\n\n    if self.operate_on is None:\n        rval = self._call(X, Y, eval_gradient, active)\n    else:\n        if self._len_active is None:\n            raise RuntimeError(\"The internal variable `_len_active` is not set.\")\n\n        if Y is None:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=None,\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n        else:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=Y[:, self.operate_on].reshape([-1, self._len_active]),\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n            Y = Y[:, self.operate_on].reshape((-1, self._len_active))\n\n    return rval\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.AbstractKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel","title":"ConstantKernel","text":"<pre><code>ConstantKernel(\n    constant_value: float = 1.0,\n    constant_value_bounds: tuple[float, float] = (\n        1e-05,\n        100000.0,\n    ),\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractKernel</code>, <code>ConstantKernel</code></p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __init__(\n    self,\n    constant_value: float = 1.0,\n    constant_value_bounds: tuple[float, float] = (1e-5, 1e5),\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n) -&gt; None:\n    super().__init__(\n        operate_on=operate_on,\n        has_conditions=has_conditions,\n        prior=prior,\n        constant_value=constant_value,\n        constant_value_bounds=constant_value_bounds,\n    )\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object. This method calls the <code>get_params</code> method to collect the parameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Return the kernel k(X, Y) and optionally its gradient.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.__call__--parameters","title":"Parameters","text":"<p>X : np.ndarray, shape (n_samples_X, n_features)     Left argument of the returned kernel k(X, Y).</p> np.ndarray, shape (n_samples_Y, n_features), (optional, default=None) <p>Right argument of the returned kernel k(X, Y). If None, k(X, X) is evaluated instead.</p> bool (optional, default=False) <p>Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.</p> np.ndarray (n_samples_X, n_features) (optional) <p>Boolean array specifying which hyperparameters are active.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.__call__--returns","title":"Returns","text":"<p>K : np.ndarray, shape (n_samples_X, n_samples_Y)     Kernel k(X, Y).</p> np.ndarray (opt.), shape (n_samples_X, n_samples_X, n_dims) <p>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples_X, n_features)\n        Left argument of the returned kernel k(X, Y).\n\n    Y : np.ndarray, shape (n_samples_Y, n_features), (optional, default=None)\n        Right argument of the returned kernel k(X, Y). If None, k(X, X)\n        is evaluated instead.\n\n    eval_gradient : bool (optional, default=False)\n        Determines whether the gradient with respect to the kernel\n        hyperparameter is determined. Only supported when Y is None.\n\n    active : np.ndarray (n_samples_X, n_features) (optional)\n        Boolean array specifying which hyperparameters are active.\n\n    Returns\n    -------\n    K : np.ndarray, shape (n_samples_X, n_samples_Y)\n        Kernel k(X, Y).\n\n    K_gradient : np.ndarray (opt.), shape (n_samples_X, n_samples_X, n_dims)\n        The gradient of the kernel k(X, X) with respect to the\n        hyperparameter of the kernel. Only returned when eval_gradient\n        is True.\n    \"\"\"\n    X = np.atleast_2d(X)\n    if Y is None:\n        Y = X\n    elif eval_gradient:\n        raise ValueError(\"Gradient can only be evaluated when Y is None.\")\n\n    K = np.full(\n        (X.shape[0], Y.shape[0]),\n        self.constant_value,\n        dtype=np.array(self.constant_value).dtype,\n    )\n    if eval_gradient:\n        if not self.hyperparameter_constant_value.fixed:\n            return (\n                K,\n                np.full(\n                    (X.shape[0], X.shape[0], 1),\n                    self.constant_value,\n                    dtype=np.array(self.constant_value).dtype,\n                ),\n            )\n        else:\n            return K, np.empty((X.shape[0], X.shape[0], 0))\n    else:\n        return K\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ConstantKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel","title":"ProductKernel","text":"<pre><code>ProductKernel(\n    k1: Kernel,\n    k2: Kernel,\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n)\n</code></pre> <p>               Bases: <code>AbstractKernel</code>, <code>Product</code></p> <p>Product kernel implementation.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __init__(\n    self,\n    k1: kernels.Kernel,\n    k2: kernels.Kernel,\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n) -&gt; None:\n    super().__init__(\n        operate_on=operate_on,\n        has_conditions=has_conditions,\n        k1=k1,\n        k2=k2,\n    )\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object. This method calls the <code>get_params</code> method to collect the parameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Return the kernel k(X, Y) and optionally its gradient.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.__call__--parameters","title":"Parameters","text":"<p>X : np.ndarray, shape (n_samples_X, n_features)     Left argument of the returned kernel k(X, Y).</p> np.ndarray, shape (n_samples_Y, n_features), (optional, default=None) <p>Right argument of the returned kernel k(X, Y). If None, k(X, X) is evaluated instead.</p> bool (optional, default=False) <p>Determines whether the gradient with respect to the kernel hyperparameter is determined.</p> np.ndarray (n_samples_X, n_features) (optional) <p>Boolean array specifying which hyperparameters are active.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.__call__--returns","title":"Returns","text":"<p>K : np.ndarray, shape (n_samples_X, n_samples_Y)     Kernel k(X, Y).</p> np.ndarray (opt.), shape (n_samples_X, n_samples_X, n_dims) <p>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples_X, n_features)\n        Left argument of the returned kernel k(X, Y).\n\n    Y : np.ndarray, shape (n_samples_Y, n_features), (optional, default=None)\n        Right argument of the returned kernel k(X, Y). If None, k(X, X)\n        is evaluated instead.\n\n    eval_gradient : bool (optional, default=False)\n        Determines whether the gradient with respect to the kernel\n        hyperparameter is determined.\n\n    active : np.ndarray (n_samples_X, n_features) (optional)\n        Boolean array specifying which hyperparameters are active.\n\n    Returns\n    -------\n    K : np.ndarray, shape (n_samples_X, n_samples_Y)\n        Kernel k(X, Y).\n\n    K_gradient : np.ndarray (opt.), shape (n_samples_X, n_samples_X, n_dims)\n        The gradient of the kernel k(X, X) with respect to the\n        hyperparameter of the kernel. Only returned when eval_gradient\n        is True.\n    \"\"\"\n    if eval_gradient:\n        K1, K1_gradient = self.k1(X, Y, eval_gradient=True, active=active)\n        K2, K2_gradient = self.k2(X, Y, eval_gradient=True, active=active)\n\n        return K1 * K2, np.dstack((K1_gradient * K2[:, :, np.newaxis], K2_gradient * K1[:, :, np.newaxis]))\n    else:\n        return self.k1(X, Y, active=active) * self.k2(X, Y, active=active)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.ProductKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel","title":"SumKernel","text":"<pre><code>SumKernel(\n    k1: Kernel,\n    k2: Kernel,\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n)\n</code></pre> <p>               Bases: <code>AbstractKernel</code>, <code>Sum</code></p> <p>Sum kernel implementation.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __init__(\n    self,\n    k1: kernels.Kernel,\n    k2: kernels.Kernel,\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n) -&gt; None:\n    super().__init__(\n        operate_on=operate_on,\n        has_conditions=has_conditions,\n        k1=k1,\n        k2=k2,\n    )\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object. This method calls the <code>get_params</code> method to collect the parameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Return the kernel k(X, Y) and optionally its gradient.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.__call__--parameters","title":"Parameters","text":"<p>X : np.ndarray, shape (n_samples_X, n_features)     Left argument of the returned kernel k(X, Y).</p> np.ndarray, shape (n_samples_Y, n_features), (optional, default=None) <p>Right argument of the returned kernel k(X, Y). If None, k(X, X) is evaluated instead.</p> bool (optional, default=False) <p>Determines whether the gradient with respect to the kernel hyperparameter is determined.</p> np.ndarray (n_samples_X, n_features) (optional) <p>Boolean array specifying which hyperparameters are active.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.__call__--returns","title":"Returns","text":"<p>K : np.ndarray, shape (n_samples_X, n_samples_Y)     Kernel k(X, Y).</p> np.ndarray (opt.), shape (n_samples_X, n_samples_X, n_dims) <p>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples_X, n_features)\n        Left argument of the returned kernel k(X, Y).\n\n    Y : np.ndarray, shape (n_samples_Y, n_features), (optional, default=None)\n        Right argument of the returned kernel k(X, Y). If None, k(X, X)\n        is evaluated instead.\n\n    eval_gradient : bool (optional, default=False)\n        Determines whether the gradient with respect to the kernel\n        hyperparameter is determined.\n\n    active : np.ndarray (n_samples_X, n_features) (optional)\n        Boolean array specifying which hyperparameters are active.\n\n    Returns\n    -------\n    K : np.ndarray, shape (n_samples_X, n_samples_Y)\n        Kernel k(X, Y).\n\n    K_gradient : np.ndarray (opt.), shape (n_samples_X, n_samples_X, n_dims)\n        The gradient of the kernel k(X, X) with respect to the\n        hyperparameter of the kernel. Only returned when eval_gradient\n        is True.\n    \"\"\"\n    if eval_gradient:\n        K1, K1_gradient = self.k1(X, Y, eval_gradient=True, active=active)\n        K2, K2_gradient = self.k2(X, Y, eval_gradient=True, active=active)\n\n        return K1 + K2, np.dstack((K1_gradient, K2_gradient))\n    else:\n        return self.k1(X, Y, active=active) + self.k2(X, Y, active=active)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/base_kernels/#smac.model.gaussian_process.kernels.base_kernels.SumKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/","title":"Hamming kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel","title":"smac.model.gaussian_process.kernels.hamming_kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel","title":"HammingKernel","text":"<pre><code>HammingKernel(\n    length_scale: float | tuple[float, ...] | ndarray = 1.0,\n    length_scale_bounds: (\n        tuple[float, float]\n        | list[tuple[float, float]]\n        | ndarray\n    ) = (1e-05, 100000.0),\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractKernel</code>, <code>StationaryKernelMixin</code>, <code>NormalizedKernelMixin</code>, <code>Kernel</code></p> <p>Hamming kernel implementation.</p> Source code in <code>smac/model/gaussian_process/kernels/hamming_kernel.py</code> <pre><code>def __init__(\n    self,\n    length_scale: float | tuple[float, ...] | np.ndarray = 1.0,\n    length_scale_bounds: tuple[float, float] | list[tuple[float, float]] | np.ndarray = (1e-5, 1e5),\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n) -&gt; None:\n    self.length_scale = length_scale\n    self.length_scale_bounds = length_scale_bounds\n\n    super().__init__(\n        operate_on=operate_on,\n        has_conditions=has_conditions,\n        prior=prior,\n    )\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel.hyperparameter_length_scale","title":"hyperparameter_length_scale  <code>property</code>","text":"<pre><code>hyperparameter_length_scale: Hyperparameter\n</code></pre> <p>Hyperparameter of the length scale.</p>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Call the kernel function. Internally, <code>self._call</code> is called, which must be specified by a subclass.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Call the kernel function. Internally, `self._call` is called, which must be specified by a subclass.\"\"\"\n    if active is None and self.has_conditions:\n        if self.operate_on is None:\n            active = get_conditional_hyperparameters(X, Y)\n        else:\n            if Y is None:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], None)\n            else:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], Y[:, self.operate_on])\n\n    if self.operate_on is None:\n        rval = self._call(X, Y, eval_gradient, active)\n    else:\n        if self._len_active is None:\n            raise RuntimeError(\"The internal variable `_len_active` is not set.\")\n\n        if Y is None:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=None,\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n        else:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=Y[:, self.operate_on].reshape([-1, self._len_active]),\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n            Y = Y[:, self.operate_on].reshape((-1, self._len_active))\n\n    return rval\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/hamming_kernel/#smac.model.gaussian_process.kernels.hamming_kernel.HammingKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/","title":"Matern kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel","title":"smac.model.gaussian_process.kernels.matern_kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel","title":"MaternKernel","text":"<pre><code>MaternKernel(\n    length_scale: float | tuple[float, ...] | ndarray = 1.0,\n    length_scale_bounds: (\n        tuple[float, float]\n        | list[tuple[float, float]]\n        | ndarray\n    ) = (1e-05, 100000.0),\n    nu: float = 1.5,\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractKernel</code>, <code>Matern</code></p> <p>Matern kernel implementation.</p> Source code in <code>smac/model/gaussian_process/kernels/matern_kernel.py</code> <pre><code>def __init__(\n    self,\n    length_scale: float | tuple[float, ...] | np.ndarray = 1.0,\n    length_scale_bounds: tuple[float, float] | list[tuple[float, float]] | np.ndarray = (1e-5, 1e5),\n    nu: float = 1.5,\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n) -&gt; None:\n    super().__init__(\n        operate_on=operate_on,\n        has_conditions=has_conditions,\n        prior=prior,\n        length_scale=length_scale,\n        length_scale_bounds=length_scale_bounds,\n        nu=nu,\n    )\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object. This method calls the <code>get_params</code> method to collect the parameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Call the kernel function. Internally, <code>self._call</code> is called, which must be specified by a subclass.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Call the kernel function. Internally, `self._call` is called, which must be specified by a subclass.\"\"\"\n    if active is None and self.has_conditions:\n        if self.operate_on is None:\n            active = get_conditional_hyperparameters(X, Y)\n        else:\n            if Y is None:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], None)\n            else:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], Y[:, self.operate_on])\n\n    if self.operate_on is None:\n        rval = self._call(X, Y, eval_gradient, active)\n    else:\n        if self._len_active is None:\n            raise RuntimeError(\"The internal variable `_len_active` is not set.\")\n\n        if Y is None:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=None,\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n        else:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=Y[:, self.operate_on].reshape([-1, self._len_active]),\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n            Y = Y[:, self.operate_on].reshape((-1, self._len_active))\n\n    return rval\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/matern_kernel/#smac.model.gaussian_process.kernels.matern_kernel.MaternKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/","title":"Rbf kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel","title":"smac.model.gaussian_process.kernels.rbf_kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel","title":"RBFKernel","text":"<pre><code>RBFKernel(\n    length_scale: float | tuple[float, ...] | ndarray = 1.0,\n    length_scale_bounds: (\n        tuple[float, float]\n        | list[tuple[float, float]]\n        | ndarray\n    ) = (1e-05, 100000.0),\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractKernel</code>, <code>RBF</code></p> <p>RBF kernel implementation.</p> Source code in <code>smac/model/gaussian_process/kernels/rbf_kernel.py</code> <pre><code>def __init__(\n    self,\n    length_scale: float | tuple[float, ...] | np.ndarray = 1.0,\n    length_scale_bounds: tuple[float, float] | list[tuple[float, float]] | np.ndarray = (1e-5, 1e5),\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n) -&gt; None:\n    super().__init__(\n        operate_on=operate_on,\n        has_conditions=has_conditions,\n        prior=prior,\n        length_scale=length_scale,\n        length_scale_bounds=length_scale_bounds,\n    )\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object. This method calls the <code>get_params</code> method to collect the parameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Call the kernel function. Internally, <code>self._call</code> is called, which must be specified by a subclass.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Call the kernel function. Internally, `self._call` is called, which must be specified by a subclass.\"\"\"\n    if active is None and self.has_conditions:\n        if self.operate_on is None:\n            active = get_conditional_hyperparameters(X, Y)\n        else:\n            if Y is None:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], None)\n            else:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], Y[:, self.operate_on])\n\n    if self.operate_on is None:\n        rval = self._call(X, Y, eval_gradient, active)\n    else:\n        if self._len_active is None:\n            raise RuntimeError(\"The internal variable `_len_active` is not set.\")\n\n        if Y is None:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=None,\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n        else:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=Y[:, self.operate_on].reshape([-1, self._len_active]),\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n            Y = Y[:, self.operate_on].reshape((-1, self._len_active))\n\n    return rval\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/rbf_kernel/#smac.model.gaussian_process.kernels.rbf_kernel.RBFKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/","title":"White kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel","title":"smac.model.gaussian_process.kernels.white_kernel","text":""},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel","title":"WhiteKernel","text":"<pre><code>WhiteKernel(\n    noise_level: float | tuple[float, ...] = 1.0,\n    noise_level_bounds: (\n        tuple[float, float] | list[tuple[float, float]]\n    ) = (1e-05, 100000.0),\n    operate_on: ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractKernel</code>, <code>WhiteKernel</code></p> <p>White kernel implementation.</p> Source code in <code>smac/model/gaussian_process/kernels/white_kernel.py</code> <pre><code>def __init__(\n    self,\n    noise_level: float | tuple[float, ...] = 1.0,\n    noise_level_bounds: tuple[float, float] | list[tuple[float, float]] = (1e-5, 1e5),\n    operate_on: np.ndarray | None = None,\n    has_conditions: bool = False,\n    prior: AbstractPrior | None = None,\n) -&gt; None:\n    super().__init__(\n        operate_on=operate_on,\n        has_conditions=has_conditions,\n        prior=prior,\n        noise_level=noise_level,\n        noise_level_bounds=noise_level_bounds,\n    )\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel.hyperparameters","title":"hyperparameters  <code>property</code>","text":"<pre><code>hyperparameters: list[Hyperparameter]\n</code></pre> <p>Returns a list of all hyperparameter specifications.</p>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object. This method calls the <code>get_params</code> method to collect the parameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel.n_dims","title":"n_dims  <code>property</code>","text":"<pre><code>n_dims: int\n</code></pre> <p>Returns the number of non-fixed hyperparameters of the kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel.__call__","title":"__call__","text":"<pre><code>__call__(\n    X: ndarray,\n    Y: ndarray | None = None,\n    eval_gradient: bool = False,\n    active: ndarray | None = None,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Call the kernel function. Internally, <code>self._call</code> is called, which must be specified by a subclass.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def __call__(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray | None = None,\n    eval_gradient: bool = False,\n    active: np.ndarray | None = None,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"Call the kernel function. Internally, `self._call` is called, which must be specified by a subclass.\"\"\"\n    if active is None and self.has_conditions:\n        if self.operate_on is None:\n            active = get_conditional_hyperparameters(X, Y)\n        else:\n            if Y is None:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], None)\n            else:\n                active = get_conditional_hyperparameters(X[:, self.operate_on], Y[:, self.operate_on])\n\n    if self.operate_on is None:\n        rval = self._call(X, Y, eval_gradient, active)\n    else:\n        if self._len_active is None:\n            raise RuntimeError(\"The internal variable `_len_active` is not set.\")\n\n        if Y is None:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=None,\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n        else:\n            rval = self._call(\n                X=X[:, self.operate_on].reshape([-1, self._len_active]),\n                Y=Y[:, self.operate_on].reshape([-1, self._len_active]),\n                eval_gradient=eval_gradient,\n                active=active,\n            )\n            X = X[:, self.operate_on].reshape((-1, self._len_active))\n            Y = Y[:, self.operate_on].reshape((-1, self._len_active))\n\n    return rval\n</code></pre>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get parameters of this kernel.</p>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel.get_params--parameters","title":"Parameters","text":"<p>deep : bool, defaults to True     If True, will return the parameters for this estimator and     contained subobjects that are estimators.</p>"},{"location":"api/smac/model/gaussian_process/kernels/white_kernel/#smac.model.gaussian_process.kernels.white_kernel.WhiteKernel.get_params--returns","title":"Returns","text":"<p>params : dict[str, Any]     Parameter names mapped to their values.</p> Source code in <code>smac/model/gaussian_process/kernels/base_kernels.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, defaults to True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict[str, Any]\n        Parameter names mapped to their values.\n    \"\"\"\n    params = {}\n\n    # ignore[misc] looks like it catches all kinds of errors, but misc is actually a category from mypy:\n    # https://mypy.readthedocs.io/en/latest/error_code_list.html#miscellaneous-checks-misc\n    tmp = super().get_params(deep)  # type: ignore[misc] # noqa F821\n    args = list(tmp.keys())\n\n    # Sum and Product do not clone the 'has_conditions' attribute by default. Instead of changing their\n    # get_params() method, we simply add the attribute here!\n    if \"has_conditions\" not in args:\n        args.append(\"has_conditions\")\n\n    for arg in args:\n        params[arg] = getattr(self, arg, None)\n\n    return params\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/","title":"Abstract prior","text":""},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior","title":"smac.model.gaussian_process.priors.abstract_prior","text":""},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior","title":"AbstractPrior","text":"<pre><code>AbstractPrior(seed: int = 0)\n</code></pre> <p>Abstract base class to define the interface for priors of Gaussian process hyperparameters.</p> <p>This class is adapted from RoBO:</p> <p>Klein, A. and Falkner, S. and Mansur, N. and Hutter, F. RoBO: A Flexible and Robust Bayesian Optimization Framework in Python In: NIPS 2017 Bayesian Optimization Workshop</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior--note","title":"Note","text":"<p>Whenever lnprob or the gradient is computed for a scalar input, we use math. rather than np..</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior--parameters","title":"Parameters","text":"<p>seed : int, defaults to 0</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def __init__(self, seed: int = 0):\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_gradient","title":"get_gradient","text":"<pre><code>get_gradient(theta: float) -&gt; float\n</code></pre> <p>Computes the gradient of the prior with respect to theta. Internally, his method calls <code>self._get_gradient</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_gradient--warning","title":"Warning","text":"<p>Theta must be on the original scale.</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_gradient--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_gradient--returns","title":"Returns","text":"<p>gradient : float     The gradient of the prior at theta.</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_gradient(self, theta: float) -&gt; float:\n    \"\"\"Computes the gradient of the prior with respect to theta. Internally, his method calls `self._get_gradient`.\n\n    Warning\n    -------\n    Theta must be on the original scale.\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space\n\n    Returns\n    -------\n    gradient : float\n        The gradient of the prior at theta.\n    \"\"\"\n    return self._get_gradient(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_log_probability","title":"get_log_probability","text":"<pre><code>get_log_probability(theta: float) -&gt; float\n</code></pre> <p>Returns the log probability of theta. This method exponentiates theta and calls <code>self._get_log_probability</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_log_probability--warning","title":"Warning","text":"<p>Theta must be on a log scale!</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_log_probability--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space.</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.get_log_probability--returns","title":"Returns","text":"<p>float     The log probability of theta</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_log_probability(self, theta: float) -&gt; float:\n    \"\"\"Returns the log probability of theta. This method exponentiates theta and calls `self._get_log_probability`.\n\n    Warning\n    -------\n    Theta must be on a log scale!\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space.\n\n    Returns\n    -------\n    float\n        The log probability of theta\n    \"\"\"\n    return self._get_log_probability(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.sample_from_prior","title":"sample_from_prior","text":"<pre><code>sample_from_prior(n_samples: int) -&gt; ndarray\n</code></pre> <p>Returns <code>n_samples</code> from the prior. All samples are on a log scale. This method calls <code>self._sample_from_prior</code> and applies a log transformation to the obtained values.</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.sample_from_prior--parameters","title":"Parameters","text":"<p>n_samples : int     The number of samples that will be drawn.</p>"},{"location":"api/smac/model/gaussian_process/priors/abstract_prior/#smac.model.gaussian_process.priors.abstract_prior.AbstractPrior.sample_from_prior--returns","title":"Returns","text":"<p>samples : np.ndarray</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def sample_from_prior(self, n_samples: int) -&gt; np.ndarray:\n    \"\"\"Returns `n_samples` from the prior. All samples are on a log scale. This method calls\n    `self._sample_from_prior` and applies a log transformation to the obtained values.\n\n    Parameters\n    ----------\n    n_samples : int\n        The number of samples that will be drawn.\n\n    Returns\n    -------\n    samples : np.ndarray\n    \"\"\"\n    if np.ndim(n_samples) != 0:\n        raise ValueError(\"argument n_samples needs to be a scalar (is %s)\" % n_samples)\n\n    if n_samples &lt;= 0:\n        raise ValueError(\"argument n_samples needs to be positive (is %d)\" % n_samples)\n\n    sample = np.log(self._sample_from_prior(n_samples=n_samples))\n\n    if np.any(~np.isfinite(sample)):\n        raise ValueError(\"Sample %s from prior %s contains infinite values!\" % (sample, self))\n\n    return sample\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/","title":"Gamma prior","text":""},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior","title":"smac.model.gaussian_process.priors.gamma_prior","text":""},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior","title":"GammaPrior","text":"<pre><code>GammaPrior(\n    a: float, scale: float, loc: float, seed: int = 0\n)\n</code></pre> <p>               Bases: <code>AbstractPrior</code></p> <p>Implementation of gamma prior.</p> <p>f(x) = (x-loc)(a-1) * e(-(x-loc)) * (1/scale)**a / gamma(a)</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior--parameters","title":"Parameters","text":"<p>a : float     The shape parameter. Must be greater than 0. scale : float     The scale parameter (1/scale corresponds to parameter p in canonical form). Must be greather than 0. loc : float     Mean parameter for the distribution. seed : int, defaults to 0</p> Source code in <code>smac/model/gaussian_process/priors/gamma_prior.py</code> <pre><code>def __init__(\n    self,\n    a: float,\n    scale: float,\n    loc: float,\n    seed: int = 0,\n):\n    super().__init__(seed=seed)\n    assert a &gt; 0\n    assert scale &gt; 0\n\n    self._a = a\n    self._loc = loc\n    self._scale = scale\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_gradient","title":"get_gradient","text":"<pre><code>get_gradient(theta: float) -&gt; float\n</code></pre> <p>Computes the gradient of the prior with respect to theta. Internally, his method calls <code>self._get_gradient</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_gradient--warning","title":"Warning","text":"<p>Theta must be on the original scale.</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_gradient--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_gradient--returns","title":"Returns","text":"<p>gradient : float     The gradient of the prior at theta.</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_gradient(self, theta: float) -&gt; float:\n    \"\"\"Computes the gradient of the prior with respect to theta. Internally, his method calls `self._get_gradient`.\n\n    Warning\n    -------\n    Theta must be on the original scale.\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space\n\n    Returns\n    -------\n    gradient : float\n        The gradient of the prior at theta.\n    \"\"\"\n    return self._get_gradient(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_log_probability","title":"get_log_probability","text":"<pre><code>get_log_probability(theta: float) -&gt; float\n</code></pre> <p>Returns the log probability of theta. This method exponentiates theta and calls <code>self._get_log_probability</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_log_probability--warning","title":"Warning","text":"<p>Theta must be on a log scale!</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_log_probability--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space.</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.get_log_probability--returns","title":"Returns","text":"<p>float     The log probability of theta</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_log_probability(self, theta: float) -&gt; float:\n    \"\"\"Returns the log probability of theta. This method exponentiates theta and calls `self._get_log_probability`.\n\n    Warning\n    -------\n    Theta must be on a log scale!\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space.\n\n    Returns\n    -------\n    float\n        The log probability of theta\n    \"\"\"\n    return self._get_log_probability(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.sample_from_prior","title":"sample_from_prior","text":"<pre><code>sample_from_prior(n_samples: int) -&gt; ndarray\n</code></pre> <p>Returns <code>n_samples</code> from the prior. All samples are on a log scale. This method calls <code>self._sample_from_prior</code> and applies a log transformation to the obtained values.</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.sample_from_prior--parameters","title":"Parameters","text":"<p>n_samples : int     The number of samples that will be drawn.</p>"},{"location":"api/smac/model/gaussian_process/priors/gamma_prior/#smac.model.gaussian_process.priors.gamma_prior.GammaPrior.sample_from_prior--returns","title":"Returns","text":"<p>samples : np.ndarray</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def sample_from_prior(self, n_samples: int) -&gt; np.ndarray:\n    \"\"\"Returns `n_samples` from the prior. All samples are on a log scale. This method calls\n    `self._sample_from_prior` and applies a log transformation to the obtained values.\n\n    Parameters\n    ----------\n    n_samples : int\n        The number of samples that will be drawn.\n\n    Returns\n    -------\n    samples : np.ndarray\n    \"\"\"\n    if np.ndim(n_samples) != 0:\n        raise ValueError(\"argument n_samples needs to be a scalar (is %s)\" % n_samples)\n\n    if n_samples &lt;= 0:\n        raise ValueError(\"argument n_samples needs to be positive (is %d)\" % n_samples)\n\n    sample = np.log(self._sample_from_prior(n_samples=n_samples))\n\n    if np.any(~np.isfinite(sample)):\n        raise ValueError(\"Sample %s from prior %s contains infinite values!\" % (sample, self))\n\n    return sample\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/","title":"Horseshoe prior","text":""},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior","title":"smac.model.gaussian_process.priors.horseshoe_prior","text":""},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior","title":"HorseshoePrior","text":"<pre><code>HorseshoePrior(scale: float, seed: int = 0)\n</code></pre> <p>               Bases: <code>AbstractPrior</code></p> <p>Horseshoe Prior as it is used in spearmint.</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior--parameters","title":"Parameters","text":"<p>scale: float     Scaling parameter. seed : int, defaults to 0</p> Source code in <code>smac/model/gaussian_process/priors/horseshoe_prior.py</code> <pre><code>def __init__(self, scale: float, seed: int = 0):\n    super().__init__(seed=seed)\n    self._scale = scale\n    self._scale_square = scale**2\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_gradient","title":"get_gradient","text":"<pre><code>get_gradient(theta: float) -&gt; float\n</code></pre> <p>Computes the gradient of the prior with respect to theta. Internally, his method calls <code>self._get_gradient</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_gradient--warning","title":"Warning","text":"<p>Theta must be on the original scale.</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_gradient--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_gradient--returns","title":"Returns","text":"<p>gradient : float     The gradient of the prior at theta.</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_gradient(self, theta: float) -&gt; float:\n    \"\"\"Computes the gradient of the prior with respect to theta. Internally, his method calls `self._get_gradient`.\n\n    Warning\n    -------\n    Theta must be on the original scale.\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space\n\n    Returns\n    -------\n    gradient : float\n        The gradient of the prior at theta.\n    \"\"\"\n    return self._get_gradient(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_log_probability","title":"get_log_probability","text":"<pre><code>get_log_probability(theta: float) -&gt; float\n</code></pre> <p>Returns the log probability of theta. This method exponentiates theta and calls <code>self._get_log_probability</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_log_probability--warning","title":"Warning","text":"<p>Theta must be on a log scale!</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_log_probability--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space.</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.get_log_probability--returns","title":"Returns","text":"<p>float     The log probability of theta</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_log_probability(self, theta: float) -&gt; float:\n    \"\"\"Returns the log probability of theta. This method exponentiates theta and calls `self._get_log_probability`.\n\n    Warning\n    -------\n    Theta must be on a log scale!\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space.\n\n    Returns\n    -------\n    float\n        The log probability of theta\n    \"\"\"\n    return self._get_log_probability(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.sample_from_prior","title":"sample_from_prior","text":"<pre><code>sample_from_prior(n_samples: int) -&gt; ndarray\n</code></pre> <p>Returns <code>n_samples</code> from the prior. All samples are on a log scale. This method calls <code>self._sample_from_prior</code> and applies a log transformation to the obtained values.</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.sample_from_prior--parameters","title":"Parameters","text":"<p>n_samples : int     The number of samples that will be drawn.</p>"},{"location":"api/smac/model/gaussian_process/priors/horseshoe_prior/#smac.model.gaussian_process.priors.horseshoe_prior.HorseshoePrior.sample_from_prior--returns","title":"Returns","text":"<p>samples : np.ndarray</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def sample_from_prior(self, n_samples: int) -&gt; np.ndarray:\n    \"\"\"Returns `n_samples` from the prior. All samples are on a log scale. This method calls\n    `self._sample_from_prior` and applies a log transformation to the obtained values.\n\n    Parameters\n    ----------\n    n_samples : int\n        The number of samples that will be drawn.\n\n    Returns\n    -------\n    samples : np.ndarray\n    \"\"\"\n    if np.ndim(n_samples) != 0:\n        raise ValueError(\"argument n_samples needs to be a scalar (is %s)\" % n_samples)\n\n    if n_samples &lt;= 0:\n        raise ValueError(\"argument n_samples needs to be positive (is %d)\" % n_samples)\n\n    sample = np.log(self._sample_from_prior(n_samples=n_samples))\n\n    if np.any(~np.isfinite(sample)):\n        raise ValueError(\"Sample %s from prior %s contains infinite values!\" % (sample, self))\n\n    return sample\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/","title":"Log normal prior","text":""},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior","title":"smac.model.gaussian_process.priors.log_normal_prior","text":""},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior","title":"LogNormalPrior","text":"<pre><code>LogNormalPrior(\n    sigma: float, mean: float = 0, seed: int = 0\n)\n</code></pre> <p>               Bases: <code>AbstractPrior</code></p> <p>Implements the log normal prior.</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior--parameters","title":"Parameters","text":"<p>sigma : float     Specifies the standard deviation of the normal distribution. mean : float     Specifies the mean of the normal distribution. seed : int, defaults to 0</p> Source code in <code>smac/model/gaussian_process/priors/log_normal_prior.py</code> <pre><code>def __init__(\n    self,\n    sigma: float,\n    mean: float = 0,\n    seed: int = 0,\n):\n    super().__init__(seed=seed)\n\n    if mean != 0:\n        raise NotImplementedError(mean)\n\n    self._sigma = sigma\n    self._sigma_square = sigma**2\n    self._mean = mean\n    self._sqrt_2_pi = np.sqrt(2 * np.pi)\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_gradient","title":"get_gradient","text":"<pre><code>get_gradient(theta: float) -&gt; float\n</code></pre> <p>Computes the gradient of the prior with respect to theta. Internally, his method calls <code>self._get_gradient</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_gradient--warning","title":"Warning","text":"<p>Theta must be on the original scale.</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_gradient--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_gradient--returns","title":"Returns","text":"<p>gradient : float     The gradient of the prior at theta.</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_gradient(self, theta: float) -&gt; float:\n    \"\"\"Computes the gradient of the prior with respect to theta. Internally, his method calls `self._get_gradient`.\n\n    Warning\n    -------\n    Theta must be on the original scale.\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space\n\n    Returns\n    -------\n    gradient : float\n        The gradient of the prior at theta.\n    \"\"\"\n    return self._get_gradient(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_log_probability","title":"get_log_probability","text":"<pre><code>get_log_probability(theta: float) -&gt; float\n</code></pre> <p>Returns the log probability of theta. This method exponentiates theta and calls <code>self._get_log_probability</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_log_probability--warning","title":"Warning","text":"<p>Theta must be on a log scale!</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_log_probability--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space.</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.get_log_probability--returns","title":"Returns","text":"<p>float     The log probability of theta</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_log_probability(self, theta: float) -&gt; float:\n    \"\"\"Returns the log probability of theta. This method exponentiates theta and calls `self._get_log_probability`.\n\n    Warning\n    -------\n    Theta must be on a log scale!\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space.\n\n    Returns\n    -------\n    float\n        The log probability of theta\n    \"\"\"\n    return self._get_log_probability(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.sample_from_prior","title":"sample_from_prior","text":"<pre><code>sample_from_prior(n_samples: int) -&gt; ndarray\n</code></pre> <p>Returns <code>n_samples</code> from the prior. All samples are on a log scale. This method calls <code>self._sample_from_prior</code> and applies a log transformation to the obtained values.</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.sample_from_prior--parameters","title":"Parameters","text":"<p>n_samples : int     The number of samples that will be drawn.</p>"},{"location":"api/smac/model/gaussian_process/priors/log_normal_prior/#smac.model.gaussian_process.priors.log_normal_prior.LogNormalPrior.sample_from_prior--returns","title":"Returns","text":"<p>samples : np.ndarray</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def sample_from_prior(self, n_samples: int) -&gt; np.ndarray:\n    \"\"\"Returns `n_samples` from the prior. All samples are on a log scale. This method calls\n    `self._sample_from_prior` and applies a log transformation to the obtained values.\n\n    Parameters\n    ----------\n    n_samples : int\n        The number of samples that will be drawn.\n\n    Returns\n    -------\n    samples : np.ndarray\n    \"\"\"\n    if np.ndim(n_samples) != 0:\n        raise ValueError(\"argument n_samples needs to be a scalar (is %s)\" % n_samples)\n\n    if n_samples &lt;= 0:\n        raise ValueError(\"argument n_samples needs to be positive (is %d)\" % n_samples)\n\n    sample = np.log(self._sample_from_prior(n_samples=n_samples))\n\n    if np.any(~np.isfinite(sample)):\n        raise ValueError(\"Sample %s from prior %s contains infinite values!\" % (sample, self))\n\n    return sample\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/","title":"Tophat prior","text":""},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior","title":"smac.model.gaussian_process.priors.tophat_prior","text":""},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.SoftTopHatPrior","title":"SoftTopHatPrior","text":"<pre><code>SoftTopHatPrior(\n    lower_bound: float,\n    upper_bound: float,\n    exponent: float,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractPrior</code></p> <p>Soft Tophat prior as it used in the original spearmint code.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.SoftTopHatPrior--parameters","title":"Parameters","text":"<p>lower_bound : float     Lower bound of the prior. In original scale. upper_bound : float     Upper bound of the prior. In original scale. exponent : float     Exponent of the prior. seed : int, defaults to 0</p> Source code in <code>smac/model/gaussian_process/priors/tophat_prior.py</code> <pre><code>def __init__(\n    self,\n    lower_bound: float,\n    upper_bound: float,\n    exponent: float,\n    seed: int = 0,\n) -&gt; None:\n    super().__init__(seed=seed)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n        self._lower_bound = lower_bound\n        try:\n            self._log_lower_bound = np.log(lower_bound)\n        except RuntimeWarning as w:\n            if \"invalid value encountered in log\" in w.args[0]:\n                raise ValueError(\"Invalid lower bound %f (cannot compute log)\" % lower_bound)\n\n            raise w\n        self._upper_bound = upper_bound\n        try:\n            self._log_upper_bound = np.log(upper_bound)\n        except RuntimeWarning as w:\n            if \"invalid value encountered in log\" in w.args[0]:\n                raise ValueError(\"Invalid lower bound %f (cannot compute log)\" % lower_bound)\n\n            raise w\n\n    if exponent &lt;= 0:\n        raise ValueError(\"Exponent cannot be less or equal than zero (but is %f)\" % exponent)\n\n    self._exponent = exponent\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.SoftTopHatPrior.sample_from_prior","title":"sample_from_prior","text":"<pre><code>sample_from_prior(n_samples: int) -&gt; ndarray\n</code></pre> <p>Returns <code>n_samples</code> from the prior. All samples are on a log scale. This method calls <code>self._sample_from_prior</code> and applies a log transformation to the obtained values.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.SoftTopHatPrior.sample_from_prior--parameters","title":"Parameters","text":"<p>n_samples : int     The number of samples that will be drawn.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.SoftTopHatPrior.sample_from_prior--returns","title":"Returns","text":"<p>samples : np.ndarray</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def sample_from_prior(self, n_samples: int) -&gt; np.ndarray:\n    \"\"\"Returns `n_samples` from the prior. All samples are on a log scale. This method calls\n    `self._sample_from_prior` and applies a log transformation to the obtained values.\n\n    Parameters\n    ----------\n    n_samples : int\n        The number of samples that will be drawn.\n\n    Returns\n    -------\n    samples : np.ndarray\n    \"\"\"\n    if np.ndim(n_samples) != 0:\n        raise ValueError(\"argument n_samples needs to be a scalar (is %s)\" % n_samples)\n\n    if n_samples &lt;= 0:\n        raise ValueError(\"argument n_samples needs to be positive (is %d)\" % n_samples)\n\n    sample = np.log(self._sample_from_prior(n_samples=n_samples))\n\n    if np.any(~np.isfinite(sample)):\n        raise ValueError(\"Sample %s from prior %s contains infinite values!\" % (sample, self))\n\n    return sample\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior","title":"TophatPrior","text":"<pre><code>TophatPrior(\n    lower_bound: float, upper_bound: float, seed: int = 0\n)\n</code></pre> <p>               Bases: <code>AbstractPrior</code></p> <p>Tophat prior as it used in the original spearmint code.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior--parameters","title":"Parameters","text":"<p>lower_bound : float     Lower bound of the prior. In original scale. upper_bound : float     Upper bound of the prior. In original scale. seed : int, defaults to 0</p> Source code in <code>smac/model/gaussian_process/priors/tophat_prior.py</code> <pre><code>def __init__(\n    self,\n    lower_bound: float,\n    upper_bound: float,\n    seed: int = 0,\n):\n    super().__init__(seed=seed)\n    self._min = lower_bound\n    self._log_min = np.log(lower_bound)\n    self._max = upper_bound\n    self._log_max = np.log(upper_bound)\n    self._prob = 1 / (self._max - self._min)\n    self._log_prob = np.log(self._prob)\n\n    if not (self._max &gt; self._min):\n        raise Exception(\"Upper bound of Tophat prior must be greater than the lower bound.\")\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior.get_log_probability","title":"get_log_probability","text":"<pre><code>get_log_probability(theta: float) -&gt; float\n</code></pre> <p>Returns the log probability of theta. This method exponentiates theta and calls <code>self._get_log_probability</code>.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior.get_log_probability--warning","title":"Warning","text":"<p>Theta must be on a log scale!</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior.get_log_probability--parameters","title":"Parameters","text":"<p>theta : float     Hyperparameter configuration in log space.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior.get_log_probability--returns","title":"Returns","text":"<p>float     The log probability of theta</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def get_log_probability(self, theta: float) -&gt; float:\n    \"\"\"Returns the log probability of theta. This method exponentiates theta and calls `self._get_log_probability`.\n\n    Warning\n    -------\n    Theta must be on a log scale!\n\n    Parameters\n    ----------\n    theta : float\n        Hyperparameter configuration in log space.\n\n    Returns\n    -------\n    float\n        The log probability of theta\n    \"\"\"\n    return self._get_log_probability(np.exp(theta))\n</code></pre>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior.sample_from_prior","title":"sample_from_prior","text":"<pre><code>sample_from_prior(n_samples: int) -&gt; ndarray\n</code></pre> <p>Returns <code>n_samples</code> from the prior. All samples are on a log scale. This method calls <code>self._sample_from_prior</code> and applies a log transformation to the obtained values.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior.sample_from_prior--parameters","title":"Parameters","text":"<p>n_samples : int     The number of samples that will be drawn.</p>"},{"location":"api/smac/model/gaussian_process/priors/tophat_prior/#smac.model.gaussian_process.priors.tophat_prior.TophatPrior.sample_from_prior--returns","title":"Returns","text":"<p>samples : np.ndarray</p> Source code in <code>smac/model/gaussian_process/priors/abstract_prior.py</code> <pre><code>def sample_from_prior(self, n_samples: int) -&gt; np.ndarray:\n    \"\"\"Returns `n_samples` from the prior. All samples are on a log scale. This method calls\n    `self._sample_from_prior` and applies a log transformation to the obtained values.\n\n    Parameters\n    ----------\n    n_samples : int\n        The number of samples that will be drawn.\n\n    Returns\n    -------\n    samples : np.ndarray\n    \"\"\"\n    if np.ndim(n_samples) != 0:\n        raise ValueError(\"argument n_samples needs to be a scalar (is %s)\" % n_samples)\n\n    if n_samples &lt;= 0:\n        raise ValueError(\"argument n_samples needs to be positive (is %d)\" % n_samples)\n\n    sample = np.log(self._sample_from_prior(n_samples=n_samples))\n\n    if np.any(~np.isfinite(sample)):\n        raise ValueError(\"Sample %s from prior %s contains infinite values!\" % (sample, self))\n\n    return sample\n</code></pre>"},{"location":"api/smac/model/random_forest/abstract_random_forest/","title":"Abstract random forest","text":""},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest","title":"smac.model.random_forest.abstract_random_forest","text":""},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest","title":"AbstractRandomForest","text":"<pre><code>AbstractRandomForest(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>AbstractModel</code></p> <p>Abstract base class for all random forest models.</p> Source code in <code>smac/model/random_forest/abstract_random_forest.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n\n    self._conditional: dict[int, bool] = dict()\n    self._impute_values: dict[int, float] = dict()\n</code></pre>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.predict_marginalized","title":"predict_marginalized","text":"<pre><code>predict_marginalized(X: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Predicts mean and variance marginalized over all instances.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.predict_marginalized--warning","title":"Warning","text":"<p>The input data must not include any features.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.predict_marginalized--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters]     Input data points.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.predict_marginalized--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, 1]     The predictive mean. vars : np.ndarray [#samples, 1]     The predictive variance.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict_marginalized(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predicts mean and variance marginalized over all instances.\n\n    Warning\n    -------\n    The input data must not include any features.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters]\n        Input data points.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, 1]\n        The predictive mean.\n    vars : np.ndarray [#samples, 1]\n        The predictive variance.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters (and no features) for this method, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._instance_features is None:\n        mean, var = self.predict(X)\n        assert var is not None\n\n        var[var &lt; self._var_threshold] = self._var_threshold\n        var[np.isnan(var)] = self._var_threshold\n\n        return mean, var\n    else:\n        n_instances = len(self._instance_features)\n\n        mean = np.zeros(X.shape[0])\n        var = np.zeros(X.shape[0])\n        for i, x in enumerate(X):\n            features = np.array(list(self._instance_features.values()))\n            x_tiled = np.tile(x, (n_instances, 1))\n            X_ = np.hstack((x_tiled, features))\n\n            means, vars = self.predict(X_)\n            assert vars is not None\n\n            # VAR[1/n (X_1 + ... + X_n)] =\n            # 1/n^2 * ( VAR(X_1) + ... + VAR(X_n))\n            # for independent X_1 ... X_n\n            var_x = np.sum(vars) / (len(vars) ** 2)\n            if var_x &lt; self._var_threshold:\n                var_x = self._var_threshold\n\n            var[i] = var_x\n            mean[i] = np.mean(means)\n\n        if len(mean.shape) == 1:\n            mean = mean.reshape((-1, 1))\n\n        if len(var.shape) == 1:\n            var = var.reshape((-1, 1))\n\n        return mean, var\n</code></pre>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/random_forest/abstract_random_forest/#smac.model.random_forest.abstract_random_forest.AbstractRandomForest.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/model/random_forest/random_forest/","title":"Random forest","text":""},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest","title":"smac.model.random_forest.random_forest","text":""},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest","title":"RandomForest","text":"<pre><code>RandomForest(\n    configspace: ConfigurationSpace,\n    n_trees: int = N_TREES,\n    n_points_per_tree: int = -1,\n    ratio_features: float = 5.0 / 6.0,\n    min_samples_split: int = 3,\n    min_samples_leaf: int = 3,\n    max_depth: int = 2**20,\n    eps_purity: float = 1e-08,\n    max_nodes: int = 2**20,\n    bootstrapping: bool = True,\n    log_y: bool = False,\n    instance_features: (\n        dict[str, list[int | float]] | None\n    ) = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractRandomForest</code></p> <p>Random forest that takes instance features into account.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest--parameters","title":"Parameters","text":"<p>n_trees : int, defaults to <code>N_TREES</code>     The number of trees in the random forest. n_points_per_tree : int, defaults to -1     Number of points per tree. If the value is smaller than 0, the number of samples will be used. ratio_features : float, defaults to 5.0 / 6.0     The ratio of features that are considered for splitting. min_samples_split : int, defaults to 3     The minimum number of data points to perform a split. min_samples_leaf : int, defaults to 3     The minimum number of data points in a leaf. max_depth : int, defaults to 220     The maximum depth of a single tree. eps_purity : float, defaults to 1e-8     The minimum difference between two target values to be considered. max_nodes : int, defaults to 220     The maximum total number of nodes in a tree. bootstrapping : bool, defaults to True     Enables bootstrapping. log_y: bool, defaults to False     The y values (passed to this random forest) are expected to be log(y) transformed.     This will be considered during predicting. instance_features : dict[str, list[int | float]] | None, defaults to None     Features (list of int or floats) of the instances (str). The features are incorporated into the X data,     on which the model is trained on. pca_components : float, defaults to 7     Number of components to keep when using PCA to reduce dimensionality of instance features. seed : int</p> Source code in <code>smac/model/random_forest/random_forest.py</code> <pre><code>def __init__(\n    self,\n    configspace: ConfigurationSpace,\n    n_trees: int = N_TREES,\n    n_points_per_tree: int = -1,\n    ratio_features: float = 5.0 / 6.0,\n    min_samples_split: int = 3,\n    min_samples_leaf: int = 3,\n    max_depth: int = 2**20,\n    eps_purity: float = 1e-8,\n    max_nodes: int = 2**20,\n    bootstrapping: bool = True,\n    log_y: bool = False,\n    instance_features: dict[str, list[int | float]] | None = None,\n    pca_components: int | None = 7,\n    seed: int = 0,\n) -&gt; None:\n    super().__init__(\n        configspace=configspace,\n        instance_features=instance_features,\n        pca_components=pca_components,\n        seed=seed,\n    )\n\n    max_features = 0 if ratio_features &gt; 1.0 else max(1, int(len(self._types) * ratio_features))\n\n    self._rf_opts = regression.forest_opts()\n    self._rf_opts.num_trees = n_trees\n    self._rf_opts.do_bootstrapping = bootstrapping\n    self._rf_opts.tree_opts.max_features = max_features\n    self._rf_opts.tree_opts.min_samples_to_split = min_samples_split\n    self._rf_opts.tree_opts.min_samples_in_leaf = min_samples_leaf\n    self._rf_opts.tree_opts.max_depth = max_depth\n    self._rf_opts.tree_opts.epsilon_purity = eps_purity\n    self._rf_opts.tree_opts.max_num_nodes = max_nodes\n    self._rf_opts.compute_law_of_total_variance = False\n    self._rf: BinaryForest | None = None\n    self._log_y = log_y\n\n    # Case to `int` incase we get an `np.integer` type\n    self._rng = regression.default_random_engine(int(seed))\n\n    self._n_trees = n_trees\n    self._n_points_per_tree = n_points_per_tree\n    self._ratio_features = ratio_features\n    self._min_samples_split = min_samples_split\n    self._min_samples_leaf = min_samples_leaf\n    self._max_depth = max_depth\n    self._eps_purity = eps_purity\n    self._max_nodes = max_nodes\n    self._bootstrapping = bootstrapping\n</code></pre>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray, covariance_type: str | None = \"diagonal\"\n) -&gt; tuple[ndarray, ndarray | None]\n</code></pre> <p>Predicts mean and variance for a given X. Internally, calls the method <code>_predict</code>.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. covariance_type: str | None, defaults to \"diagonal\"     Specifies what to return along with the mean. Applied only to Gaussian Processes.     Takes four valid inputs:     * None: Only the mean is returned.     * \"std\": Standard deviation at test points is returned.     * \"diagonal\": Diagonal of the covariance matrix is returned.     * \"full\": Whole covariance matrix between the test points is returned.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.predict--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, #objectives]     The predictive mean. vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None     Predictive variance or standard deviation.</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def predict(\n    self,\n    X: np.ndarray,\n    covariance_type: str | None = \"diagonal\",\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    covariance_type: str | None, defaults to \"diagonal\"\n        Specifies what to return along with the mean. Applied only to Gaussian Processes.\n        Takes four valid inputs:\n        * None: Only the mean is returned.\n        * \"std\": Standard deviation at test points is returned.\n        * \"diagonal\": Diagonal of the covariance matrix is returned.\n        * \"full\": Whole covariance matrix between the test points is returned.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, #objectives]\n        The predictive mean.\n    vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n        Predictive variance or standard deviation.\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if self._apply_pca:\n        try:\n            X_feats = X[:, -self._n_features :]\n            X_feats = self._scaler.transform(X_feats)\n            X_feats = self._pca.transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n        except NotFittedError:\n            # PCA not fitted if only one training sample\n            pass\n\n    if X.shape[1] != len(self._types):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n        mean, var = self._predict(X, covariance_type)\n\n    if len(mean.shape) == 1:\n        mean = mean.reshape((-1, 1))\n\n    if var is not None and len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean, var\n</code></pre>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.predict_marginalized","title":"predict_marginalized","text":"<pre><code>predict_marginalized(X: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Predicts mean and variance marginalized over all instances.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.predict_marginalized--note","title":"Note","text":"<p>The method is random forest specific and follows the SMAC2 implementation. It requires no distribution assumption to marginalize the uncertainty estimates.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.predict_marginalized--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameter + #features]     Input data points.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.predict_marginalized--returns","title":"Returns","text":"<p>means : np.ndarray [#samples, 1]     The predictive mean. vars : np.ndarray [#samples, 1]     The predictive variance.</p> Source code in <code>smac/model/random_forest/random_forest.py</code> <pre><code>def predict_marginalized(self, X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Predicts mean and variance marginalized over all instances.\n\n    Note\n    ----\n    The method is random forest specific and follows the SMAC2 implementation. It requires\n    no distribution assumption to marginalize the uncertainty estimates.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameter + #features]\n        Input data points.\n\n    Returns\n    -------\n    means : np.ndarray [#samples, 1]\n        The predictive mean.\n    vars : np.ndarray [#samples, 1]\n        The predictive variance.\n    \"\"\"\n    if self._n_features == 0:\n        mean_, var = self.predict(X)\n        assert var is not None\n\n        var[var &lt; self._var_threshold] = self._var_threshold\n        var[np.isnan(var)] = self._var_threshold\n\n        return mean_, var\n\n    assert self._instance_features is not None\n\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != len(self._bounds):\n        raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._bounds), X.shape[1]))\n\n    assert self._rf is not None\n    X = self._impute_inactive(X)\n\n    X_feat = list(self._instance_features.values())\n    dat_ = self._rf.predict_marginalized_over_instances_batch(X, X_feat, self._log_y)\n    dat_ = np.array(dat_)\n\n    # 3. compute statistics across trees\n    mean_ = dat_.mean(axis=1)\n    var = dat_.var(axis=1)\n\n    if var is None:\n        raise RuntimeError(\"The variance must not be none.\")\n\n    var[var &lt; self._var_threshold] = self._var_threshold\n\n    if len(mean_.shape) == 1:\n        mean_ = mean_.reshape((-1, 1))\n    if len(var.shape) == 1:\n        var = var.reshape((-1, 1))\n\n    return mean_, var\n</code></pre>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.train","title":"train","text":"<pre><code>train(X: ndarray, Y: ndarray) -&gt; Self\n</code></pre> <p>Trains the random forest on X and Y. Internally, calls the method <code>_train</code>.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.train--parameters","title":"Parameters","text":"<p>X : np.ndarray [#samples, #hyperparameters + #features]     Input data points. Y : np.ndarray [#samples, #objectives]     The corresponding target values.</p>"},{"location":"api/smac/model/random_forest/random_forest/#smac.model.random_forest.random_forest.RandomForest.train--returns","title":"Returns","text":"<p>self : AbstractModel</p> Source code in <code>smac/model/abstract_model.py</code> <pre><code>def train(self: Self, X: np.ndarray, Y: np.ndarray) -&gt; Self:\n    \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n    Parameters\n    ----------\n    X : np.ndarray [#samples, #hyperparameters + #features]\n        Input data points.\n    Y : np.ndarray [#samples, #objectives]\n        The corresponding target values.\n\n    Returns\n    -------\n    self : AbstractModel\n    \"\"\"\n    if len(X.shape) != 2:\n        raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n    if X.shape[1] != self._n_hps + self._n_features:\n        raise ValueError(\n            f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n            f\"but has {X.shape[1]} in total.\"\n        )\n\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n    # Reduce dimensionality of features if larger than PCA_DIM\n    if (\n        self._pca_components is not None\n        and X.shape[0] &gt; self._pca.n_components\n        and self._n_features &gt;= self._pca_components\n    ):\n        X_feats = X[:, -self._n_features :]\n\n        # Scale features\n        X_feats = self._scaler.fit_transform(X_feats)\n        X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n        # PCA\n        X_feats = self._pca.fit_transform(X_feats)\n        X = np.hstack((X[:, : self._n_hps], X_feats))\n\n        if hasattr(self, \"_types\"):\n            # For RF, adapt types list\n            # if X_feats.shape[0] &lt; self._pca, X_feats.shape[1] == X_feats.shape[0]\n            self._types = np.array(\n                np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                dtype=np.uint,\n            )  # type: ignore\n\n        self._apply_pca = True\n    else:\n        self._apply_pca = False\n\n        if hasattr(self, \"_types\"):\n            self._types = copy.deepcopy(self._initial_types)\n\n    return self._train(X, Y)\n</code></pre>"},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/","title":"Abstract multi objective algorithm","text":""},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/#smac.multi_objective.abstract_multi_objective_algorithm","title":"smac.multi_objective.abstract_multi_objective_algorithm","text":""},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/#smac.multi_objective.abstract_multi_objective_algorithm.AbstractMultiObjectiveAlgorithm","title":"AbstractMultiObjectiveAlgorithm","text":"<pre><code>AbstractMultiObjectiveAlgorithm()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A general interface for multi-objective optimizer, depending on different strategies.</p> Source code in <code>smac/multi_objective/abstract_multi_objective_algorithm.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/#smac.multi_objective.abstract_multi_objective_algorithm.AbstractMultiObjectiveAlgorithm.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/#smac.multi_objective.abstract_multi_objective_algorithm.AbstractMultiObjectiveAlgorithm.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(values: list[float]) -&gt; float\n</code></pre> <p>Transform a multi-objective loss to a single loss.</p>"},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/#smac.multi_objective.abstract_multi_objective_algorithm.AbstractMultiObjectiveAlgorithm.__call__--parameters","title":"Parameters","text":"<p>values : list[float]     Normalized values in the range [0, 1].</p>"},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/#smac.multi_objective.abstract_multi_objective_algorithm.AbstractMultiObjectiveAlgorithm.__call__--returns","title":"Returns","text":"<p>cost : float     Combined cost.</p> Source code in <code>smac/multi_objective/abstract_multi_objective_algorithm.py</code> <pre><code>@abstractmethod\ndef __call__(self, values: list[float]) -&gt; float:\n    \"\"\"Transform a multi-objective loss to a single loss.\n\n    Parameters\n    ----------\n    values : list[float]\n        Normalized values in the range [0, 1].\n\n    Returns\n    -------\n    cost : float\n        Combined cost.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/multi_objective/abstract_multi_objective_algorithm/#smac.multi_objective.abstract_multi_objective_algorithm.AbstractMultiObjectiveAlgorithm.update_on_iteration_start","title":"update_on_iteration_start","text":"<pre><code>update_on_iteration_start() -&gt; None\n</code></pre> <p>Update the internal state on start of each SMBO iteration.</p> Source code in <code>smac/multi_objective/abstract_multi_objective_algorithm.py</code> <pre><code>def update_on_iteration_start(self) -&gt; None:\n    \"\"\"Update the internal state on start of each SMBO iteration.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/multi_objective/aggregation_strategy/","title":"Aggregation strategy","text":""},{"location":"api/smac/multi_objective/aggregation_strategy/#smac.multi_objective.aggregation_strategy","title":"smac.multi_objective.aggregation_strategy","text":""},{"location":"api/smac/multi_objective/aggregation_strategy/#smac.multi_objective.aggregation_strategy.MeanAggregationStrategy","title":"MeanAggregationStrategy","text":"<pre><code>MeanAggregationStrategy(\n    scenario: Scenario,\n    objective_weights: list[float] | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractMultiObjectiveAlgorithm</code></p> <p>A class to mean-aggregate multi-objective costs to a single cost.</p>"},{"location":"api/smac/multi_objective/aggregation_strategy/#smac.multi_objective.aggregation_strategy.MeanAggregationStrategy--parameters","title":"Parameters","text":"<p>scenario : Scenario objective_weights : list[float] | None, defaults to None     Weights for an weighted average. Must be of the same length as the number of objectives.</p> Source code in <code>smac/multi_objective/aggregation_strategy.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    objective_weights: list[float] | None = None,\n):\n    super(MeanAggregationStrategy, self).__init__()\n\n    if objective_weights is not None and scenario.count_objectives() != len(objective_weights):\n        raise ValueError(\"Number of objectives and number of weights must be equal.\")\n\n    self._objective_weights = objective_weights\n</code></pre>"},{"location":"api/smac/multi_objective/aggregation_strategy/#smac.multi_objective.aggregation_strategy.MeanAggregationStrategy.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/multi_objective/aggregation_strategy/#smac.multi_objective.aggregation_strategy.MeanAggregationStrategy.update_on_iteration_start","title":"update_on_iteration_start","text":"<pre><code>update_on_iteration_start() -&gt; None\n</code></pre> <p>Update the internal state on start of each SMBO iteration.</p> Source code in <code>smac/multi_objective/abstract_multi_objective_algorithm.py</code> <pre><code>def update_on_iteration_start(self) -&gt; None:\n    \"\"\"Update the internal state on start of each SMBO iteration.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/multi_objective/parego/","title":"Parego","text":""},{"location":"api/smac/multi_objective/parego/#smac.multi_objective.parego","title":"smac.multi_objective.parego","text":""},{"location":"api/smac/multi_objective/parego/#smac.multi_objective.parego.ParEGO","title":"ParEGO","text":"<pre><code>ParEGO(\n    scenario: Scenario,\n    rho: float = 0.05,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractMultiObjectiveAlgorithm</code></p> <p>ParEGO implementation based on www.cs.bham.ac.uk/~jdk/UKCI-2015.pdf.</p>"},{"location":"api/smac/multi_objective/parego/#smac.multi_objective.parego.ParEGO--parameters","title":"Parameters","text":"<p>scenario : Scenario rho : float, defaults to 0.05     A small positive value. seed : int | None, defaults to None</p> Source code in <code>smac/multi_objective/parego.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    rho: float = 0.05,\n    seed: int | None = None,\n):\n    super(ParEGO, self).__init__()\n\n    if seed is None:\n        seed = scenario.seed\n\n    self._n_objectives = scenario.count_objectives()\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n\n    self._rho = rho\n    # Will be set on starting an SMBO iteration\n    self._theta: np.ndarray | None = None\n</code></pre>"},{"location":"api/smac/random_design/abstract_random_design/","title":"Abstract random design","text":""},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design","title":"smac.random_design.abstract_random_design","text":""},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design.AbstractRandomDesign","title":"AbstractRandomDesign","text":"<pre><code>AbstractRandomDesign(seed: int = 0)\n</code></pre> <p>Abstract base of helper classes to configure interleaving of random configurations in a list of challengers.</p>"},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design.AbstractRandomDesign--parameters","title":"Parameters","text":"<p>seed : int     The random seed initializing this design.</p> Source code in <code>smac/random_design/abstract_random_design.py</code> <pre><code>def __init__(self, seed: int = 0):\n    self._seed = seed\n    self._rng = np.random.RandomState(seed=seed)\n</code></pre>"},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design.AbstractRandomDesign.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta data of the created object.</p>"},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design.AbstractRandomDesign.check","title":"check  <code>abstractmethod</code>","text":"<pre><code>check(iteration: int) -&gt; bool\n</code></pre> <p>Check, if the next configuration should be random.</p>"},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design.AbstractRandomDesign.check--parameters","title":"Parameters","text":"<p>iteration : int     Number of the i-th configuration evaluated in an SMBO iteration.</p>"},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design.AbstractRandomDesign.check--returns","title":"Returns","text":"<p>bool     Whether the next configuration should be random.</p> Source code in <code>smac/random_design/abstract_random_design.py</code> <pre><code>@abstractmethod\ndef check(self, iteration: int) -&gt; bool:\n    \"\"\"Check, if the next configuration should be random.\n\n    Parameters\n    ----------\n    iteration : int\n        Number of the i-th configuration evaluated in an SMBO iteration.\n\n    Returns\n    -------\n    bool\n        Whether the next configuration should be random.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/random_design/abstract_random_design/#smac.random_design.abstract_random_design.AbstractRandomDesign.next_iteration","title":"next_iteration","text":"<pre><code>next_iteration() -&gt; None\n</code></pre> <p>Indicates the beginning of the next SMBO iteration.</p> Source code in <code>smac/random_design/abstract_random_design.py</code> <pre><code>def next_iteration(self) -&gt; None:\n    \"\"\"Indicates the beginning of the next SMBO iteration.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/random_design/annealing_design/","title":"Annealing design","text":""},{"location":"api/smac/random_design/annealing_design/#smac.random_design.annealing_design","title":"smac.random_design.annealing_design","text":""},{"location":"api/smac/random_design/annealing_design/#smac.random_design.annealing_design.CosineAnnealingRandomDesign","title":"CosineAnnealingRandomDesign","text":"<pre><code>CosineAnnealingRandomDesign(\n    min_probability: float,\n    max_probability: float,\n    restart_iteration: int,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractRandomDesign</code></p> <p>Interleaves a random configuration according to a given probability which is decreased according to a cosine annealing schedule.</p>"},{"location":"api/smac/random_design/annealing_design/#smac.random_design.annealing_design.CosineAnnealingRandomDesign--parameters","title":"Parameters","text":"<p>max_probability : float     Initial (maximum) probability of a random configuration. min_probability : float     Final (minimal) probability of a random configuration used in iteration <code>restart_iteration</code>. restart_iteration : int     Restart the annealing schedule every <code>restart_iteration</code> iterations. seed : int     Integer used to initialize random state.</p> Source code in <code>smac/random_design/annealing_design.py</code> <pre><code>def __init__(self, min_probability: float, max_probability: float, restart_iteration: int, seed: int = 0):\n    super().__init__(seed)\n    assert 0 &lt;= min_probability &lt;= 1\n    assert 0 &lt;= max_probability &lt;= 1\n    assert max_probability &gt; min_probability\n    assert restart_iteration &gt; 2\n    self._max_probability = max_probability\n    self._min_probability = min_probability\n\n    # Internally, iteration indices start at 0, so we need to decrease this\n    self._restart_iteration = restart_iteration - 1\n    self._iteration = 0\n    self._probability = max_probability\n</code></pre>"},{"location":"api/smac/random_design/annealing_design/#smac.random_design.annealing_design.CosineAnnealingRandomDesign.next_iteration","title":"next_iteration","text":"<pre><code>next_iteration() -&gt; None\n</code></pre> <p>Moves to the next iteration and set <code>self._probability</code>.</p> Source code in <code>smac/random_design/annealing_design.py</code> <pre><code>def next_iteration(self) -&gt; None:  # noqa: D102\n    \"\"\"Moves to the next iteration and set ``self._probability``.\"\"\"\n    self._iteration += 1\n    if self._iteration &gt; self._restart_iteration:\n        self._iteration = 0\n        logger.debug(\"Perform a restart.\")\n\n    self._probability = self._min_probability + (\n        0.5\n        * (self._max_probability - self._min_probability)\n        * (1 + np.cos(self._iteration * np.pi / self._restart_iteration))\n    )\n    logger.debug(f\"Probability for random configs: {self._probability}\")\n</code></pre>"},{"location":"api/smac/random_design/modulus_design/","title":"Modulus design","text":""},{"location":"api/smac/random_design/modulus_design/#smac.random_design.modulus_design","title":"smac.random_design.modulus_design","text":""},{"location":"api/smac/random_design/modulus_design/#smac.random_design.modulus_design.DynamicModulusRandomDesign","title":"DynamicModulusRandomDesign","text":"<pre><code>DynamicModulusRandomDesign(\n    start_modulus: float = 2.0,\n    modulus_increment: float = 0.3,\n    end_modulus: float = inf,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>AbstractRandomDesign</code></p> <p>Interleave a random configuration, decreasing the fraction of random configurations over time.</p>"},{"location":"api/smac/random_design/modulus_design/#smac.random_design.modulus_design.DynamicModulusRandomDesign--parameters","title":"Parameters","text":"<p>start_modulus : float, defaults to 2.0    Initially, every modulus-th configuration will be at random. modulus_increment : float, defaults to 0.3    Increase modulus by this amount in every iteration. end_modulus : float, defaults to np.inf    The maximum modulus ever used. If the value is reached before the optimization    is over, it is not further increased. If it is not reached before the optimization is over,    there will be no adjustment to make sure that the <code>end_modulus</code> is reached. seed : int, defaults to 0     Integer used to initialize the random state. This class does not use the seed.</p> Source code in <code>smac/random_design/modulus_design.py</code> <pre><code>def __init__(\n    self, start_modulus: float = 2.0, modulus_increment: float = 0.3, end_modulus: float = np.inf, seed: int = 0\n):\n    super().__init__(seed)\n    assert start_modulus &gt; 0\n    assert modulus_increment &gt; 0\n    assert end_modulus &gt; 0\n    assert end_modulus &gt; start_modulus\n\n    if start_modulus &lt;= 1.0 and modulus_increment &lt;= 0.0:\n        logger.warning(\"Using SMAC with random configurations only. ROAR is the better choice for this.\")\n\n    self._modulus = start_modulus\n    self._start_modulus = start_modulus\n    self._modulus_increment = modulus_increment\n    self._end_modulus = end_modulus\n</code></pre>"},{"location":"api/smac/random_design/modulus_design/#smac.random_design.modulus_design.ModulusRandomDesign","title":"ModulusRandomDesign","text":"<pre><code>ModulusRandomDesign(modulus: float = 2.0, seed: int = 0)\n</code></pre> <p>               Bases: <code>AbstractRandomDesign</code></p> <p>Interleave a random configuration after a constant number of configurations found by Bayesian optimization.</p>"},{"location":"api/smac/random_design/modulus_design/#smac.random_design.modulus_design.ModulusRandomDesign--parameters","title":"Parameters","text":"<p>modulus : float     Every modulus-th configuration will be at random. seed : int     Integer used to initialize random state. This class does not use the seed.</p> Source code in <code>smac/random_design/modulus_design.py</code> <pre><code>def __init__(self, modulus: float = 2.0, seed: int = 0):\n    super().__init__(seed)\n    assert modulus &gt; 0\n    if modulus &lt;= 1.0:\n        logger.warning(\"Using SMAC with random configurations only. ROAR is the better choice for this.\")\n\n    self._modulus = modulus\n</code></pre>"},{"location":"api/smac/random_design/modulus_design/#smac.random_design.modulus_design.ModulusRandomDesign.next_iteration","title":"next_iteration","text":"<pre><code>next_iteration() -&gt; None\n</code></pre> <p>Indicates the beginning of the next SMBO iteration.</p> Source code in <code>smac/random_design/abstract_random_design.py</code> <pre><code>def next_iteration(self) -&gt; None:\n    \"\"\"Indicates the beginning of the next SMBO iteration.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/random_design/probability_design/","title":"Probability design","text":""},{"location":"api/smac/random_design/probability_design/#smac.random_design.probability_design","title":"smac.random_design.probability_design","text":""},{"location":"api/smac/random_design/probability_design/#smac.random_design.probability_design.DynamicProbabilityRandomDesign","title":"DynamicProbabilityRandomDesign","text":"<pre><code>DynamicProbabilityRandomDesign(\n    probability: float, factor: float, seed: int = 0\n)\n</code></pre> <p>               Bases: <code>AbstractRandomDesign</code></p> <p>Interleave a random configuration according to a given probability which is decreased over time.</p>"},{"location":"api/smac/random_design/probability_design/#smac.random_design.probability_design.DynamicProbabilityRandomDesign--parameters","title":"Parameters","text":"<p>probability : float     Probability that a configuration will be drawn at random. factor : float     Multiply the <code>probability</code> by <code>factor</code> in each iteration. seed : int, defaults to 0     Integer used to initialize the random state.</p> Source code in <code>smac/random_design/probability_design.py</code> <pre><code>def __init__(self, probability: float, factor: float, seed: int = 0):\n    super().__init__(seed)\n    assert 0 &lt;= probability &lt;= 1\n    assert factor &gt; 0\n\n    self._probability = probability\n    self._factor = factor\n</code></pre>"},{"location":"api/smac/random_design/probability_design/#smac.random_design.probability_design.DynamicProbabilityRandomDesign.next_iteration","title":"next_iteration","text":"<pre><code>next_iteration() -&gt; None\n</code></pre> <p>Sets the probability to the current value multiplied by <code>factor</code>.</p> Source code in <code>smac/random_design/probability_design.py</code> <pre><code>def next_iteration(self) -&gt; None:\n    \"\"\"Sets the probability to the current value multiplied by ``factor``.\"\"\"\n    self._probability *= self._factor\n</code></pre>"},{"location":"api/smac/random_design/probability_design/#smac.random_design.probability_design.ProbabilityRandomDesign","title":"ProbabilityRandomDesign","text":"<pre><code>ProbabilityRandomDesign(probability: float, seed: int = 0)\n</code></pre> <p>               Bases: <code>AbstractRandomDesign</code></p> <p>Interleave a random configuration according to a given probability.</p>"},{"location":"api/smac/random_design/probability_design/#smac.random_design.probability_design.ProbabilityRandomDesign--parameters","title":"Parameters","text":"<p>probability : float     Probability that a configuration will be drawn at random. seed : int, defaults to 0     Integer used to initialize the random state.</p> Source code in <code>smac/random_design/probability_design.py</code> <pre><code>def __init__(self, probability: float, seed: int = 0):\n    super().__init__(seed=seed)\n    assert 0 &lt;= probability &lt;= 1\n    self._probability = probability\n</code></pre>"},{"location":"api/smac/random_design/probability_design/#smac.random_design.probability_design.ProbabilityRandomDesign.next_iteration","title":"next_iteration","text":"<pre><code>next_iteration() -&gt; None\n</code></pre> <p>Indicates the beginning of the next SMBO iteration.</p> Source code in <code>smac/random_design/abstract_random_design.py</code> <pre><code>def next_iteration(self) -&gt; None:\n    \"\"\"Indicates the beginning of the next SMBO iteration.\"\"\"\n    pass\n</code></pre>"},{"location":"api/smac/runhistory/dataclasses/","title":"Dataclasses","text":""},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses","title":"smac.runhistory.dataclasses","text":""},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.InstanceSeedBudgetKey","title":"InstanceSeedBudgetKey  <code>dataclass</code>","text":"<pre><code>InstanceSeedBudgetKey(\n    instance: str | None = None,\n    seed: int | None = None,\n    budget: float | None = None,\n)\n</code></pre> <p>Key for instance, seed and budget.</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.InstanceSeedBudgetKey--parameters","title":"Parameters","text":"<p>instance : str | None, defaults to None seed : int | None, defaults to None budget : float | None, defaults to None</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.InstanceSeedBudgetKey.get_instance_seed_key","title":"get_instance_seed_key","text":"<pre><code>get_instance_seed_key() -&gt; InstanceSeedKey\n</code></pre> <p>Returns the instance-seed key. The budget is omitted.</p> Source code in <code>smac/runhistory/dataclasses.py</code> <pre><code>def get_instance_seed_key(self) -&gt; InstanceSeedKey:\n    \"\"\"Returns the instance-seed key. The budget is omitted.\"\"\"\n    return InstanceSeedKey(instance=self.instance, seed=self.seed)\n</code></pre>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.InstanceSeedKey","title":"InstanceSeedKey  <code>dataclass</code>","text":"<pre><code>InstanceSeedKey(\n    instance: str | None = None, seed: int | None = None\n)\n</code></pre> <p>Key for instance and seed.</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.InstanceSeedKey--parameters","title":"Parameters","text":"<p>instance : str | None, defaults to None seed : int | None, defaults to None</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrajectoryItem","title":"TrajectoryItem  <code>dataclass</code>","text":"<pre><code>TrajectoryItem(\n    config_ids: list[int],\n    costs: list[float | list[float]],\n    trial: int,\n    walltime: float,\n)\n</code></pre> <p>Item of a trajectory.</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrajectoryItem--parameters","title":"Parameters","text":"<p>config_ids : list[int]     Configuration ids of the current incumbents. costs : list[float | list[float]]     Costs of the current incumbents. In case of multi-objective, this is a list of lists. trial : int     How many trials have been evaluated so far. walltime : float     How much walltime has been used so far.</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialInfo","title":"TrialInfo  <code>dataclass</code>","text":"<pre><code>TrialInfo(\n    config: Configuration,\n    instance: str | None = None,\n    seed: int | None = None,\n    budget: float | None = None,\n)\n</code></pre> <p>Information about a trial.</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialInfo--parameters","title":"Parameters","text":"<p>config : Configuration instance : str | None, defaults to None seed : int | None, defaults to None budget : float | None, defaults to None</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialInfo.get_instance_seed_budget_key","title":"get_instance_seed_budget_key","text":"<pre><code>get_instance_seed_budget_key() -&gt; InstanceSeedBudgetKey\n</code></pre> <p>Instantiates and returns an InstanceSeedBudgetKey object.</p> Source code in <code>smac/runhistory/dataclasses.py</code> <pre><code>def get_instance_seed_budget_key(self) -&gt; InstanceSeedBudgetKey:\n    \"\"\"Instantiates and returns an InstanceSeedBudgetKey object.\"\"\"\n    return InstanceSeedBudgetKey(instance=self.instance, seed=self.seed, budget=self.budget)\n</code></pre>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialInfo.get_instance_seed_key","title":"get_instance_seed_key","text":"<pre><code>get_instance_seed_key() -&gt; InstanceSeedKey\n</code></pre> <p>Instantiates and returns an InstanceSeedKey object</p> Source code in <code>smac/runhistory/dataclasses.py</code> <pre><code>def get_instance_seed_key(self) -&gt; InstanceSeedKey:\n    \"\"\"Instantiates and returns an InstanceSeedKey object\"\"\"\n    return InstanceSeedKey(instance=self.instance, seed=self.seed)\n</code></pre>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialKey","title":"TrialKey  <code>dataclass</code>","text":"<pre><code>TrialKey(\n    config_id: int,\n    instance: str | None = None,\n    seed: int | None = None,\n    budget: float | None = None,\n)\n</code></pre> <p>Key of a trial.</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialKey--parameters","title":"Parameters","text":"<p>config_id : int instance : str | None, defaults to None seed : int | None, defaults to None budget : float | None, defaults to None</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialValue","title":"TrialValue  <code>dataclass</code>","text":"<pre><code>TrialValue(\n    cost: float | list[float],\n    time: float = 0.0,\n    cpu_time: float = 0.0,\n    status: StatusType = SUCCESS,\n    starttime: float = 0.0,\n    endtime: float = 0.0,\n    additional_info: dict[str, Any] = dict(),\n)\n</code></pre> <p>Values of a trial.</p>"},{"location":"api/smac/runhistory/dataclasses/#smac.runhistory.dataclasses.TrialValue--parameters","title":"Parameters","text":"<p>cost : float | list[float] time : float, defaults to 0.0 cpu_time : float, defaults to 0.0     Describes the amount of time the trial spend on hardware. status : StatusType, defaults to StatusType.SUCCESS starttime : float, defaults to 0.0 endtime : float, defaults to 0.0 additional_info : dict[str, Any], defaults to {}</p>"},{"location":"api/smac/runhistory/enumerations/","title":"Enumerations","text":""},{"location":"api/smac/runhistory/enumerations/#smac.runhistory.enumerations","title":"smac.runhistory.enumerations","text":""},{"location":"api/smac/runhistory/enumerations/#smac.runhistory.enumerations.StatusType","title":"StatusType","text":"<p>               Bases: <code>IntEnum</code></p> <p>Class to define status types of configs.</p>"},{"location":"api/smac/runhistory/errors/","title":"Errors","text":""},{"location":"api/smac/runhistory/errors/#smac.runhistory.errors","title":"smac.runhistory.errors","text":""},{"location":"api/smac/runhistory/runhistory/","title":"Runhistory","text":""},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory","title":"smac.runhistory.runhistory","text":""},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory","title":"RunHistory","text":"<pre><code>RunHistory(\n    multi_objective_algorithm: (\n        AbstractMultiObjectiveAlgorithm | None\n    ) = None,\n    overwrite_existing_trials: bool = False,\n)\n</code></pre> <p>               Bases: <code>Mapping[TrialKey, TrialValue]</code></p> <p>Container for the target function run information.</p> <p>Most importantly, the runhistory contains an efficient mapping from each evaluated configuration to the empirical cost observed on either the full instance set or a subset. The cost is the average over all observed costs for one configuration:</p> <ul> <li>If using budgets for a single instance, only the cost on the highest observed budget is returned.</li> <li>If using instances as the budget, the average cost over all evaluated instances is returned.</li> <li>Theoretically, the runhistory object can handle instances and budgets at the same time. This is   neither used nor tested.</li> </ul>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory--note","title":"Note","text":"<p>Guaranteed to be picklable.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory--parameters","title":"Parameters","text":"<p>multi_objective_algorithm : AbstractMultiObjectiveAlgorithm | None, defaults to None     The multi-objective algorithm is required to scalarize the costs in case of multi-objective. overwrite_existing_trials : bool, defaults to false     Overwrites a trial (combination of configuration, instance, budget and seed) if it already exists.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def __init__(\n    self,\n    multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None,\n    overwrite_existing_trials: bool = False,\n) -&gt; None:\n    self._multi_objective_algorithm = multi_objective_algorithm\n    self._overwrite_existing_trials = overwrite_existing_trials\n    self.reset()\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.config_ids","title":"config_ids  <code>property</code>","text":"<pre><code>config_ids: dict[Configuration, int]\n</code></pre> <p>Mapping from configuration to config id.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.finished","title":"finished  <code>property</code>","text":"<pre><code>finished: int\n</code></pre> <p>Returns how many trials have been finished.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.ids_config","title":"ids_config  <code>property</code>","text":"<pre><code>ids_config: dict[int, Configuration]\n</code></pre> <p>Mapping from config id to configuration.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi-objective algorithm required to scaralize the costs in case of multi-objective.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.objective_bounds","title":"objective_bounds  <code>property</code>","text":"<pre><code>objective_bounds: list[tuple[float, float]]\n</code></pre> <p>Returns the lower and upper bound of each objective.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.running","title":"running  <code>property</code>","text":"<pre><code>running: int\n</code></pre> <p>Returns how many trials are still running.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.submitted","title":"submitted  <code>property</code>","text":"<pre><code>submitted: int\n</code></pre> <p>Returns how many trials have been submitted.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.__contains__","title":"__contains__","text":"<pre><code>__contains__(k: object) -&gt; bool\n</code></pre> <p>Dictionary semantics for <code>k in runhistory</code>.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def __contains__(self, k: object) -&gt; bool:\n    \"\"\"Dictionary semantics for `k in runhistory`.\"\"\"\n    return k in self._data\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Enables to check equality of runhistory if the run is continued.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Enables to check equality of runhistory if the run is continued.\"\"\"\n    return self._data == other._data\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(k: TrialKey) -&gt; TrialValue\n</code></pre> <p>Dictionary semantics for <code>v = runhistory[k]</code>.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def __getitem__(self, k: TrialKey) -&gt; TrialValue:\n    \"\"\"Dictionary semantics for `v = runhistory[k]`.\"\"\"\n    return self._data[k]\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[TrialKey]\n</code></pre> <p>Dictionary semantics for <code>for k in runhistory.keys()</code>.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def __iter__(self) -&gt; Iterator[TrialKey]:\n    \"\"\"Dictionary semantics for `for k in runhistory.keys()`.\"\"\"\n    return iter(self._data.keys())\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Enables the <code>len(runhistory)</code></p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Enables the `len(runhistory)`\"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.add","title":"add","text":"<pre><code>add(\n    config: Configuration,\n    cost: int | float | list[int | float],\n    time: float = 0.0,\n    cpu_time: float = 0.0,\n    status: StatusType = SUCCESS,\n    instance: str | None = None,\n    seed: int | None = None,\n    budget: float | None = None,\n    starttime: float = 0.0,\n    endtime: float = 0.0,\n    additional_info: dict[str, Any] = None,\n    force_update: bool = False,\n) -&gt; None\n</code></pre> <p>Adds a new trial to the RunHistory.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.add--parameters","title":"Parameters","text":"<p>config : Configuration cost : int | float | list[int | float]     Cost of the evaluated trial. Might be a list in case of multi-objective. time : float     How much time was needed to evaluate the trial. cpu_time : float     How much time was needed on the hardware to evaluate the trial. status : StatusType, defaults to StatusType.SUCCESS     The status of the trial. instance : str | None, defaults to none seed : int | None, defaults to none budget : float | None, defaults to none starttime : float, defaults to 0.0 endtime : float, defaults to 0.0 additional_info : dict[str, Any], defaults to {} force_update : bool, defaults to false     Overwrites a previous trial if the trial already exists.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def add(\n    self,\n    config: Configuration,\n    cost: int | float | list[int | float],\n    time: float = 0.0,\n    cpu_time: float = 0.0,\n    status: StatusType = StatusType.SUCCESS,\n    instance: str | None = None,\n    seed: int | None = None,\n    budget: float | None = None,\n    starttime: float = 0.0,\n    endtime: float = 0.0,\n    additional_info: dict[str, Any] = None,\n    force_update: bool = False,\n) -&gt; None:\n    \"\"\"Adds a new trial to the RunHistory.\n\n    Parameters\n    ----------\n    config : Configuration\n    cost : int | float | list[int | float]\n        Cost of the evaluated trial. Might be a list in case of multi-objective.\n    time : float\n        How much time was needed to evaluate the trial.\n    cpu_time : float\n        How much time was needed on the hardware to evaluate the trial.\n    status : StatusType, defaults to StatusType.SUCCESS\n        The status of the trial.\n    instance : str | None, defaults to none\n    seed : int | None, defaults to none\n    budget : float | None, defaults to none\n    starttime : float, defaults to 0.0\n    endtime : float, defaults to 0.0\n    additional_info : dict[str, Any], defaults to {}\n    force_update : bool, defaults to false\n        Overwrites a previous trial if the trial already exists.\n    \"\"\"\n    if config is None:\n        raise TypeError(\"Configuration must not be None.\")\n    elif not isinstance(config, Configuration):\n        raise TypeError(\"Configuration is not of type Configuration, but %s.\" % type(config))\n    if additional_info is None:\n        additional_info = {}\n\n    # Squeeze is important to reduce arrays with one element\n    # to scalars.\n    cost_array = np.asarray(cost).squeeze()\n    n_objectives = np.size(cost_array)\n\n    # Get the config id\n    config_id = self._config_ids.get(config)\n\n    if config_id is None:\n        self._n_id += 1\n        self._config_ids[config] = self._n_id\n        self._ids_config[self._n_id] = config\n\n        config_id = self._n_id\n\n    # Set the id attribute of the config object, so that users can access it\n    config.config_id = config_id\n\n    if status != StatusType.RUNNING:\n        if self._n_objectives == -1:\n            self._n_objectives = n_objectives\n        elif self._n_objectives != n_objectives:\n            raise ValueError(\n                f\"Cost is not of the same length ({n_objectives}) as the number of \"\n                f\"objectives ({self._n_objectives}).\"\n            )\n\n        # Let's always work with floats; Makes it easier to deal with later on\n        # array.tolist(), it returns a scalar if the array has one element.\n        c = cost_array.tolist()\n        if self._n_objectives == 1:\n            c = float(c)\n        else:\n            c = [float(i) for i in c]\n    else:\n        c = cost_array.tolist()\n\n    if budget is not None:\n        # Just to make sure we really add a float\n        budget = float(budget)\n\n    k = TrialKey(config_id=config_id, instance=instance, seed=seed, budget=budget)\n    v = TrialValue(\n        cost=c,\n        time=time,\n        cpu_time=cpu_time,\n        status=status,\n        starttime=starttime,\n        endtime=endtime,\n        additional_info=additional_info,\n    )\n\n    # Construct keys and values for the data dictionary\n    for key, value in (\n        (\"config\", dict(config)),\n        (\"config_id\", config_id),\n        (\"instance\", instance),\n        (\"seed\", seed),\n        (\"budget\", budget),\n        (\"cost\", c),\n        (\"time\", time),\n        (\"cpu_time\", cpu_time),\n        (\"status\", status),\n        (\"starttime\", starttime),\n        (\"endtime\", endtime),\n        (\"additional_info\", additional_info),\n        (\"origin\", config.origin),\n    ):\n        self._check_json_serializable(key, value, k, v)\n\n    # Each trial_key is supposed to be used only once. Repeated tries to add\n    # the same trial_key will be ignored silently if not capped.\n    previous_k = self._data.get(k)\n    if self._overwrite_existing_trials or force_update or previous_k is None:\n        # Update stati\n        if previous_k is None:\n            if status == StatusType.RUNNING:\n                self._running += 1\n            else:\n                self._finished += 1\n\n            self._submitted += 1\n        else:\n            if previous_k.status == StatusType.RUNNING and status != StatusType.RUNNING:\n                self._running -= 1\n                self._finished += 1\n\n        self._add(k, v, status)\n    else:\n        logger.info(\"Entry was not added to the runhistory because existing trials will not be overwritten.\")\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.add_running_trial","title":"add_running_trial","text":"<pre><code>add_running_trial(trial: TrialInfo) -&gt; None\n</code></pre> <p>Adds a running trial to the runhistory.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.add_running_trial--parameters","title":"Parameters","text":"<p>trial : TrialInfo     The <code>TrialInfo</code> object of the running trial.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def add_running_trial(self, trial: TrialInfo) -&gt; None:\n    \"\"\"Adds a running trial to the runhistory.\n\n    Parameters\n    ----------\n    trial : TrialInfo\n        The ``TrialInfo`` object of the running trial.\n    \"\"\"\n    self.add(\n        config=trial.config,\n        cost=float(MAXINT),\n        time=0.0,\n        cpu_time=0.0,\n        status=StatusType.RUNNING,\n        instance=trial.instance,\n        seed=trial.seed,\n        budget=trial.budget,\n    )\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.add_trial","title":"add_trial","text":"<pre><code>add_trial(info: TrialInfo, value: TrialValue) -&gt; None\n</code></pre> <p>Adds a trial to the runhistory.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.add_trial--parameters","title":"Parameters","text":"<p>trial : TrialInfo     The <code>TrialInfo</code> object of the running trial.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def add_trial(self, info: TrialInfo, value: TrialValue) -&gt; None:\n    \"\"\"Adds a trial to the runhistory.\n\n    Parameters\n    ----------\n    trial : TrialInfo\n        The ``TrialInfo`` object of the running trial.\n    \"\"\"\n    self.add(\n        config=info.config,\n        cost=value.cost,\n        time=value.time,\n        cpu_time=value.cpu_time,\n        status=value.status,\n        instance=info.instance,\n        seed=info.seed,\n        budget=info.budget,\n        starttime=value.starttime,\n        endtime=value.endtime,\n        additional_info=value.additional_info,\n    )\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.average_cost","title":"average_cost","text":"<pre><code>average_cost(\n    config: Configuration,\n    instance_seed_budget_keys: (\n        list[InstanceSeedBudgetKey] | None\n    ) = None,\n    normalize: bool = False,\n) -&gt; float | list[float]\n</code></pre> <p>Return the average cost of a configuration. This is the mean of costs of all instance- seed pairs.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.average_cost--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to calculate objective for. instance_seed_budget_keys : list, optional (default=None)     List of tuples of instance-seeds-budget keys. If None, the runhistory is     queried for all trials of the given configuration. normalize : bool, optional (default=False)     Normalizes the costs wrt. objective bounds in the multi-objective setting.     Only a float is returned if normalize is True. Warning: The value can change     over time because the objective bounds are changing. Also, the objective weights are     incorporated.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.average_cost--returns","title":"Returns","text":"<p>Cost: float | list[float]     Average cost. In case of multiple objectives, the mean of each objective is returned.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def average_cost(\n    self,\n    config: Configuration,\n    instance_seed_budget_keys: list[InstanceSeedBudgetKey] | None = None,\n    normalize: bool = False,\n) -&gt; float | list[float]:\n    \"\"\"Return the average cost of a configuration. This is the mean of costs of all instance-\n    seed pairs.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to calculate objective for.\n    instance_seed_budget_keys : list, optional (default=None)\n        List of tuples of instance-seeds-budget keys. If None, the runhistory is\n        queried for all trials of the given configuration.\n    normalize : bool, optional (default=False)\n        Normalizes the costs wrt. objective bounds in the multi-objective setting.\n        Only a float is returned if normalize is True. Warning: The value can change\n        over time because the objective bounds are changing. Also, the objective weights are\n        incorporated.\n\n    Returns\n    -------\n    Cost: float | list[float]\n        Average cost. In case of multiple objectives, the mean of each objective is returned.\n    \"\"\"\n    costs = self._cost(config, instance_seed_budget_keys)\n    if costs:\n        if self._n_objectives &gt; 1:\n            # Each objective is averaged separately\n            # [[100, 200], [0, 0]] -&gt; [50, 100]\n            averaged_costs = np.mean(costs, axis=0).tolist()\n\n            if normalize:\n                assert self.multi_objective_algorithm is not None\n                normalized_costs = normalize_costs(averaged_costs, self._objective_bounds)\n\n                return self.multi_objective_algorithm(normalized_costs)\n            else:\n                return averaged_costs\n\n        return float(np.mean(costs))\n\n    return np.nan\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.empty","title":"empty","text":"<pre><code>empty() -&gt; bool\n</code></pre> <p>Check whether the RunHistory is empty.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.empty--returns","title":"Returns","text":"<p>emptiness: bool     True if trials have been added to the RunHistory.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def empty(self) -&gt; bool:\n    \"\"\"Check whether the RunHistory is empty.\n\n    Returns\n    -------\n    emptiness: bool\n        True if trials have been added to the RunHistory.\n    \"\"\"\n    return len(self._data) == 0\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_config","title":"get_config","text":"<pre><code>get_config(config_id: int) -&gt; Configuration\n</code></pre> <p>Returns the configuration from the configuration id.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_config(self, config_id: int) -&gt; Configuration:\n    \"\"\"Returns the configuration from the configuration id.\"\"\"\n    return self._ids_config[config_id]\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_config_id","title":"get_config_id","text":"<pre><code>get_config_id(config: Configuration) -&gt; int\n</code></pre> <p>Returns the configuration id from a configuration.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_config_id(self, config: Configuration) -&gt; int:\n    \"\"\"Returns the configuration id from a configuration.\"\"\"\n    return self._config_ids[config]\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_configs","title":"get_configs","text":"<pre><code>get_configs(\n    sort_by: str | None = None,\n) -&gt; list[Configuration]\n</code></pre> <p>Return all configurations in this RunHistory object.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_configs--parameters","title":"Parameters","text":"<p>sort_by : str | None, defaults to None     Sort the configs by <code>cost</code> (lowest cost first) or <code>num_trials</code> (config with lowest number of trials     first).</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_configs--returns","title":"Returns","text":"<p>configurations : list     All configurations in the runhistory.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_configs(self, sort_by: str | None = None) -&gt; list[Configuration]:\n    \"\"\"Return all configurations in this RunHistory object.\n\n    Parameters\n    ----------\n    sort_by : str | None, defaults to None\n        Sort the configs by ``cost`` (lowest cost first) or ``num_trials`` (config with lowest number of trials\n        first).\n\n    Returns\n    -------\n    configurations : list\n        All configurations in the runhistory.\n    \"\"\"\n    configs = list(self._config_ids.keys())\n\n    if sort_by == \"cost\":\n        return sorted(configs, key=lambda config: self._cost_per_config[self._config_ids[config]])\n    elif sort_by == \"num_trials\":\n        return sorted(configs, key=lambda config: len(self.get_trials(config)))\n    elif sort_by is None:\n        return configs\n    else:\n        raise ValueError(f\"Unknown sort_by value: {sort_by}.\")\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_configs_per_budget","title":"get_configs_per_budget","text":"<pre><code>get_configs_per_budget(\n    budget_subset: list[float | int | None] | None = None,\n) -&gt; list[Configuration]\n</code></pre> <p>Return all configs in this runhistory that have been run on one of these budgets.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_configs_per_budget--parameters","title":"Parameters","text":"<p>budget_subset: list[float | int | None] | None, defaults to None</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_configs_per_budget--returns","title":"Returns","text":"<p>configurations : list     List of configurations that have been run on the budgets in <code>budget_subset</code>.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_configs_per_budget(\n    self,\n    budget_subset: list[float | int | None] | None = None,\n) -&gt; list[Configuration]:\n    \"\"\"Return all configs in this runhistory that have been run on one of these budgets.\n\n    Parameters\n    ----------\n    budget_subset: list[float | int | None] | None, defaults to None\n\n    Returns\n    -------\n    configurations : list\n        List of configurations that have been run on the budgets in ``budget_subset``.\n    \"\"\"\n    if budget_subset is None:\n        return self.get_configs()\n\n    configs = []\n    for key in self._data.keys():\n        if key.budget in budget_subset:\n            configs.append(self._ids_config[key.config_id])\n\n    return configs\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_cost","title":"get_cost","text":"<pre><code>get_cost(config: Configuration) -&gt; float\n</code></pre> <p>Returns empirical cost for a configuration. See the class docstring for how the costs are computed. The costs are not re-computed, but are read from cache.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_cost--parameters","title":"Parameters","text":"<p>config: Configuration</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_cost--returns","title":"Returns","text":"<p>cost: float     Computed cost for configuration</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_cost(self, config: Configuration) -&gt; float:\n    \"\"\"Returns empirical cost for a configuration. See the class docstring for how the costs are\n    computed. The costs are not re-computed, but are read from cache.\n\n    Parameters\n    ----------\n    config: Configuration\n\n    Returns\n    -------\n    cost: float\n        Computed cost for configuration\n    \"\"\"\n    config_id = self._config_ids.get(config)\n\n    # Cost is always a single value (Single objective) or a list of values (Multi-objective)\n    # For example, _cost_per_config always holds the value on the highest budget\n    cost = self._cost_per_config.get(config_id, np.nan)  # type: ignore[arg-type] # noqa F821\n\n    if self._n_objectives &gt; 1:\n        assert isinstance(cost, list)\n        assert self.multi_objective_algorithm is not None\n\n        # We have to normalize the costs here\n        costs = normalize_costs(cost, self._objective_bounds)\n\n        # After normalization, we get the weighted average\n        return self.multi_objective_algorithm(costs)\n\n    assert isinstance(cost, float)\n    return float(cost)\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_instance_seed_budget_keys","title":"get_instance_seed_budget_keys","text":"<pre><code>get_instance_seed_budget_keys(\n    config: Configuration,\n    highest_observed_budget_only: bool = True,\n) -&gt; list[InstanceSeedBudgetKey]\n</code></pre> <p>Uses <code>get_trials</code> to return a list of instance-seed-budget keys.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_instance_seed_budget_keys--warning","title":"Warning","text":"<p>Does not return running instances.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_instance_seed_budget_keys--parameters","title":"Parameters","text":"<p>config : Configuration highest_observed_budget_only : bool, defaults to True     Select only the highest observed budget run for this configuration.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_instance_seed_budget_keys--returns","title":"Returns","text":"<p>list[InstanceSeedBudgetKey]</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_instance_seed_budget_keys(\n    self,\n    config: Configuration,\n    highest_observed_budget_only: bool = True,\n) -&gt; list[InstanceSeedBudgetKey]:\n    \"\"\"\n    Uses ``get_trials`` to return a list of instance-seed-budget keys.\n\n    Warning\n    -------\n    Does not return running instances.\n\n    Parameters\n    ----------\n    config : Configuration\n    highest_observed_budget_only : bool, defaults to True\n        Select only the highest observed budget run for this configuration.\n\n    Returns\n    -------\n    list[InstanceSeedBudgetKey]\n    \"\"\"\n    trials = self.get_trials(config, highest_observed_budget_only)\n\n    # Convert to instance-seed-budget key\n    return [InstanceSeedBudgetKey(t.instance, t.seed, t.budget) for t in trials]\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_min_cost","title":"get_min_cost","text":"<pre><code>get_min_cost(config: Configuration) -&gt; float\n</code></pre> <p>Returns the lowest empirical cost for a configuration across all trials.</p> <p>See the class docstring for how the costs are computed. The costs are not re-computed but are read from cache.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_min_cost--parameters","title":"Parameters","text":"<p>config : Configuration</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_min_cost--returns","title":"Returns","text":"<p>min_cost: float     Computed cost for configuration</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_min_cost(self, config: Configuration) -&gt; float:\n    \"\"\"Returns the lowest empirical cost for a configuration across all trials.\n\n    See the class docstring for how the costs are computed. The costs are not re-computed\n    but are read from cache.\n\n    Parameters\n    ----------\n    config : Configuration\n\n    Returns\n    -------\n    min_cost: float\n        Computed cost for configuration\n    \"\"\"\n    config_id = self._config_ids.get(config)\n    cost = self._min_cost_per_config.get(config_id, np.nan)  # type: ignore\n\n    if self._n_objectives &gt; 1:\n        assert isinstance(cost, list)\n        assert self.multi_objective_algorithm is not None\n\n        costs = normalize_costs(cost, self._objective_bounds)\n\n        # Note: We have to mean here because we already got the min cost\n        return self.multi_objective_algorithm(costs)\n\n    assert isinstance(cost, float)\n    return float(cost)\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_running_configs","title":"get_running_configs","text":"<pre><code>get_running_configs() -&gt; list[Configuration]\n</code></pre> <p>Returns all configurations which have at least one running trial.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_running_configs--returns","title":"Returns","text":"<p>list[Configuration]     List of configurations, all of which have at least one running trial.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_running_configs(self) -&gt; list[Configuration]:\n    \"\"\"Returns all configurations which have at least one running trial.\n\n    Returns\n    -------\n    list[Configuration]\n        List of configurations, all of which have at least one running trial.\n    \"\"\"\n    configs = []\n    for trial in self._running_trials:\n        if trial.config not in configs:\n            configs.append(trial.config)\n\n    return configs\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_running_trials","title":"get_running_trials","text":"<pre><code>get_running_trials(\n    config: Configuration | None = None,\n) -&gt; list[TrialInfo]\n</code></pre> <p>Returns all running trials for the passed configuration.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_running_trials--parameters","title":"Parameters","text":"<p>config : Configuration | None, defaults to None     Return only running trials from the passed configuration. If None, all configs are     considered.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_running_trials--returns","title":"Returns","text":"<p>trials : list[TrialInfo]     List of trials, all of which are still running.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_running_trials(self, config: Configuration | None = None) -&gt; list[TrialInfo]:\n    \"\"\"Returns all running trials for the passed configuration.\n\n    Parameters\n    ----------\n    config : Configuration | None, defaults to None\n        Return only running trials from the passed configuration. If None, all configs are\n        considered.\n\n    Returns\n    -------\n    trials : list[TrialInfo]\n        List of trials, all of which are still running.\n    \"\"\"\n    # Always work on copies\n    if config is None:\n        return [trial for trial in self._running_trials]\n    else:\n        return [trial for trial in self._running_trials if trial.config == config]\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_trials","title":"get_trials","text":"<pre><code>get_trials(\n    config: Configuration,\n    highest_observed_budget_only: bool = True,\n) -&gt; list[TrialInfo]\n</code></pre> <p>Returns all trials for a configuration.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_trials--warning","title":"Warning","text":"<p>Does not return running trials. Please use <code>get_running_trials</code> to receive running trials.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_trials--parameters","title":"Parameters","text":"<p>config : Configuration highest_observed_budget_only : bool     Select only the highest observed budget run for this configuration.     Meaning on multiple executions of the same instance-seed pair for a     a given configuration, only the highest observed budget is returned.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.get_trials--returns","title":"Returns","text":"<p>trials : list[InstanceSeedBudgetKey]     List of trials for the passed configuration.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def get_trials(\n    self,\n    config: Configuration,\n    highest_observed_budget_only: bool = True,\n) -&gt; list[TrialInfo]:\n    \"\"\"Returns all trials for a configuration.\n\n    Warning\n    -------\n    Does not return running trials. Please use ``get_running_trials`` to receive running trials.\n\n    Parameters\n    ----------\n    config : Configuration\n    highest_observed_budget_only : bool\n        Select only the highest observed budget run for this configuration.\n        Meaning on multiple executions of the same instance-seed pair for a\n        a given configuration, only the highest observed budget is returned.\n\n    Returns\n    -------\n    trials : list[InstanceSeedBudgetKey]\n        List of trials for the passed configuration.\n    \"\"\"\n    config_id = self._config_ids.get(config)\n    trials = {}\n    if config_id in self._config_id_to_isk_to_budget:\n        trials = self._config_id_to_isk_to_budget[config_id].copy()\n\n    # Select only the max budget run if specified\n    if highest_observed_budget_only:\n        for k, v in trials.items():\n            if None in v:\n                trials[k] = [None]\n            else:\n                trials[k] = [max([v_ for v_ in v if v_ is not None])]\n\n    return [TrialInfo(config, k.instance, k.seed, budget) for k, v in trials.items() for budget in v]\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.has_config","title":"has_config","text":"<pre><code>has_config(config: Configuration) -&gt; bool\n</code></pre> <p>Check if the config is stored in the runhistory</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def has_config(self, config: Configuration) -&gt; bool:\n    \"\"\"Check if the config is stored in the runhistory\"\"\"\n    return config in self._config_ids\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.incremental_update_cost","title":"incremental_update_cost","text":"<pre><code>incremental_update_cost(\n    config: Configuration, cost: float | list[float]\n) -&gt; None\n</code></pre> <p>Incrementally updates the performance of a configuration by using a moving average.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.incremental_update_cost--parameters","title":"Parameters","text":"<p>config: Configuration     configuration to update cost based on all trials in runhistory cost: float     cost of new run of config</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def incremental_update_cost(self, config: Configuration, cost: float | list[float]) -&gt; None:\n    \"\"\"Incrementally updates the performance of a configuration by using a moving average.\n\n    Parameters\n    ----------\n    config: Configuration\n        configuration to update cost based on all trials in runhistory\n    cost: float\n        cost of new run of config\n    \"\"\"\n    config_id = self._config_ids[config]\n    n_trials = self._num_trials_per_config.get(config_id, 0)\n\n    if self._n_objectives &gt; 1:\n        costs = np.array(cost)\n        old_costs = self._cost_per_config.get(config_id, np.array([0.0 for _ in range(self._n_objectives)]))\n        old_costs = np.array(old_costs)\n\n        new_costs = ((old_costs * n_trials) + costs) / (n_trials + 1)\n        self._cost_per_config[config_id] = new_costs.tolist()\n    else:\n        old_cost = self._cost_per_config.get(config_id, 0.0)\n\n        assert isinstance(cost, float)\n        assert isinstance(old_cost, float)\n        self._cost_per_config[config_id] = ((old_cost * n_trials) + cost) / (n_trials + 1)\n\n    self._num_trials_per_config[config_id] = n_trials + 1\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.load","title":"load","text":"<pre><code>load(\n    filename: str | Path, configspace: ConfigurationSpace\n) -&gt; None\n</code></pre> <p>Loads the runhistory from disk.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.load--warning","title":"Warning","text":"<p>Overwrites the current runhistory.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.load--parameters","title":"Parameters","text":"<p>filename : str | Path configspace : ConfigSpace</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def load(self, filename: str | Path, configspace: ConfigurationSpace) -&gt; None:\n    \"\"\"Loads the runhistory from disk.\n\n    Warning\n    -------\n    Overwrites the current runhistory.\n\n    Parameters\n    ----------\n    filename : str | Path\n    configspace : ConfigSpace\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    # We reset the RunHistory first to avoid any inconsistencies\n    self.reset()\n\n    try:\n        with open(filename) as fp:\n            data = json.load(fp)\n    except Exception as e:\n        logger.warning(\n            f\"Encountered exception {e} while reading RunHistory from {filename}. Not adding any trials!\"\n        )\n        return\n\n    config_origins = data.get(\"config_origins\", {})\n\n    self._ids_config = {}\n    for id_, values in data[\"configs\"].items():\n        self._ids_config[int(id_)] = Configuration(\n            configspace,\n            values=values,\n            origin=config_origins.get(id_, None),\n        )\n\n    self._config_ids = {config: id_ for id_, config in self._ids_config.items()}\n    self._n_id = len(self._config_ids)\n\n    # Important to use add method to use all data structure correctly\n    # NOTE: These hardcoded indices can easily lead to trouble\n    for entry in data[\"data\"]:\n        if self._n_objectives == -1:\n            if isinstance(entry[\"cost\"], (float, int)):\n                self._n_objectives = 1\n            else:\n                self._n_objectives = len(entry[\"cost\"])\n\n        cost: list[float] | float\n        if self._n_objectives == 1:\n            cost = float(entry[\"cost\"])\n        else:\n            cost = [float(x) for x in entry[\"cost\"]]\n        self.add(\n            config=self._ids_config[int(entry[\"config_id\"])],\n            cost=cost,\n            time=entry[\"time\"],\n            cpu_time=entry[\"cpu_time\"],\n            status=StatusType(entry[\"status\"]),\n            instance=entry[\"instance\"],\n            seed=entry[\"seed\"],\n            budget=entry[\"budget\"],\n            starttime=entry[\"starttime\"],\n            endtime=entry[\"endtime\"],\n            additional_info=entry[\"additional_info\"],\n        )\n\n    # Although adding trials should give us the same stats, the trajectory might be different\n    # because of the running status and/or overwriting trials\n    # Therefore, we just overwrite them\n    self._submitted = data[\"stats\"][\"submitted\"]\n    self._finished = data[\"stats\"][\"finished\"]\n    self._running = data[\"stats\"][\"running\"]\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.min_cost","title":"min_cost","text":"<pre><code>min_cost(\n    config: Configuration,\n    instance_seed_budget_keys: (\n        list[InstanceSeedBudgetKey] | None\n    ) = None,\n    normalize: bool = False,\n) -&gt; float | list[float]\n</code></pre> <p>Return the minimum cost of a configuration. This is the minimum cost of all instance-seed  pairs.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.min_cost--warning","title":"Warning","text":"<p>In the case of multi-fidelity, the minimum cost per objectives is returned.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.min_cost--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to calculate objective for. instance_seed_budget_keys : list, optional (default=None)     List of tuples of instance-seeds-budget keys. If None, the runhistory is     queried for all trials of the given configuration. normalize : bool, optional (default=False)     Normalizes the costs wrt objective bounds in the multi-objective setting.     Only a float is returned if normalize is True. Warning: The value can change     over time because the objective bounds are changing. Also, the objective weights are     incorporated.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.min_cost--returns","title":"Returns","text":"<p>min_cost: float | list[float]     Minimum cost of the config. In case of multi-objective, the minimum cost per objective     is returned.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def min_cost(\n    self,\n    config: Configuration,\n    instance_seed_budget_keys: list[InstanceSeedBudgetKey] | None = None,\n    normalize: bool = False,\n) -&gt; float | list[float]:\n    \"\"\"Return the minimum cost of a configuration. This is the minimum cost of all instance-seed\n     pairs.\n\n    Warning\n    -------\n    In the case of multi-fidelity, the minimum cost per objectives is returned.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to calculate objective for.\n    instance_seed_budget_keys : list, optional (default=None)\n        List of tuples of instance-seeds-budget keys. If None, the runhistory is\n        queried for all trials of the given configuration.\n    normalize : bool, optional (default=False)\n        Normalizes the costs wrt objective bounds in the multi-objective setting.\n        Only a float is returned if normalize is True. Warning: The value can change\n        over time because the objective bounds are changing. Also, the objective weights are\n        incorporated.\n\n    Returns\n    -------\n    min_cost: float | list[float]\n        Minimum cost of the config. In case of multi-objective, the minimum cost per objective\n        is returned.\n    \"\"\"\n    costs = self._cost(config, instance_seed_budget_keys)\n    if costs:\n        if self._n_objectives &gt; 1:\n            # Each objective is viewed separately\n            # [[100, 200], [20, 500]] -&gt; [20, 200]\n            min_costs = np.min(costs, axis=0).tolist()\n\n            if normalize:\n                assert self.multi_objective_algorithm is not None\n                normalized_costs = normalize_costs(min_costs, self._objective_bounds)\n\n                return self.multi_objective_algorithm(normalized_costs)\n            else:\n                return min_costs\n\n        return float(np.min(costs))\n\n    return np.nan\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Resets this runhistory to its default state.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Resets this runhistory to its default state.\"\"\"\n    # By having the data in a deterministic order we can do useful tests when we\n    # serialize the data and can assume it is still in the same order as it was added.\n    self._data: dict[TrialKey, TrialValue] = OrderedDict()\n\n    # Keep track of trials\n    self._submitted = 0\n    self._finished = 0\n    self._running = 0\n\n    # For fast access, we have also an unordered data structure to get all instance\n    # seed pairs of a configuration.\n    self._config_id_to_isk_to_budget: dict[int, dict[InstanceSeedKey, list[float | None]]] = {}\n    self._running_trials: list[TrialInfo] = []\n\n    self._config_ids: dict[Configuration, int] = {}\n    self._ids_config: dict[int, Configuration] = {}\n    self._n_id = 0\n\n    # Stores cost for each configuration ID\n    self._cost_per_config: dict[int, float | list[float]] = {}\n    # Stores min cost across all budgets for each configuration ID\n    self._min_cost_per_config: dict[int, float | list[float]] = {}\n    # Maps the configuration ID to the number of runs for that configuration\n    # and is necessary for computing the moving average.\n    self._num_trials_per_config: dict[int, int] = {}\n\n    # Store whether a datapoint is \"external\", which means it was read from\n    # a JSON file. Can be chosen to not be written to disk.\n    self._n_objectives: int = -1\n    self._objective_bounds: list[tuple[float, float]] = []\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.save","title":"save","text":"<pre><code>save(filename: str | Path = 'runhistory.json') -&gt; None\n</code></pre> <p>Saves RunHistory to disk.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.save--parameters","title":"Parameters","text":"<p>filename : str | Path, defaults to \"runhistory.json\"</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def save(self, filename: str | Path = \"runhistory.json\") -&gt; None:\n    \"\"\"Saves RunHistory to disk.\n\n    Parameters\n    ----------\n    filename : str | Path, defaults to \"runhistory.json\"\n    \"\"\"\n    data = list()\n    for k, v in self._data.items():\n        data.append(\n            {\n                \"config_id\": int(k.config_id),\n                \"instance\": str(k.instance) if k.instance is not None else None,\n                \"seed\": int(k.seed) if k.seed is not None else None,\n                \"budget\": float(k.budget) if k.budget is not None else None,\n                \"cost\": v.cost,\n                \"time\": v.time,\n                \"cpu_time\": v.cpu_time,\n                \"status\": v.status,\n                \"starttime\": v.starttime,\n                \"endtime\": v.endtime,\n                \"additional_info\": v.additional_info,\n            }\n        )\n\n    config_ids_to_serialize = set([entry[\"config_id\"] for entry in data])\n    configs = {}\n    config_origins = {}\n    for id_, config in self._ids_config.items():\n        if id_ in config_ids_to_serialize:\n            configs[id_] = dict(config)\n\n        config_origins[id_] = config.origin\n\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    assert str(filename).endswith(\".json\")\n    filename.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(filename, \"w\") as fp:\n        assert self._running == len(self._running_trials)\n        json.dump(\n            {\n                \"stats\": {\"submitted\": self._submitted, \"finished\": self._finished, \"running\": self._running},\n                \"data\": data,\n                \"configs\": configs,\n                \"config_origins\": config_origins,\n            },\n            fp,\n            indent=2,\n            cls=NumpyEncoder,\n        )\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.sum_cost","title":"sum_cost","text":"<pre><code>sum_cost(\n    config: Configuration,\n    instance_seed_budget_keys: (\n        list[InstanceSeedBudgetKey] | None\n    ) = None,\n    normalize: bool = False,\n) -&gt; float | list[float]\n</code></pre> <p>Return the sum of costs of a configuration. This is the sum of costs of all instance-seed pairs.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.sum_cost--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to calculate objective for. instance_seed_budget_keys : list, optional (default=None)     List of tuples of instance-seeds-budget keys. If None, the runhistory is     queried for all trials of the given configuration. normalize : bool, optional (default=False)     Normalizes the costs wrt objective bounds in the multi-objective setting.     Only a float is returned if normalize is True. Warning: The value can change     over time because the objective bounds are changing. Also, the objective weights are     incorporated.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.sum_cost--returns","title":"Returns","text":"<p>sum_cost: float | list[float]     Sum of costs of config. In case of multiple objectives, the costs are summed up for each     objective individually.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def sum_cost(\n    self,\n    config: Configuration,\n    instance_seed_budget_keys: list[InstanceSeedBudgetKey] | None = None,\n    normalize: bool = False,\n) -&gt; float | list[float]:\n    \"\"\"Return the sum of costs of a configuration. This is the sum of costs of all instance-seed\n    pairs.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to calculate objective for.\n    instance_seed_budget_keys : list, optional (default=None)\n        List of tuples of instance-seeds-budget keys. If None, the runhistory is\n        queried for all trials of the given configuration.\n    normalize : bool, optional (default=False)\n        Normalizes the costs wrt objective bounds in the multi-objective setting.\n        Only a float is returned if normalize is True. Warning: The value can change\n        over time because the objective bounds are changing. Also, the objective weights are\n        incorporated.\n\n    Returns\n    -------\n    sum_cost: float | list[float]\n        Sum of costs of config. In case of multiple objectives, the costs are summed up for each\n        objective individually.\n    \"\"\"\n    costs = self._cost(config, instance_seed_budget_keys)\n    if costs:\n        if self._n_objectives &gt; 1:\n            # Each objective is summed separately\n            # [[100, 200], [20, 10]] -&gt; [120, 210]\n            summed_costs = np.sum(costs, axis=0).tolist()\n\n            if normalize:\n                assert self.multi_objective_algorithm is not None\n                normalized_costs = normalize_costs(summed_costs, self._objective_bounds)\n\n                return self.multi_objective_algorithm(normalized_costs)\n            else:\n                return summed_costs\n\n    return float(np.sum(costs))\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update","title":"update","text":"<pre><code>update(runhistory: RunHistory) -&gt; None\n</code></pre> <p>Updates the current RunHistory by adding new trials from another RunHistory.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update--parameters","title":"Parameters","text":"<p>runhistory : RunHistory     RunHistory with additional data to be added to self</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def update(self, runhistory: RunHistory) -&gt; None:\n    \"\"\"Updates the current RunHistory by adding new trials from another RunHistory.\n\n    Parameters\n    ----------\n    runhistory : RunHistory\n        RunHistory with additional data to be added to self\n    \"\"\"\n    # Configurations might be already known, but by a different ID. This\n    # does not matter here because the add() method handles this\n    # correctly by assigning an ID to unknown configurations and re-using the ID.\n    for key, value in runhistory.items():\n        config = runhistory._ids_config[key.config_id]\n        self.add(\n            config=config,\n            cost=value.cost,\n            time=value.time,\n            cpu_time=value.cpu_time,\n            status=value.status,\n            instance=key.instance,\n            starttime=value.starttime,\n            endtime=value.endtime,\n            seed=key.seed,\n            budget=key.budget,\n            additional_info=value.additional_info,\n        )\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update_cost","title":"update_cost","text":"<pre><code>update_cost(config: Configuration) -&gt; None\n</code></pre> <p>Stores the performance of a configuration across the instances in <code>self._cost_per_config</code> and also updates <code>self._num_trials_per_config</code>.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update_cost--parameters","title":"Parameters","text":"<p>config: Configuration     configuration to update cost based on all trials in runhistory</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def update_cost(self, config: Configuration) -&gt; None:\n    \"\"\"Stores the performance of a configuration across the instances in `self._cost_per_config`\n    and also updates `self._num_trials_per_config`.\n\n    Parameters\n    ----------\n    config: Configuration\n        configuration to update cost based on all trials in runhistory\n    \"\"\"\n    config_id = self._config_ids[config]\n\n    # Removing duplicates while keeping the order\n    inst_seed_budgets = list(\n        dict.fromkeys(self.get_instance_seed_budget_keys(config, highest_observed_budget_only=True))\n    )\n    self._cost_per_config[config_id] = self.average_cost(config, inst_seed_budgets)\n    self._num_trials_per_config[config_id] = len(inst_seed_budgets)\n\n    all_isb = list(dict.fromkeys(self.get_instance_seed_budget_keys(config, highest_observed_budget_only=False)))\n    self._min_cost_per_config[config_id] = self.min_cost(config, all_isb)\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update_costs","title":"update_costs","text":"<pre><code>update_costs(instances: list[str] | None = None) -&gt; None\n</code></pre> <p>Computes the cost of all configurations from scratch and overwrites <code>self._cost_per_config</code> and <code>self._num_trials_per_config</code> accordingly.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update_costs--parameters","title":"Parameters","text":"<p>instances: list[str] | None, defaults to none     List of instances; if given, cost is only computed wrt to this instance set.</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def update_costs(self, instances: list[str] | None = None) -&gt; None:\n    \"\"\"Computes the cost of all configurations from scratch and overwrites `self._cost_per_config`\n    and `self._num_trials_per_config` accordingly.\n\n    Parameters\n    ----------\n    instances: list[str] | None, defaults to none\n        List of instances; if given, cost is only computed wrt to this instance set.\n    \"\"\"\n    self._cost_per_config = {}\n    self._num_trials_per_config = {}\n    for config, config_id in self._config_ids.items():\n        # Removing duplicates while keeping the order\n        inst_seed_budgets = list(\n            dict.fromkeys(self.get_instance_seed_budget_keys(config, highest_observed_budget_only=True))\n        )\n        if instances is not None:\n            inst_seed_budgets = list(filter(lambda x: x.instance in cast(list, instances), inst_seed_budgets))\n\n        if inst_seed_budgets:  # can be empty if never saw any trials on instances\n            self._cost_per_config[config_id] = self.average_cost(config, inst_seed_budgets)\n            self._min_cost_per_config[config_id] = self.min_cost(config, inst_seed_budgets)\n            self._num_trials_per_config[config_id] = len(inst_seed_budgets)\n</code></pre>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update_from_json","title":"update_from_json","text":"<pre><code>update_from_json(\n    filename: str, configspace: ConfigurationSpace\n) -&gt; None\n</code></pre> <p>Updates the current RunHistory by adding new trials from a json file.</p>"},{"location":"api/smac/runhistory/runhistory/#smac.runhistory.runhistory.RunHistory.update_from_json--parameters","title":"Parameters","text":"<p>filename : str     File name to load from. configspace : ConfigurationSpace</p> Source code in <code>smac/runhistory/runhistory.py</code> <pre><code>def update_from_json(\n    self,\n    filename: str,\n    configspace: ConfigurationSpace,\n) -&gt; None:\n    \"\"\"Updates the current RunHistory by adding new trials from a json file.\n\n    Parameters\n    ----------\n    filename : str\n        File name to load from.\n    configspace : ConfigurationSpace\n    \"\"\"\n    new_runhistory = RunHistory()\n    new_runhistory.load(filename, configspace)\n    self.update(runhistory=new_runhistory)\n</code></pre>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/","title":"Abstract encoder","text":""},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder","title":"smac.runhistory.encoder.abstract_encoder","text":""},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder","title":"AbstractRunHistoryEncoder","text":"<pre><code>AbstractRunHistoryEncoder(\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n)\n</code></pre> <p>Abstract class for preparing data in order to train a surrogate model.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder--parameters","title":"Parameters","text":"<p>scenario : Scenario object. considered_states : list[StatusType], defaults to [StatusType.SUCCESS, StatusType.CRASHED, StatusType.MEMORYOUT]  # noqa: E501     Trials with the passed states are considered. lower_budget_states : list[StatusType], defaults to []     Additionally consider all trials with these states for budget &lt; current budget. scale_percentage : int, defaults to 5     Scaled y-transformation use a percentile to estimate distance to optimum. Only used in some sub-classes. seed : int | None, defaults to none</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder--raises","title":"Raises","text":"<p>TypeError     If no success states are given.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n) -&gt; None:\n    if considered_states is None:\n        considered_states = [\n            StatusType.SUCCESS,\n            StatusType.CRASHED,\n            StatusType.MEMORYOUT,\n        ]\n\n    if seed is None:\n        seed = scenario.seed\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._scale_percentage = scale_percentage\n    self._n_objectives = scenario.count_objectives()\n    self._algorithm_walltime_limit = scenario.trial_walltime_limit\n    self._lower_budget_states = lower_budget_states if lower_budget_states is not None else []\n    self._considered_states = considered_states\n\n    self._instances = scenario.instances\n    self._instance_features = scenario.instance_features\n    self._n_features = scenario.count_instance_features()\n    self._n_params = len(list(scenario.configspace.values()))\n\n    if self._instances is not None and self._n_features == 0:\n        logger.warning(\n            \"We strongly encourage to use instance features when using instances.\",\n            \"If no instance features are passed, the runhistory encoder can not distinguish between different \"\n            \"instances and therefore returns the same data points with different values, all of which are \"\n            \"used to train the surrogate model.\\n\"\n            \"Consider using instance indices as features.\",\n        )\n\n    # Learned statistics\n    self._min_y = np.array([np.nan] * self._n_objectives)\n    self._max_y = np.array([np.nan] * self._n_objectives)\n    self._percentile = np.array([np.nan] * self._n_objectives)\n    self._multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None\n    self._runhistory: RunHistory | None = None\n</code></pre>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.transform_response_values","title":"transform_response_values  <code>abstractmethod</code>","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Transform function response values.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.transform_response_values--parameters","title":"Parameters","text":"<p>values : np.ndarray     Response values to be transformed.</p>"},{"location":"api/smac/runhistory/encoder/abstract_encoder/#smac.runhistory.encoder.abstract_encoder.AbstractRunHistoryEncoder.transform_response_values--returns","title":"Returns","text":"<p>transformed_values : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>@abstractmethod\ndef transform_response_values(\n    self,\n    values: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Transform function response values.\n\n    Parameters\n    ----------\n    values : np.ndarray\n        Response values to be transformed.\n\n    Returns\n    -------\n    transformed_values : np.ndarray\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runhistory/encoder/boing_encoder/","title":"Boing encoder","text":""},{"location":"api/smac/runhistory/encoder/boing_encoder/#smac.runhistory.encoder.boing_encoder","title":"smac.runhistory.encoder.boing_encoder","text":""},{"location":"api/smac/runhistory/encoder/eips_encoder/","title":"Eips encoder","text":""},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder","title":"smac.runhistory.encoder.eips_encoder","text":""},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder","title":"RunHistoryEIPSEncoder","text":"<pre><code>RunHistoryEIPSEncoder(\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractRunHistoryEncoder</code></p> <p>Encoder specifically for the EIPS (expected improvement per second) acquisition function.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n) -&gt; None:\n    if considered_states is None:\n        considered_states = [\n            StatusType.SUCCESS,\n            StatusType.CRASHED,\n            StatusType.MEMORYOUT,\n        ]\n\n    if seed is None:\n        seed = scenario.seed\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._scale_percentage = scale_percentage\n    self._n_objectives = scenario.count_objectives()\n    self._algorithm_walltime_limit = scenario.trial_walltime_limit\n    self._lower_budget_states = lower_budget_states if lower_budget_states is not None else []\n    self._considered_states = considered_states\n\n    self._instances = scenario.instances\n    self._instance_features = scenario.instance_features\n    self._n_features = scenario.count_instance_features()\n    self._n_params = len(list(scenario.configspace.values()))\n\n    if self._instances is not None and self._n_features == 0:\n        logger.warning(\n            \"We strongly encourage to use instance features when using instances.\",\n            \"If no instance features are passed, the runhistory encoder can not distinguish between different \"\n            \"instances and therefore returns the same data points with different values, all of which are \"\n            \"used to train the surrogate model.\\n\"\n            \"Consider using instance indices as features.\",\n        )\n\n    # Learned statistics\n    self._min_y = np.array([np.nan] * self._n_objectives)\n    self._max_y = np.array([np.nan] * self._n_objectives)\n    self._percentile = np.array([np.nan] * self._n_objectives)\n    self._multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None\n    self._runhistory: RunHistory | None = None\n</code></pre>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.transform_response_values","title":"transform_response_values","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Transform function response values. Transform the runtimes by a log transformation log(1. + runtime).</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.transform_response_values--parameters","title":"Parameters","text":"<p>values : np.ndarray     Response values to be transformed.</p>"},{"location":"api/smac/runhistory/encoder/eips_encoder/#smac.runhistory.encoder.eips_encoder.RunHistoryEIPSEncoder.transform_response_values--returns","title":"Returns","text":"<p>np.ndarray</p> Source code in <code>smac/runhistory/encoder/eips_encoder.py</code> <pre><code>def transform_response_values(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transform function response values. Transform the runtimes by a log transformation\n    log(1. + runtime).\n\n    Parameters\n    ----------\n    values : np.ndarray\n        Response values to be transformed.\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    # We need to ensure that time remains positive after the log transform.\n    values[:, 1] = np.log(1 + values[:, 1])\n    return values\n</code></pre>"},{"location":"api/smac/runhistory/encoder/encoder/","title":"Encoder","text":""},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder","title":"smac.runhistory.encoder.encoder","text":""},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder","title":"RunHistoryEncoder","text":"<pre><code>RunHistoryEncoder(\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractRunHistoryEncoder</code></p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n) -&gt; None:\n    if considered_states is None:\n        considered_states = [\n            StatusType.SUCCESS,\n            StatusType.CRASHED,\n            StatusType.MEMORYOUT,\n        ]\n\n    if seed is None:\n        seed = scenario.seed\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._scale_percentage = scale_percentage\n    self._n_objectives = scenario.count_objectives()\n    self._algorithm_walltime_limit = scenario.trial_walltime_limit\n    self._lower_budget_states = lower_budget_states if lower_budget_states is not None else []\n    self._considered_states = considered_states\n\n    self._instances = scenario.instances\n    self._instance_features = scenario.instance_features\n    self._n_features = scenario.count_instance_features()\n    self._n_params = len(list(scenario.configspace.values()))\n\n    if self._instances is not None and self._n_features == 0:\n        logger.warning(\n            \"We strongly encourage to use instance features when using instances.\",\n            \"If no instance features are passed, the runhistory encoder can not distinguish between different \"\n            \"instances and therefore returns the same data points with different values, all of which are \"\n            \"used to train the surrogate model.\\n\"\n            \"Consider using instance indices as features.\",\n        )\n\n    # Learned statistics\n    self._min_y = np.array([np.nan] * self._n_objectives)\n    self._max_y = np.array([np.nan] * self._n_objectives)\n    self._percentile = np.array([np.nan] * self._n_objectives)\n    self._multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None\n    self._runhistory: RunHistory | None = None\n</code></pre>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/encoder/#smac.runhistory.encoder.encoder.RunHistoryEncoder.transform_response_values","title":"transform_response_values","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Returns the input values.</p> Source code in <code>smac/runhistory/encoder/encoder.py</code> <pre><code>def transform_response_values(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Returns the input values.\"\"\"\n    return values\n</code></pre>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/","title":"Inverse scaled encoder","text":""},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder","title":"smac.runhistory.encoder.inverse_scaled_encoder","text":""},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder","title":"RunHistoryInverseScaledEncoder","text":"<pre><code>RunHistoryInverseScaledEncoder(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RunHistoryEncoder</code></p> Source code in <code>smac/runhistory/encoder/inverse_scaled_encoder.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n    if self._instances is not None and len(self._instances) &gt; 1:\n        raise NotImplementedError(\"Handling more than one instance is not supported for inverse scaled cost.\")\n</code></pre>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/inverse_scaled_encoder/#smac.runhistory.encoder.inverse_scaled_encoder.RunHistoryInverseScaledEncoder.transform_response_values","title":"transform_response_values","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Transform the response values by linearly scaling them between zero and one and then use inverse scaling.</p> Source code in <code>smac/runhistory/encoder/inverse_scaled_encoder.py</code> <pre><code>def transform_response_values(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transform the response values by linearly scaling\n    them between zero and one and then use inverse scaling.\n    \"\"\"\n    min_y = self._min_y - (\n        self._percentile - self._min_y\n    )  # Subtract the difference between the percentile and the minimum\n    min_y -= constants.VERY_SMALL_NUMBER  # Minimal value to avoid numerical issues in the log scaling below\n    # linear scaling\n    # prevent diving by zero\n\n    min_y[np.where(min_y == self._max_y)] *= 1 - 10**-10\n\n    values = (values - min_y) / (self._max_y - min_y)\n    values = 1 - 1 / values\n    return values\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_encoder/","title":"Log encoder","text":""},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder","title":"smac.runhistory.encoder.log_encoder","text":""},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder","title":"RunHistoryLogEncoder","text":"<pre><code>RunHistoryLogEncoder(\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>RunHistoryEncoder</code></p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n) -&gt; None:\n    if considered_states is None:\n        considered_states = [\n            StatusType.SUCCESS,\n            StatusType.CRASHED,\n            StatusType.MEMORYOUT,\n        ]\n\n    if seed is None:\n        seed = scenario.seed\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._scale_percentage = scale_percentage\n    self._n_objectives = scenario.count_objectives()\n    self._algorithm_walltime_limit = scenario.trial_walltime_limit\n    self._lower_budget_states = lower_budget_states if lower_budget_states is not None else []\n    self._considered_states = considered_states\n\n    self._instances = scenario.instances\n    self._instance_features = scenario.instance_features\n    self._n_features = scenario.count_instance_features()\n    self._n_params = len(list(scenario.configspace.values()))\n\n    if self._instances is not None and self._n_features == 0:\n        logger.warning(\n            \"We strongly encourage to use instance features when using instances.\",\n            \"If no instance features are passed, the runhistory encoder can not distinguish between different \"\n            \"instances and therefore returns the same data points with different values, all of which are \"\n            \"used to train the surrogate model.\\n\"\n            \"Consider using instance indices as features.\",\n        )\n\n    # Learned statistics\n    self._min_y = np.array([np.nan] * self._n_objectives)\n    self._max_y = np.array([np.nan] * self._n_objectives)\n    self._percentile = np.array([np.nan] * self._n_objectives)\n    self._multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None\n    self._runhistory: RunHistory | None = None\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_encoder/#smac.runhistory.encoder.log_encoder.RunHistoryLogEncoder.transform_response_values","title":"transform_response_values","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Transforms the response values by using log.</p> Source code in <code>smac/runhistory/encoder/log_encoder.py</code> <pre><code>def transform_response_values(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transforms the response values by using log.\"\"\"\n    # ensure that minimal value is larger than 0\n    if np.any(values &lt;= 0):\n        logger.warning(\n            \"Got cost of smaller/equal to 0. Replace by %f since we use\"\n            \" log cost.\" % constants.MINIMAL_COST_FOR_LOG\n        )\n        values[values &lt; constants.MINIMAL_COST_FOR_LOG] = constants.MINIMAL_COST_FOR_LOG\n\n    return np.log(values)\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/","title":"Log scaled encoder","text":""},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder","title":"smac.runhistory.encoder.log_scaled_encoder","text":""},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder","title":"RunHistoryLogScaledEncoder","text":"<pre><code>RunHistoryLogScaledEncoder(\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>RunHistoryEncoder</code></p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n) -&gt; None:\n    if considered_states is None:\n        considered_states = [\n            StatusType.SUCCESS,\n            StatusType.CRASHED,\n            StatusType.MEMORYOUT,\n        ]\n\n    if seed is None:\n        seed = scenario.seed\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._scale_percentage = scale_percentage\n    self._n_objectives = scenario.count_objectives()\n    self._algorithm_walltime_limit = scenario.trial_walltime_limit\n    self._lower_budget_states = lower_budget_states if lower_budget_states is not None else []\n    self._considered_states = considered_states\n\n    self._instances = scenario.instances\n    self._instance_features = scenario.instance_features\n    self._n_features = scenario.count_instance_features()\n    self._n_params = len(list(scenario.configspace.values()))\n\n    if self._instances is not None and self._n_features == 0:\n        logger.warning(\n            \"We strongly encourage to use instance features when using instances.\",\n            \"If no instance features are passed, the runhistory encoder can not distinguish between different \"\n            \"instances and therefore returns the same data points with different values, all of which are \"\n            \"used to train the surrogate model.\\n\"\n            \"Consider using instance indices as features.\",\n        )\n\n    # Learned statistics\n    self._min_y = np.array([np.nan] * self._n_objectives)\n    self._max_y = np.array([np.nan] * self._n_objectives)\n    self._percentile = np.array([np.nan] * self._n_objectives)\n    self._multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None\n    self._runhistory: RunHistory | None = None\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/log_scaled_encoder/#smac.runhistory.encoder.log_scaled_encoder.RunHistoryLogScaledEncoder.transform_response_values","title":"transform_response_values","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Transform the response values by linearly scaling them between zero and one and then using the log transformation.</p> Source code in <code>smac/runhistory/encoder/log_scaled_encoder.py</code> <pre><code>def transform_response_values(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transform the response values by linearly scaling them between zero and one and\n    then using the log transformation.\n    \"\"\"\n    min_y = self._min_y - (\n        self._percentile - self._min_y\n    )  # Subtract the difference between the percentile and the minimum\n    min_y -= constants.VERY_SMALL_NUMBER  # Minimal value to avoid numerical issues in the log scaling below\n\n    # Linear scaling\n    # prevent diving by zero\n    min_y[np.where(min_y == self._max_y)] *= 1 - 10**-10\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n        values = (values - min_y) / (self._max_y - min_y)\n        return np.log(values)\n</code></pre>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/","title":"Scaled encoder","text":""},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder","title":"smac.runhistory.encoder.scaled_encoder","text":""},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder","title":"RunHistoryScaledEncoder","text":"<pre><code>RunHistoryScaledEncoder(\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>RunHistoryEncoder</code></p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    considered_states: list[StatusType] = None,\n    lower_budget_states: list[StatusType] = None,\n    scale_percentage: int = 5,\n    seed: int | None = None,\n) -&gt; None:\n    if considered_states is None:\n        considered_states = [\n            StatusType.SUCCESS,\n            StatusType.CRASHED,\n            StatusType.MEMORYOUT,\n        ]\n\n    if seed is None:\n        seed = scenario.seed\n\n    self._seed = seed\n    self._rng = np.random.RandomState(seed)\n    self._scale_percentage = scale_percentage\n    self._n_objectives = scenario.count_objectives()\n    self._algorithm_walltime_limit = scenario.trial_walltime_limit\n    self._lower_budget_states = lower_budget_states if lower_budget_states is not None else []\n    self._considered_states = considered_states\n\n    self._instances = scenario.instances\n    self._instance_features = scenario.instance_features\n    self._n_features = scenario.count_instance_features()\n    self._n_params = len(list(scenario.configspace.values()))\n\n    if self._instances is not None and self._n_features == 0:\n        logger.warning(\n            \"We strongly encourage to use instance features when using instances.\",\n            \"If no instance features are passed, the runhistory encoder can not distinguish between different \"\n            \"instances and therefore returns the same data points with different values, all of which are \"\n            \"used to train the surrogate model.\\n\"\n            \"Consider using instance indices as features.\",\n        )\n\n    # Learned statistics\n    self._min_y = np.array([np.nan] * self._n_objectives)\n    self._max_y = np.array([np.nan] * self._n_objectives)\n    self._percentile = np.array([np.nan] * self._n_objectives)\n    self._multi_objective_algorithm: AbstractMultiObjectiveAlgorithm | None = None\n    self._runhistory: RunHistory | None = None\n</code></pre>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/scaled_encoder/#smac.runhistory.encoder.scaled_encoder.RunHistoryScaledEncoder.transform_response_values","title":"transform_response_values","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Transforms the response values by linearly scaling them between zero and one.</p> Source code in <code>smac/runhistory/encoder/scaled_encoder.py</code> <pre><code>def transform_response_values(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transforms the response values by linearly scaling them between zero and one.\"\"\"\n    min_y = self._min_y - (\n        self._percentile - self._min_y\n    )  # Subtract the difference between the percentile and the minimum\n    min_y -= constants.VERY_SMALL_NUMBER  # Minimal value to avoid numerical issues in the log scaling below\n\n    # Linear scaling\n    # prevent diving by zero\n    min_y[np.where(min_y == self._max_y)] *= 1 - 10**-101\n    values = (values - min_y) / (self._max_y - min_y)\n    return values\n</code></pre>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/","title":"Sqrt scaled encoder","text":""},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder","title":"smac.runhistory.encoder.sqrt_scaled_encoder","text":""},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder","title":"RunHistorySqrtScaledEncoder","text":"<pre><code>RunHistorySqrtScaledEncoder(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RunHistoryEncoder</code></p> Source code in <code>smac/runhistory/encoder/sqrt_scaled_encoder.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n    if self._instances is not None and len(self._instances) &gt; 1:\n        raise NotImplementedError(\"Handling more than one instance is not supported for sqrt scaled cost.\")\n</code></pre>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.meta--returns","title":"Returns","text":"<p>dict[str, Any]: meta-data of the created object: name, considered states, lower budget states, scale_percentage, seed.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.multi_objective_algorithm","title":"multi_objective_algorithm  <code>property</code> <code>writable</code>","text":"<pre><code>multi_objective_algorithm: (\n    AbstractMultiObjectiveAlgorithm | None\n)\n</code></pre> <p>The multi objective algorithm used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.runhistory","title":"runhistory  <code>property</code> <code>writable</code>","text":"<pre><code>runhistory: RunHistory\n</code></pre> <p>The RunHistory used to transform the data.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.get_configurations","title":"get_configurations","text":"<pre><code>get_configurations(\n    budget_subset: list | None = None,\n) -&gt; ndarray\n</code></pre> <p>Returns vector representation of the configurations.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.get_configurations--warning","title":"Warning","text":"<p>Instance features are not appended and cost values are not taken into account.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.get_configurations--parameters","title":"Parameters","text":"<p>budget_subset : list[int|float] | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.get_configurations--returns","title":"Returns","text":"<p>configs_array : np.ndarray</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def get_configurations(\n    self,\n    budget_subset: list | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Returns vector representation of the configurations.\n\n    Warning\n    -------\n    Instance features are not\n    appended and cost values are not taken into account.\n\n    Parameters\n    ----------\n    budget_subset : list[int|float] | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    configs_array : np.ndarray\n    \"\"\"\n    s_trials = self._get_considered_trials(budget_subset)\n    s_config_ids = set(s_trial.config_id for s_trial in s_trials)\n    t_trials = self._get_timeout_trials(budget_subset)\n    t_config_ids = set(t_trial.config_id for t_trial in t_trials)\n    config_ids = s_config_ids | t_config_ids\n    configurations = [self.runhistory._ids_config[config_id] for config_id in config_ids]\n    configs_array = convert_configurations_to_array(configurations)\n\n    return configs_array\n</code></pre>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.transform","title":"transform","text":"<pre><code>transform(\n    budget_subset: list | None = None,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a vector representation of the RunHistory.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.transform--parameters","title":"Parameters","text":"<p>budget_subset : list | None, defaults to none     List of budgets to consider.</p>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.transform--returns","title":"Returns","text":"<p>X : np.ndarray     Configuration vector and instance features. Y : np.ndarray     Cost values.</p> Source code in <code>smac/runhistory/encoder/abstract_encoder.py</code> <pre><code>def transform(\n    self,\n    budget_subset: list | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a vector representation of the RunHistory.\n\n    Parameters\n    ----------\n    budget_subset : list | None, defaults to none\n        List of budgets to consider.\n\n    Returns\n    -------\n    X : np.ndarray\n        Configuration vector and instance features.\n    Y : np.ndarray\n        Cost values.\n    \"\"\"\n    logger.debug(\"Transforming RunHistory into X, y format...\")\n\n    considered_trials = self._get_considered_trials(budget_subset)\n    X, Y = self._build_matrix(trials=considered_trials, store_statistics=True)\n\n    # Get real TIMEOUT runs\n    timeout_trials = self._get_timeout_trials(budget_subset)\n\n    # Use penalization (e.g. PAR10) for EPM training\n    store_statistics = True if np.any(np.isnan(self._min_y)) else False\n    tX, tY = self._build_matrix(trials=timeout_trials, store_statistics=store_statistics)\n\n    # If we don't have successful runs, we have to return all timeout runs\n    if not considered_trials:\n        return tX, tY\n\n    # If we do not impute, we also return TIMEOUT data\n    X = np.vstack((X, tX))\n    Y = np.concatenate((Y, tY))\n\n    logger.debug(\"Converted %d observations.\" % (X.shape[0]))\n    return X, Y\n</code></pre>"},{"location":"api/smac/runhistory/encoder/sqrt_scaled_encoder/#smac.runhistory.encoder.sqrt_scaled_encoder.RunHistorySqrtScaledEncoder.transform_response_values","title":"transform_response_values","text":"<pre><code>transform_response_values(values: ndarray) -&gt; ndarray\n</code></pre> <p>Transform the response values by linearly scaling them between zero and one and then using the square root.</p> Source code in <code>smac/runhistory/encoder/sqrt_scaled_encoder.py</code> <pre><code>def transform_response_values(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transform the response values by linearly scaling them between zero and one and then using the\n    square root.\n    \"\"\"\n    # Subtract the difference between the percentile and the minimum\n    min_y = self._min_y - (self._percentile - self._min_y)\n\n    # Minimal value to avoid numerical issues in the log scaling below\n    min_y -= constants.VERY_SMALL_NUMBER\n\n    # Linear scaling: prevent diving by zero\n    min_y[np.where(min_y == self._max_y)] *= 1 - 10**-10\n\n    values = (values - min_y) / (self._max_y - min_y)\n    values = np.sqrt(values)\n\n    return values\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/","title":"Abstract runner","text":""},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner","title":"smac.runner.abstract_runner","text":""},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner","title":"AbstractRunner","text":"<pre><code>AbstractRunner(\n    scenario: Scenario, required_arguments: list[str] = None\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Interface class to handle the execution of SMAC configurations. This interface defines how to interact with the SMBO loop. The complexity of running a configuration as well as handling the results is abstracted to the SMBO via an AbstractRunner.</p> <p>From SMBO perspective, launching a configuration follows a submit/collect scheme as follows:</p> <ol> <li> <p>A run is launched via <code>submit_run()</code></p> </li> <li> <p><code>submit_run</code> internally calls <code>run_wrapper()</code>, a method that contains common processing functions among      different runners.</p> </li> <li>A class that implements AbstractRunner defines <code>run()</code> which is really the algorithm to      translate a <code>TrialInfo</code> to a <code>TrialValue</code>, i.e. a configuration to an actual result.</li> <li>A completed run is collected via <code>iter_results()</code>, which iterates and consumes any finished runs, if any.</li> <li>This interface also offers the method <code>wait()</code> as a mechanism to make sure we have enough    data in the next iteration to make a decision. For example, the intensifier might not be    able to select the next challenger until more results are available.</li> </ol>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner--parameters","title":"Parameters","text":"<p>scenario : Scenario required_arguments : list[str]     A list of required arguments, which are passed to the target function.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    required_arguments: list[str] = None,\n):\n    if required_arguments is None:\n        required_arguments = []\n    self._scenario = scenario\n    self._required_arguments = required_arguments\n\n    # The results are a FIFO structure, implemented via a list\n    # (because the Queue lock is not pickable). Finished runs are\n    # put in this list and collected via _process_pending_runs\n    self._results_queue: list[tuple[TrialInfo, TrialValue]] = []\n    self._crash_cost = scenario.crash_cost\n    self._supports_memory_limit = False\n\n    if isinstance(scenario.objectives, str):\n        objectives = [scenario.objectives]\n    else:\n        objectives = scenario.objectives\n\n    self._objectives = objectives\n    self._n_objectives = scenario.count_objectives()\n\n    # We need to exapdn crash cost if the user did not do it\n    if self._n_objectives &gt; 1:\n        if not isinstance(scenario.crash_cost, list):\n            assert isinstance(scenario.crash_cost, float)\n            self._crash_cost = [scenario.crash_cost for _ in range(self._n_objectives)]\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.count_available_workers","title":"count_available_workers  <code>abstractmethod</code>","text":"<pre><code>count_available_workers() -&gt; int\n</code></pre> <p>Returns the number of available workers.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>@abstractmethod\ndef count_available_workers(self) -&gt; int:\n    \"\"\"Returns the number of available workers.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.is_running","title":"is_running  <code>abstractmethod</code>","text":"<pre><code>is_running() -&gt; bool\n</code></pre> <p>Whether there are trials still running.</p> <p>Generally, if the runner is serial, launching a trial instantly returns its result. On parallel runners, there might be pending configurations to complete.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>@abstractmethod\ndef is_running(self) -&gt; bool:\n    \"\"\"Whether there are trials still running.\n\n    Generally, if the runner is serial, launching a trial instantly returns its result. On\n    parallel runners, there might be pending configurations to complete.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.iter_results","title":"iter_results  <code>abstractmethod</code>","text":"<pre><code>iter_results() -&gt; Iterator[tuple[TrialInfo, TrialValue]]\n</code></pre> <p>This method returns any finished configuration, and returns a list with the results of executing the configurations. This class keeps populating results to <code>self._results_queue</code> until a call to <code>get_finished</code> trials is done. In this case, the <code>self._results_queue</code> list is emptied and all trial values produced by running <code>run</code> are returned.</p>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.iter_results--returns","title":"Returns","text":"<p>Iterator[tuple[TrialInfo, TrialValue]]:     A list of TrialInfo/TrialValue tuples, all of which have been finished.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>@abstractmethod\ndef iter_results(self) -&gt; Iterator[tuple[TrialInfo, TrialValue]]:\n    \"\"\"This method returns any finished configuration, and returns a list with the\n    results of executing the configurations. This class keeps populating results\n    to ``self._results_queue`` until a call to ``get_finished`` trials is done. In this case,\n    the `self._results_queue` list is emptied and all trial values produced by running\n    `run` are returned.\n\n    Returns\n    -------\n    Iterator[tuple[TrialInfo, TrialValue]]:\n        A list of TrialInfo/TrialValue tuples, all of which have been finished.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n) -&gt; tuple[\n    StatusType, float | list[float], float, float, dict\n]\n</code></pre> <p>Runs the target function with a configuration on a single instance-budget-seed combination (aka trial).</p>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.run--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to be passed to the target function. instance : str | None, defaults to None     The Problem instance. budget : float | None, defaults to None     A positive, real-valued number representing an arbitrary limit to the target function     handled by the target function internally. seed : int, defaults to None</p>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.run--returns","title":"Returns","text":"<p>status : StatusType     Status of the trial. cost : float | list[float]     Resulting cost(s) of the trial. runtime : float     The time the target function took to run. cpu_time : float     The time the target function took on hardware to run. additional_info : dict     All further additional trial information.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>@abstractmethod\ndef run(\n    self,\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n) -&gt; tuple[StatusType, float | list[float], float, float, dict]:\n    \"\"\"Runs the target function with a configuration on a single instance-budget-seed\n    combination (aka trial).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to be passed to the target function.\n    instance : str | None, defaults to None\n        The Problem instance.\n    budget : float | None, defaults to None\n        A positive, real-valued number representing an arbitrary limit to the target function\n        handled by the target function internally.\n    seed : int, defaults to None\n\n    Returns\n    -------\n    status : StatusType\n        Status of the trial.\n    cost : float | list[float]\n        Resulting cost(s) of the trial.\n    runtime : float\n        The time the target function took to run.\n    cpu_time : float\n        The time the target function took on hardware to run.\n    additional_info : dict\n        All further additional trial information.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.run_wrapper","title":"run_wrapper","text":"<pre><code>run_wrapper(\n    trial_info: TrialInfo,\n    **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]\n</code></pre> <p>Wrapper around run() to execute and check the execution of a given config. This function encapsulates common handling/processing, so that run() implementation is simplified.</p>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.run_wrapper--parameters","title":"Parameters","text":"<p>trial_info : RunInfo     Object that contains enough information to execute a configuration run in isolation. dask_data_to_scatter: dict[str, Any]     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.run_wrapper--returns","title":"Returns","text":"<p>info : TrialInfo     An object containing the configuration launched. value : TrialValue     Contains information about the status/performance of config.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>def run_wrapper(\n    self, trial_info: TrialInfo, **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]:\n    \"\"\"Wrapper around run() to execute and check the execution of a given config.\n    This function encapsulates common\n    handling/processing, so that run() implementation is simplified.\n\n    Parameters\n    ----------\n    trial_info : RunInfo\n        Object that contains enough information to execute a configuration run in isolation.\n    dask_data_to_scatter: dict[str, Any]\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    info : TrialInfo\n        An object containing the configuration launched.\n    value : TrialValue\n        Contains information about the status/performance of config.\n    \"\"\"\n    start = time.time()\n    cpu_time = time.process_time()\n    try:\n        status, cost, runtime, cpu_time, additional_info = self.run(\n            config=trial_info.config,\n            instance=trial_info.instance,\n            budget=trial_info.budget,\n            seed=trial_info.seed,\n            **dask_data_to_scatter,\n        )\n    except Exception as e:\n        status = StatusType.CRASHED\n        cost = self._crash_cost\n        cpu_time = time.process_time() - cpu_time\n        runtime = time.time() - start\n\n        # Add context information to the error message\n        exception_traceback = traceback.format_exc()\n        error_message = repr(e)\n        additional_info = {\n            \"traceback\": exception_traceback,\n            \"error\": error_message,\n        }\n\n    end = time.time()\n\n    # Catch NaN or inf\n    if not np.all(np.isfinite(cost)):\n        logger.warning(\n            \"Target function returned infinity or nothing at all. Result is treated as CRASHED\"\n            f\" and cost is set to {self._crash_cost}.\"\n        )\n\n        if \"traceback\" in additional_info:\n            logger.warning(f\"Traceback: {additional_info['traceback']}\\n\")\n\n        status = StatusType.CRASHED\n\n    if status == StatusType.CRASHED:\n        cost = self._crash_cost\n\n    trial_value = TrialValue(\n        status=status,\n        cost=cost,\n        time=runtime,\n        cpu_time=cpu_time,\n        additional_info=additional_info,\n        starttime=start,\n        endtime=end,\n    )\n\n    return trial_info, trial_value\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.submit_trial","title":"submit_trial  <code>abstractmethod</code>","text":"<pre><code>submit_trial(trial_info: TrialInfo) -&gt; None\n</code></pre> <p>This function submits a configuration embedded in a TrialInfo object, and uses one of the workers to produce a result (such result will eventually be available on the <code>self._results_queue</code> FIFO).</p> <p>This interface method will be called by SMBO, with the expectation that a function will be executed by a worker. What will be executed is dictated by <code>trial_info</code>, and <code>how</code> it will be executed is decided via the child class that implements a <code>run</code> method.</p> <p>Because config submission can be a serial/parallel endeavor, it is expected to be implemented by a child class.</p>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.submit_trial--parameters","title":"Parameters","text":"<p>trial_info : TrialInfo     An object containing the configuration launched.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>@abstractmethod\ndef submit_trial(self, trial_info: TrialInfo) -&gt; None:\n    \"\"\"This function submits a configuration embedded in a TrialInfo object, and uses one of the workers to produce\n    a result (such result will eventually be available on the ``self._results_queue`` FIFO).\n\n    This interface method will be called by SMBO, with the expectation that a function will be executed by a worker.\n    What will be executed is dictated by ``trial_info``, and `how` it will be executed is decided via the child\n    class that implements a ``run`` method.\n\n    Because config submission can be a serial/parallel endeavor, it is expected to be implemented by a child class.\n\n    Parameters\n    ----------\n    trial_info : TrialInfo\n        An object containing the configuration launched.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runner/abstract_runner/#smac.runner.abstract_runner.AbstractRunner.wait","title":"wait  <code>abstractmethod</code>","text":"<pre><code>wait() -&gt; None\n</code></pre> <p>The SMBO/intensifier might need to wait for trials to finish before making a decision.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>@abstractmethod\ndef wait(self) -&gt; None:\n    \"\"\"The SMBO/intensifier might need to wait for trials to finish before making a decision.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runner/abstract_serial_runner/","title":"Abstract serial runner","text":""},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner","title":"smac.runner.abstract_serial_runner","text":""},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner","title":"AbstractSerialRunner","text":"<pre><code>AbstractSerialRunner(\n    scenario: Scenario, required_arguments: list[str] = None\n)\n</code></pre> <p>               Bases: <code>AbstractRunner</code></p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    required_arguments: list[str] = None,\n):\n    if required_arguments is None:\n        required_arguments = []\n    self._scenario = scenario\n    self._required_arguments = required_arguments\n\n    # The results are a FIFO structure, implemented via a list\n    # (because the Queue lock is not pickable). Finished runs are\n    # put in this list and collected via _process_pending_runs\n    self._results_queue: list[tuple[TrialInfo, TrialValue]] = []\n    self._crash_cost = scenario.crash_cost\n    self._supports_memory_limit = False\n\n    if isinstance(scenario.objectives, str):\n        objectives = [scenario.objectives]\n    else:\n        objectives = scenario.objectives\n\n    self._objectives = objectives\n    self._n_objectives = scenario.count_objectives()\n\n    # We need to exapdn crash cost if the user did not do it\n    if self._n_objectives &gt; 1:\n        if not isinstance(scenario.crash_cost, list):\n            assert isinstance(scenario.crash_cost, float)\n            self._crash_cost = [scenario.crash_cost for _ in range(self._n_objectives)]\n</code></pre>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.count_available_workers","title":"count_available_workers","text":"<pre><code>count_available_workers() -&gt; int\n</code></pre> <p>Returns the number of available workers. Serial workers only have one worker.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def count_available_workers(self) -&gt; int:\n    \"\"\"Returns the number of available workers. Serial workers only have one worker.\"\"\"\n    return 1\n</code></pre>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n) -&gt; tuple[\n    StatusType, float | list[float], float, float, dict\n]\n</code></pre> <p>Runs the target function with a configuration on a single instance-budget-seed combination (aka trial).</p>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.run--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to be passed to the target function. instance : str | None, defaults to None     The Problem instance. budget : float | None, defaults to None     A positive, real-valued number representing an arbitrary limit to the target function     handled by the target function internally. seed : int, defaults to None</p>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.run--returns","title":"Returns","text":"<p>status : StatusType     Status of the trial. cost : float | list[float]     Resulting cost(s) of the trial. runtime : float     The time the target function took to run. cpu_time : float     The time the target function took on hardware to run. additional_info : dict     All further additional trial information.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>@abstractmethod\ndef run(\n    self,\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n) -&gt; tuple[StatusType, float | list[float], float, float, dict]:\n    \"\"\"Runs the target function with a configuration on a single instance-budget-seed\n    combination (aka trial).\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to be passed to the target function.\n    instance : str | None, defaults to None\n        The Problem instance.\n    budget : float | None, defaults to None\n        A positive, real-valued number representing an arbitrary limit to the target function\n        handled by the target function internally.\n    seed : int, defaults to None\n\n    Returns\n    -------\n    status : StatusType\n        Status of the trial.\n    cost : float | list[float]\n        Resulting cost(s) of the trial.\n    runtime : float\n        The time the target function took to run.\n    cpu_time : float\n        The time the target function took on hardware to run.\n    additional_info : dict\n        All further additional trial information.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.run_wrapper","title":"run_wrapper","text":"<pre><code>run_wrapper(\n    trial_info: TrialInfo,\n    **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]\n</code></pre> <p>Wrapper around run() to execute and check the execution of a given config. This function encapsulates common handling/processing, so that run() implementation is simplified.</p>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.run_wrapper--parameters","title":"Parameters","text":"<p>trial_info : RunInfo     Object that contains enough information to execute a configuration run in isolation. dask_data_to_scatter: dict[str, Any]     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.run_wrapper--returns","title":"Returns","text":"<p>info : TrialInfo     An object containing the configuration launched. value : TrialValue     Contains information about the status/performance of config.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>def run_wrapper(\n    self, trial_info: TrialInfo, **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]:\n    \"\"\"Wrapper around run() to execute and check the execution of a given config.\n    This function encapsulates common\n    handling/processing, so that run() implementation is simplified.\n\n    Parameters\n    ----------\n    trial_info : RunInfo\n        Object that contains enough information to execute a configuration run in isolation.\n    dask_data_to_scatter: dict[str, Any]\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    info : TrialInfo\n        An object containing the configuration launched.\n    value : TrialValue\n        Contains information about the status/performance of config.\n    \"\"\"\n    start = time.time()\n    cpu_time = time.process_time()\n    try:\n        status, cost, runtime, cpu_time, additional_info = self.run(\n            config=trial_info.config,\n            instance=trial_info.instance,\n            budget=trial_info.budget,\n            seed=trial_info.seed,\n            **dask_data_to_scatter,\n        )\n    except Exception as e:\n        status = StatusType.CRASHED\n        cost = self._crash_cost\n        cpu_time = time.process_time() - cpu_time\n        runtime = time.time() - start\n\n        # Add context information to the error message\n        exception_traceback = traceback.format_exc()\n        error_message = repr(e)\n        additional_info = {\n            \"traceback\": exception_traceback,\n            \"error\": error_message,\n        }\n\n    end = time.time()\n\n    # Catch NaN or inf\n    if not np.all(np.isfinite(cost)):\n        logger.warning(\n            \"Target function returned infinity or nothing at all. Result is treated as CRASHED\"\n            f\" and cost is set to {self._crash_cost}.\"\n        )\n\n        if \"traceback\" in additional_info:\n            logger.warning(f\"Traceback: {additional_info['traceback']}\\n\")\n\n        status = StatusType.CRASHED\n\n    if status == StatusType.CRASHED:\n        cost = self._crash_cost\n\n    trial_value = TrialValue(\n        status=status,\n        cost=cost,\n        time=runtime,\n        cpu_time=cpu_time,\n        additional_info=additional_info,\n        starttime=start,\n        endtime=end,\n    )\n\n    return trial_info, trial_value\n</code></pre>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.submit_trial","title":"submit_trial","text":"<pre><code>submit_trial(trial_info: TrialInfo) -&gt; None\n</code></pre> <p>This function submits a trial_info object in a serial fashion. As there is a single  worker for this task, this interface can be considered a wrapper over the <code>run</code> method.</p> <p>Both result/exceptions can be completely determined in this step so both lists are properly filled.</p>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.submit_trial--parameters","title":"Parameters","text":"<p>trial_info : TrialInfo     An object containing the configuration launched.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def submit_trial(self, trial_info: TrialInfo) -&gt; None:\n    \"\"\"This function submits a trial_info object in a serial fashion. As there is a single\n     worker for this task, this interface can be considered a wrapper over the `run` method.\n\n    Both result/exceptions can be completely determined in this step so both lists\n    are properly filled.\n\n    Parameters\n    ----------\n    trial_info : TrialInfo\n        An object containing the configuration launched.\n    \"\"\"\n    self._results_queue.append(self.run_wrapper(trial_info))\n</code></pre>"},{"location":"api/smac/runner/abstract_serial_runner/#smac.runner.abstract_serial_runner.AbstractSerialRunner.wait","title":"wait","text":"<pre><code>wait() -&gt; None\n</code></pre> <p>The SMBO/intensifier might need to wait for trials to finish before making a decision. For serial runners, no wait is needed as the result is immediately available.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def wait(self) -&gt; None:\n    \"\"\"The SMBO/intensifier might need to wait for trials to finish before making a decision.\n    For serial runners, no wait is needed as the result is immediately available.\n    \"\"\"\n    # There is no need to wait in serial runners. When launching a trial via submit, as\n    # the serial trial uses the same process to run, the result is always available\n    # immediately after. This method implements is just an implementation of the\n    # abstract method via a simple return, again, because there is no need to wait\n    return\n</code></pre>"},{"location":"api/smac/runner/dask_runner/","title":"Dask runner","text":""},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner","title":"smac.runner.dask_runner","text":""},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner","title":"DaskParallelRunner","text":"<pre><code>DaskParallelRunner(\n    single_worker: AbstractRunner,\n    patience: int = 5,\n    dask_client: Client | None = None,\n)\n</code></pre> <p>               Bases: <code>AbstractRunner</code></p> <p>Interface to submit and collect a job in a distributed fashion. DaskParallelRunner is intended to comply with the bridge design pattern. Nevertheless, to reduce the amount of code within single-vs-parallel implementations, DaskParallelRunner wraps a BaseRunner object which is then executed in parallel on <code>n_workers</code>.</p> <p>This class then is constructed by passing an AbstractRunner that implements a <code>run</code> method, and is capable of doing so in a serial fashion. Next, this wrapper class uses dask to initialize <code>N</code> number of AbstractRunner that actively wait of a TrialInfo to produce a RunInfo object.</p> <p>To be more precise, the work model is then:</p> <ol> <li>The intensifier dictates \"what\" to run (a configuration/instance/seed) via a TrialInfo object.</li> <li>An abstract runner takes this TrialInfo object and launches the task via    <code>submit_run</code>. In the case of DaskParallelRunner, <code>n_workers</code> receive a pickle-object of    <code>DaskParallelRunner.single_worker</code>, each with a <code>run</code> method coming from    <code>DaskParallelRunner.single_worker.run()</code></li> <li>TrialInfo objects are run in a distributed fashion, and their results are available locally to each worker. The    result is collected by <code>iter_results</code> and then passed to SMBO.</li> <li>Exceptions are also locally available to each worker and need to be collected.</li> </ol> <p>Dask works with <code>Future</code> object which are managed via the DaskParallelRunner.client.</p>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner--parameters","title":"Parameters","text":"<p>single_worker : AbstractRunner     A runner to run in a distributed fashion. Will be distributed using <code>n_workers</code>. patience: int, default to 5     How much to wait for workers (seconds) to be available if one fails. dask_client: Client | None, defaults to None     User-created dask client, which can be used to start a dask cluster and then attach SMAC to it. This will not     be closed automatically and will have to be closed manually if provided explicitly. If none is provided     (default), a local one will be created for you and closed upon completion.</p> Source code in <code>smac/runner/dask_runner.py</code> <pre><code>def __init__(\n    self,\n    single_worker: AbstractRunner,\n    patience: int = 5,\n    dask_client: Client | None = None,\n):\n    super().__init__(\n        scenario=single_worker._scenario,\n        required_arguments=single_worker._required_arguments,\n    )\n\n    # The single worker to hold on to and call run on\n    self._single_worker = single_worker\n\n    # The list of futures that dask will use to indicate in progress runs\n    self._pending_trials: list[Future] = []\n\n    # Dask related variables\n    self._scheduler_file: Path | None = None\n    self._patience = patience\n\n    self._client: Client\n    self._close_client_at_del: bool\n\n    if dask_client is None:\n        dask.config.set({\"distributed.worker.daemon\": False})\n        self._close_client_at_del = True\n        self._client = Client(\n            n_workers=self._scenario.n_workers,\n            processes=True,\n            threads_per_worker=1,\n            local_directory=str(self._scenario.output_directory),\n        )\n\n        if self._scenario.output_directory is not None:\n            self._scheduler_file = Path(self._scenario.output_directory, \".dask_scheduler_file\")\n            self._client.write_scheduler_file(scheduler_file=str(self._scheduler_file))\n    else:\n        # We just use their set up\n        self._client = dask_client\n        self._close_client_at_del = False\n</code></pre>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Returns the meta-data of the created object.</p>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> <p>Makes sure that when this object gets deleted, the client is terminated. This is only done if the client was created by the dask runner.</p> Source code in <code>smac/runner/dask_runner.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Makes sure that when this object gets deleted, the client is terminated. This\n    is only done if the client was created by the dask runner.\n    \"\"\"\n    if self._close_client_at_del:\n        self.close()\n</code></pre>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.close","title":"close","text":"<pre><code>close(force: bool = False) -&gt; None\n</code></pre> <p>Closes the client.</p> Source code in <code>smac/runner/dask_runner.py</code> <pre><code>def close(self, force: bool = False) -&gt; None:\n    \"\"\"Closes the client.\"\"\"\n    if self._close_client_at_del or force:\n        self._client.close()\n</code></pre>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.count_available_workers","title":"count_available_workers","text":"<pre><code>count_available_workers() -&gt; int\n</code></pre> <p>Total number of workers available. This number is dynamic as more resources can be allocated.</p> Source code in <code>smac/runner/dask_runner.py</code> <pre><code>def count_available_workers(self) -&gt; int:\n    \"\"\"Total number of workers available. This number is dynamic as more resources\n    can be allocated.\n    \"\"\"\n    return sum(self._client.nthreads().values()) - len(self._pending_trials)\n</code></pre>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.run_wrapper","title":"run_wrapper","text":"<pre><code>run_wrapper(\n    trial_info: TrialInfo,\n    **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]\n</code></pre> <p>Wrapper around run() to execute and check the execution of a given config. This function encapsulates common handling/processing, so that run() implementation is simplified.</p>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.run_wrapper--parameters","title":"Parameters","text":"<p>trial_info : RunInfo     Object that contains enough information to execute a configuration run in isolation. dask_data_to_scatter: dict[str, Any]     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.run_wrapper--returns","title":"Returns","text":"<p>info : TrialInfo     An object containing the configuration launched. value : TrialValue     Contains information about the status/performance of config.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>def run_wrapper(\n    self, trial_info: TrialInfo, **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]:\n    \"\"\"Wrapper around run() to execute and check the execution of a given config.\n    This function encapsulates common\n    handling/processing, so that run() implementation is simplified.\n\n    Parameters\n    ----------\n    trial_info : RunInfo\n        Object that contains enough information to execute a configuration run in isolation.\n    dask_data_to_scatter: dict[str, Any]\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    info : TrialInfo\n        An object containing the configuration launched.\n    value : TrialValue\n        Contains information about the status/performance of config.\n    \"\"\"\n    start = time.time()\n    cpu_time = time.process_time()\n    try:\n        status, cost, runtime, cpu_time, additional_info = self.run(\n            config=trial_info.config,\n            instance=trial_info.instance,\n            budget=trial_info.budget,\n            seed=trial_info.seed,\n            **dask_data_to_scatter,\n        )\n    except Exception as e:\n        status = StatusType.CRASHED\n        cost = self._crash_cost\n        cpu_time = time.process_time() - cpu_time\n        runtime = time.time() - start\n\n        # Add context information to the error message\n        exception_traceback = traceback.format_exc()\n        error_message = repr(e)\n        additional_info = {\n            \"traceback\": exception_traceback,\n            \"error\": error_message,\n        }\n\n    end = time.time()\n\n    # Catch NaN or inf\n    if not np.all(np.isfinite(cost)):\n        logger.warning(\n            \"Target function returned infinity or nothing at all. Result is treated as CRASHED\"\n            f\" and cost is set to {self._crash_cost}.\"\n        )\n\n        if \"traceback\" in additional_info:\n            logger.warning(f\"Traceback: {additional_info['traceback']}\\n\")\n\n        status = StatusType.CRASHED\n\n    if status == StatusType.CRASHED:\n        cost = self._crash_cost\n\n    trial_value = TrialValue(\n        status=status,\n        cost=cost,\n        time=runtime,\n        cpu_time=cpu_time,\n        additional_info=additional_info,\n        starttime=start,\n        endtime=end,\n    )\n\n    return trial_info, trial_value\n</code></pre>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.submit_trial","title":"submit_trial","text":"<pre><code>submit_trial(\n    trial_info: TrialInfo,\n    **dask_data_to_scatter: dict[str, Any]\n) -&gt; None\n</code></pre> <p>This function submits a configuration embedded in a <code>trial_info</code> object, and uses one of the workers to produce a result locally to each worker.</p> <p>The execution of a configuration follows this procedure:</p>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.submit_trial--the-smbointensifier-generates-a-trialinfo","title":". The SMBO/intensifier generates a <code>TrialInfo</code>.","text":""},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.submit_trial--smbo-calls-submit_trial-so-that-a-worker-launches-the-trial_info","title":". SMBO calls <code>submit_trial</code> so that a worker launches the <code>trial_info</code>.","text":""},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.submit_trial--submit_trial-internally-calls-selfrun-it-does-so-via-a-call-to-run_wrapper-which-contains-common","title":". <code>submit_trial</code> internally calls <code>self.run()</code>. It does so via a call to <code>run_wrapper</code> which contains common","text":"<p>code that any <code>run</code> method will otherwise have to implement.</p> <p>All results will be only available locally to each worker, so the main node needs to collect them.</p>"},{"location":"api/smac/runner/dask_runner/#smac.runner.dask_runner.DaskParallelRunner.submit_trial--parameters","title":"Parameters","text":"<p>trial_info : TrialInfo     An object containing the configuration launched.</p> dict[str, Any] <p>When a user scatters data from their local process to the distributed network, this data is distributed in a round-robin fashion grouping by number of cores. Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data every time we would like to execute a target function with a big dataset. For example, when your target function has a big dataset shared across all the target function, this argument is very useful.</p> Source code in <code>smac/runner/dask_runner.py</code> <pre><code>def submit_trial(self, trial_info: TrialInfo, **dask_data_to_scatter: dict[str, Any]) -&gt; None:\n    \"\"\"This function submits a configuration embedded in a ``trial_info`` object, and uses one of\n    the workers to produce a result locally to each worker.\n\n    The execution of a configuration follows this procedure:\n\n    #. The SMBO/intensifier generates a `TrialInfo`.\n    #. SMBO calls `submit_trial` so that a worker launches the `trial_info`.\n    #. `submit_trial` internally calls ``self.run()``. It does so via a call to `run_wrapper` which contains common\n       code that any `run` method will otherwise have to implement.\n\n    All results will be only available locally to each worker, so the main node needs to collect them.\n\n    Parameters\n    ----------\n    trial_info : TrialInfo\n        An object containing the configuration launched.\n\n    dask_data_to_scatter: dict[str, Any]\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n    \"\"\"\n    # Check for resources or block till one is available\n    if self.count_available_workers() &lt;= 0:\n        logger.debug(\"No worker available. Waiting for one to be available...\")\n        wait(self._pending_trials, return_when=\"FIRST_COMPLETED\")\n        self._process_pending_trials()\n\n    # Check again to make sure that there are resources\n    if self.count_available_workers() &lt;= 0:\n        logger.warning(\"No workers are available. This could mean workers crashed. Waiting for new workers...\")\n        time.sleep(self._patience)\n        if self.count_available_workers() &lt;= 0:\n            raise RuntimeError(\n                \"Tried to execute a job, but no worker was ever available.\"\n                \"This likely means that a worker crashed or no workers were properly configured.\"\n            )\n\n    # At this point we can submit the job\n    trial = self._client.submit(self._single_worker.run_wrapper, trial_info=trial_info, **dask_data_to_scatter)\n    self._pending_trials.append(trial)\n</code></pre>"},{"location":"api/smac/runner/exceptions/","title":"Exceptions","text":""},{"location":"api/smac/runner/exceptions/#smac.runner.exceptions","title":"smac.runner.exceptions","text":""},{"location":"api/smac/runner/exceptions/#smac.runner.exceptions.FirstRunCrashedException","title":"FirstRunCrashedException","text":"<p>               Bases: <code>TargetAlgorithmAbortException</code></p> <p>Exception indicating that the first run crashed (depending on options this could trigger an ABORT of SMAC).</p>"},{"location":"api/smac/runner/exceptions/#smac.runner.exceptions.TargetAlgorithmAbortException","title":"TargetAlgorithmAbortException","text":"<p>               Bases: <code>Exception</code></p> <p>Exception indicating that the target function suggests an ABORT of SMAC, usually because it assumes that all further runs will surely fail.</p>"},{"location":"api/smac/runner/target_function_runner/","title":"Target function runner","text":""},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner","title":"smac.runner.target_function_runner","text":""},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner","title":"TargetFunctionRunner","text":"<pre><code>TargetFunctionRunner(\n    scenario: Scenario,\n    target_function: Callable,\n    required_arguments: list[str] = None,\n)\n</code></pre> <p>               Bases: <code>AbstractSerialRunner</code></p> <p>Class to execute target functions which are python functions. Evaluates function for given configuration and resource limit.</p> <p>The target function can either return a float (the loss), or a tuple with the first element being a float and the second being additional run information. In a multi-objective setting, the float value is replaced by a list of floats.</p>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner--parameters","title":"Parameters","text":"<p>target_function : Callable     The target function. scenario : Scenario required_arguments : list[str], defaults to []     A list of required arguments, which are passed to the target function.</p> Source code in <code>smac/runner/target_function_runner.py</code> <pre><code>def __init__(\n    self,\n    scenario: Scenario,\n    target_function: Callable,\n    required_arguments: list[str] = None,\n):\n    if required_arguments is None:\n        required_arguments = []\n    super().__init__(scenario=scenario, required_arguments=required_arguments)\n    self._target_function = target_function\n\n    # Check if target function is callable\n    if not callable(self._target_function):\n        raise TypeError(\n            \"Argument `target_function` must be a callable but is type\" f\"`{type(self._target_function)}`.\"\n        )\n\n    # Signatures here\n    signature = inspect.signature(self._target_function).parameters\n    for argument in required_arguments:\n        if argument not in signature.keys():\n            raise RuntimeError(\n                f\"Target function needs to have the arguments {required_arguments} \"\n                f\"but could not find {argument}.\"\n            )\n\n    # Now we check for additional arguments which are not used by SMAC\n    # However, we only want to warn the user and not\n    for key in list(signature.keys())[1:]:\n        if key not in required_arguments:\n            logger.warning(f\"The argument {key} is not set by SMAC: Consider removing it from the target function.\")\n\n    # Pynisher limitations\n    if (memory := self._scenario.trial_memory_limit) is not None:\n        unit = None\n        if isinstance(memory, (tuple, list)):\n            memory, unit = memory\n        memory = int(math.ceil(memory))\n        if unit is not None:\n            memory = (memory, unit)\n\n    if (time := self._scenario.trial_walltime_limit) is not None:\n        time = int(math.ceil(time))\n\n    self._memory_limit = memory\n    self._algorithm_walltime_limit = time\n</code></pre>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.__call__","title":"__call__","text":"<pre><code>__call__(\n    config: Configuration,\n    algorithm: Callable,\n    algorithm_kwargs: dict[str, Any],\n) -&gt; (\n    float\n    | list[float]\n    | dict[str, float]\n    | tuple[float, dict]\n    | tuple[list[float], dict]\n    | tuple[dict[str, float], dict]\n)\n</code></pre> <p>Calls the algorithm, which is processed in the <code>run</code> method.</p> Source code in <code>smac/runner/target_function_runner.py</code> <pre><code>def __call__(\n    self,\n    config: Configuration,\n    algorithm: Callable,\n    algorithm_kwargs: dict[str, Any],\n) -&gt; (\n    float\n    | list[float]\n    | dict[str, float]\n    | tuple[float, dict]\n    | tuple[list[float], dict]\n    | tuple[dict[str, float], dict]\n):\n    \"\"\"Calls the algorithm, which is processed in the ``run`` method.\"\"\"\n    return algorithm(config, **algorithm_kwargs)\n</code></pre>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.count_available_workers","title":"count_available_workers","text":"<pre><code>count_available_workers() -&gt; int\n</code></pre> <p>Returns the number of available workers. Serial workers only have one worker.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def count_available_workers(self) -&gt; int:\n    \"\"\"Returns the number of available workers. Serial workers only have one worker.\"\"\"\n    return 1\n</code></pre>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.run","title":"run","text":"<pre><code>run(\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n    **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[\n    StatusType, float | list[float], float, float, dict\n]\n</code></pre> <p>Calls the target function with pynisher if algorithm wall time limit or memory limit is set. Otherwise, the function is called directly.</p>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.run--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to be passed to the target function. instance : str | None, defaults to None     The Problem instance. budget : float | None, defaults to None     A positive, real-valued number representing an arbitrary limit to the target function     handled by the target function internally. seed : int, defaults to None dask_data_to_scatter: dict[str, Any]     This kwargs must be empty when we do not use dask! ()     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.run--returns","title":"Returns","text":"<p>status : StatusType     Status of the trial. cost : float | list[float]     Resulting cost(s) of the trial. runtime : float     The time the target function took to run. cpu_time : float     The time the target function took on the hardware to run. additional_info : dict     All further additional trial information.</p> Source code in <code>smac/runner/target_function_runner.py</code> <pre><code>def run(\n    self,\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n    **dask_data_to_scatter: dict[str, Any],\n) -&gt; tuple[StatusType, float | list[float], float, float, dict]:\n    \"\"\"Calls the target function with pynisher if algorithm wall time limit or memory limit is\n    set. Otherwise, the function is called directly.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to be passed to the target function.\n    instance : str | None, defaults to None\n        The Problem instance.\n    budget : float | None, defaults to None\n        A positive, real-valued number representing an arbitrary limit to the target function\n        handled by the target function internally.\n    seed : int, defaults to None\n    dask_data_to_scatter: dict[str, Any]\n        This kwargs must be empty when we do not use dask! ()\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    status : StatusType\n        Status of the trial.\n    cost : float | list[float]\n        Resulting cost(s) of the trial.\n    runtime : float\n        The time the target function took to run.\n    cpu_time : float\n        The time the target function took on the hardware to run.\n    additional_info : dict\n        All further additional trial information.\n    \"\"\"\n    # The kwargs are passed to the target function.\n    kwargs: dict[str, Any] = {}\n    kwargs.update(dask_data_to_scatter)\n\n    if \"seed\" in self._required_arguments:\n        kwargs[\"seed\"] = seed\n\n    if \"instance\" in self._required_arguments:\n        kwargs[\"instance\"] = instance\n\n    if \"budget\" in self._required_arguments:\n        kwargs[\"budget\"] = budget\n\n    # Presetting\n    cost: float | list[float] = self._crash_cost\n    runtime = 0.0\n    cpu_time = runtime\n    additional_info = {}\n    status = StatusType.CRASHED\n\n    # If memory limit or walltime limit is set, we wanna use pynisher\n    target_function: Callable\n    if self._memory_limit is not None or self._algorithm_walltime_limit is not None:\n        target_function = limit(\n            self._target_function,\n            memory=self._memory_limit,\n            wall_time=self._algorithm_walltime_limit,\n            wrap_errors=True,  # Hard to describe; see https://github.com/automl/pynisher\n        )\n    else:\n        target_function = self._target_function\n\n    # We don't want the user to change the configuration\n    config_copy = copy.deepcopy(config)\n\n    # Call target function\n    try:\n        start_time = time.time()\n        cpu_time = time.process_time()\n        rval = self(config_copy, target_function, kwargs)\n        cpu_time = time.process_time() - cpu_time\n        runtime = time.time() - start_time\n        status = StatusType.SUCCESS\n    except WallTimeoutException:\n        status = StatusType.TIMEOUT\n    except MemoryLimitException:\n        status = StatusType.MEMORYOUT\n    except Exception as e:\n        cost = np.asarray(cost).squeeze().tolist()\n        additional_info = {\n            \"traceback\": traceback.format_exc(),\n            \"error\": repr(e),\n        }\n        status = StatusType.CRASHED\n\n    if status != StatusType.SUCCESS:\n        return status, cost, runtime, cpu_time, additional_info\n\n    if isinstance(rval, tuple):\n        result, additional_info = rval\n    else:\n        result, additional_info = rval, {}\n\n    # Do some sanity checking (for multi objective)\n    error = f\"Returned costs {result} does not match the number of objectives {self._objectives}.\"\n\n    # If dict convert to array and make sure the order is correct\n    if isinstance(result, dict):\n        if len(result) != len(self._objectives):\n            raise RuntimeError(error)\n\n        ordered_cost: list[float] = []\n        for name in self._objectives:\n            if name not in result:\n                raise RuntimeError(f\"Objective {name} was not found in the returned costs.\")  # noqa: E713\n\n            ordered_cost.append(result[name])\n\n        result = ordered_cost\n\n    if isinstance(result, list):\n        if len(result) != len(self._objectives):\n            raise RuntimeError(error)\n\n    if isinstance(result, float):\n        if isinstance(self._objectives, list) and len(self._objectives) != 1:\n            raise RuntimeError(error)\n\n    cost = result\n\n    if cost is None:\n        status = StatusType.CRASHED\n        cost = self._crash_cost\n\n    # We want to get either a float or a list of floats.\n    cost = np.asarray(cost).squeeze().tolist()\n\n    return status, cost, runtime, cpu_time, additional_info\n</code></pre>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.run_wrapper","title":"run_wrapper","text":"<pre><code>run_wrapper(\n    trial_info: TrialInfo,\n    **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]\n</code></pre> <p>Wrapper around run() to execute and check the execution of a given config. This function encapsulates common handling/processing, so that run() implementation is simplified.</p>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.run_wrapper--parameters","title":"Parameters","text":"<p>trial_info : RunInfo     Object that contains enough information to execute a configuration run in isolation. dask_data_to_scatter: dict[str, Any]     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.run_wrapper--returns","title":"Returns","text":"<p>info : TrialInfo     An object containing the configuration launched. value : TrialValue     Contains information about the status/performance of config.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>def run_wrapper(\n    self, trial_info: TrialInfo, **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]:\n    \"\"\"Wrapper around run() to execute and check the execution of a given config.\n    This function encapsulates common\n    handling/processing, so that run() implementation is simplified.\n\n    Parameters\n    ----------\n    trial_info : RunInfo\n        Object that contains enough information to execute a configuration run in isolation.\n    dask_data_to_scatter: dict[str, Any]\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    info : TrialInfo\n        An object containing the configuration launched.\n    value : TrialValue\n        Contains information about the status/performance of config.\n    \"\"\"\n    start = time.time()\n    cpu_time = time.process_time()\n    try:\n        status, cost, runtime, cpu_time, additional_info = self.run(\n            config=trial_info.config,\n            instance=trial_info.instance,\n            budget=trial_info.budget,\n            seed=trial_info.seed,\n            **dask_data_to_scatter,\n        )\n    except Exception as e:\n        status = StatusType.CRASHED\n        cost = self._crash_cost\n        cpu_time = time.process_time() - cpu_time\n        runtime = time.time() - start\n\n        # Add context information to the error message\n        exception_traceback = traceback.format_exc()\n        error_message = repr(e)\n        additional_info = {\n            \"traceback\": exception_traceback,\n            \"error\": error_message,\n        }\n\n    end = time.time()\n\n    # Catch NaN or inf\n    if not np.all(np.isfinite(cost)):\n        logger.warning(\n            \"Target function returned infinity or nothing at all. Result is treated as CRASHED\"\n            f\" and cost is set to {self._crash_cost}.\"\n        )\n\n        if \"traceback\" in additional_info:\n            logger.warning(f\"Traceback: {additional_info['traceback']}\\n\")\n\n        status = StatusType.CRASHED\n\n    if status == StatusType.CRASHED:\n        cost = self._crash_cost\n\n    trial_value = TrialValue(\n        status=status,\n        cost=cost,\n        time=runtime,\n        cpu_time=cpu_time,\n        additional_info=additional_info,\n        starttime=start,\n        endtime=end,\n    )\n\n    return trial_info, trial_value\n</code></pre>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.submit_trial","title":"submit_trial","text":"<pre><code>submit_trial(trial_info: TrialInfo) -&gt; None\n</code></pre> <p>This function submits a trial_info object in a serial fashion. As there is a single  worker for this task, this interface can be considered a wrapper over the <code>run</code> method.</p> <p>Both result/exceptions can be completely determined in this step so both lists are properly filled.</p>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.submit_trial--parameters","title":"Parameters","text":"<p>trial_info : TrialInfo     An object containing the configuration launched.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def submit_trial(self, trial_info: TrialInfo) -&gt; None:\n    \"\"\"This function submits a trial_info object in a serial fashion. As there is a single\n     worker for this task, this interface can be considered a wrapper over the `run` method.\n\n    Both result/exceptions can be completely determined in this step so both lists\n    are properly filled.\n\n    Parameters\n    ----------\n    trial_info : TrialInfo\n        An object containing the configuration launched.\n    \"\"\"\n    self._results_queue.append(self.run_wrapper(trial_info))\n</code></pre>"},{"location":"api/smac/runner/target_function_runner/#smac.runner.target_function_runner.TargetFunctionRunner.wait","title":"wait","text":"<pre><code>wait() -&gt; None\n</code></pre> <p>The SMBO/intensifier might need to wait for trials to finish before making a decision. For serial runners, no wait is needed as the result is immediately available.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def wait(self) -&gt; None:\n    \"\"\"The SMBO/intensifier might need to wait for trials to finish before making a decision.\n    For serial runners, no wait is needed as the result is immediately available.\n    \"\"\"\n    # There is no need to wait in serial runners. When launching a trial via submit, as\n    # the serial trial uses the same process to run, the result is always available\n    # immediately after. This method implements is just an implementation of the\n    # abstract method via a simple return, again, because there is no need to wait\n    return\n</code></pre>"},{"location":"api/smac/runner/target_function_script_runner/","title":"Target function script runner","text":""},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner","title":"smac.runner.target_function_script_runner","text":""},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner","title":"TargetFunctionScriptRunner","text":"<pre><code>TargetFunctionScriptRunner(\n    target_function: str,\n    scenario: Scenario,\n    required_arguments: list[str] = None,\n)\n</code></pre> <p>               Bases: <code>AbstractSerialRunner</code></p> <p>Class to execute target functions from scripts. Uses <code>Popen</code> to execute the script in a  subprocess.</p> <p>The following example shows how the script is called: <code>target_function --instance=test --instance_features=test --seed=0 --hyperparameter1=5323</code></p> <p>The script must return an echo in the following form (white-spaces are removed): <code>cost=0.5; runtime=0.01; status=SUCCESS; additional_info=test</code> (single-objective) <code>cost=0.5, 0.4; runtime=0.01; status=SUCCESS; additional_info=test</code> (multi-objective)</p> <p>The status must be a string and must be one of the <code>StatusType</code> values. However, <code>runtime</code>, <code>status</code> and <code>additional_info</code> are optional.</p>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner--note","title":"Note","text":"<p>Everytime an instance is passed, also an instance feature in form of a comma-separated list (no spaces) of floats is passed. If no instance feature for the instance is given, an empty list is passed.</p>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner--parameters","title":"Parameters","text":"<p>target_function : Callable     The target function. scenario : Scenario required_arguments : list[str]     A list of required arguments, which are passed to the target function.</p> Source code in <code>smac/runner/target_function_script_runner.py</code> <pre><code>def __init__(\n    self,\n    target_function: str,\n    scenario: Scenario,\n    required_arguments: list[str] = None,\n):\n    if required_arguments is None:\n        required_arguments = []\n    super().__init__(scenario=scenario, required_arguments=required_arguments)\n    self._target_function = target_function\n\n    # Check if target function is callable\n    if not isinstance(self._target_function, str):\n        raise TypeError(\n            \"Argument `target_function` must be a string but is type\" f\"`{type(self._target_function)}`.\"\n        )\n\n    if self._scenario.trial_memory_limit is not None:\n        logger.warning(\"Trial memory limit is not supported for script target functions.\")\n\n    if self._scenario.trial_walltime_limit is not None:\n        logger.warning(\"Trial walltime limit is not supported for script target functions.\")\n</code></pre>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.__call__","title":"__call__","text":"<pre><code>__call__(\n    algorithm_kwargs: dict[str, Any]\n) -&gt; tuple[str, str]\n</code></pre> <p>Calls the algorithm, which is processed in the <code>run</code> method.</p> Source code in <code>smac/runner/target_function_script_runner.py</code> <pre><code>def __call__(\n    self,\n    algorithm_kwargs: dict[str, Any],\n) -&gt; tuple[str, str]:\n    \"\"\"Calls the algorithm, which is processed in the ``run`` method.\"\"\"\n    cmd = [self._target_function]\n    for k, v in algorithm_kwargs.items():\n        v = str(v)\n        k = str(k)\n\n        # Let's remove some spaces\n        v = v.replace(\" \", \"\")\n\n        cmd += [f\"--{k}={v}\"]\n\n    logger.debug(f\"Calling: {' '.join(cmd)}\")\n    p = Popen(cmd, shell=False, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n    output, error = p.communicate()\n\n    logger.debug(\"Stdout: %s\" % output)\n    logger.debug(\"Stderr: %s\" % error)\n\n    return output, error\n</code></pre>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.count_available_workers","title":"count_available_workers","text":"<pre><code>count_available_workers() -&gt; int\n</code></pre> <p>Returns the number of available workers. Serial workers only have one worker.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def count_available_workers(self) -&gt; int:\n    \"\"\"Returns the number of available workers. Serial workers only have one worker.\"\"\"\n    return 1\n</code></pre>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.run","title":"run","text":"<pre><code>run(\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n) -&gt; tuple[\n    StatusType, float | list[float], float, float, dict\n]\n</code></pre> <p>Calls the target function.</p>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.run--parameters","title":"Parameters","text":"<p>config : Configuration     Configuration to be passed to the target function. instance : str | None, defaults to None     The Problem instance. budget : float | None, defaults to None     A positive, real-valued number representing an arbitrary limit to the target function     handled by the target function internally. seed : int, defaults to None</p>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.run--returns","title":"Returns","text":"<p>status : StatusType     Status of the trial. cost : float | list[float]     Resulting cost(s) of the trial. runtime : float     The time the target function took to run. cpu_time : float     The time the target function took on the hardware to run. additional_info : dict     All further additional trial information.</p> Source code in <code>smac/runner/target_function_script_runner.py</code> <pre><code>def run(\n    self,\n    config: Configuration,\n    instance: str | None = None,\n    budget: float | None = None,\n    seed: int | None = None,\n) -&gt; tuple[StatusType, float | list[float], float, float, dict]:\n    \"\"\"Calls the target function.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to be passed to the target function.\n    instance : str | None, defaults to None\n        The Problem instance.\n    budget : float | None, defaults to None\n        A positive, real-valued number representing an arbitrary limit to the target function\n        handled by the target function internally.\n    seed : int, defaults to None\n\n    Returns\n    -------\n    status : StatusType\n        Status of the trial.\n    cost : float | list[float]\n        Resulting cost(s) of the trial.\n    runtime : float\n        The time the target function took to run.\n    cpu_time : float\n        The time the target function took on the hardware to run.\n    additional_info : dict\n        All further additional trial information.\n    \"\"\"\n    # The kwargs are passed to the target function.\n    kwargs: dict[str, Any] = {}\n    if \"seed\" in self._required_arguments:\n        kwargs[\"seed\"] = seed\n\n    if \"instance\" in self._required_arguments:\n        kwargs[\"instance\"] = instance\n\n        # In contrast to the normal target function runner, we also add the instance features here.\n        if self._scenario.instance_features is not None and instance in self._scenario.instance_features:\n            kwargs[\"instance_features\"] = self._scenario.instance_features[instance]\n        else:\n            kwargs[\"instance_features\"] = []\n\n    if \"budget\" in self._required_arguments:\n        kwargs[\"budget\"] = budget\n\n    # Presetting\n    cost: float | list[float] = self._crash_cost\n    runtime = 0.0\n    cpu_time = runtime\n    additional_info = {}\n    status = StatusType.SUCCESS\n\n    # Add config arguments to the kwargs\n    for k, v in dict(config).items():\n        if k in kwargs:\n            raise RuntimeError(f\"The key {k} is already in use. Please use a different one.\")\n        kwargs[k] = v\n\n    # Call target function\n    start_time = time.time()\n    cpu_time = time.process_time()\n    output, error = self(kwargs)\n    cpu_time = time.process_time() - cpu_time\n    runtime = time.time() - start_time\n\n    # Now we have to parse the std output\n    # First remove white-spaces\n    output = output.replace(\" \", \"\")\n\n    outputs = {}\n    for pair in output.split(\";\"):\n        try:\n            kv = pair.split(\"=\")\n            k, v = kv[0], kv[1]\n\n            # Get rid of the trailing newline\n            v = v.strip()\n\n            outputs[k] = v\n        except Exception:\n            pass\n\n    # Parse status\n    if \"status\" in outputs:\n        status = getattr(StatusType, outputs[\"status\"])\n\n    # Parse costs (depends on the number of objectives)\n    if \"cost\" in outputs:\n        if self._n_objectives == 1:\n            cost = float(outputs[\"cost\"])\n        else:\n            costs = outputs[\"cost\"].split(\",\")\n            costs = [float(c) for c in costs]\n            cost = costs\n\n            if len(costs) != self._n_objectives:\n                raise RuntimeError(\"The number of costs does not match the number of objectives.\")\n    else:\n        status = StatusType.CRASHED\n\n    # Overwrite runtime\n    if \"runtime\" in outputs:\n        runtime = float(outputs[\"runtime\"])\n\n    # Overwrite CPU time\n    if \"cpu_time\" in outputs:\n        cpu_time = float(outputs[\"cpu_time\"])\n\n    # Add additional info\n    if \"additional_info\" in outputs:\n        additional_info[\"additional_info\"] = outputs[\"additional_info\"]\n\n    if status != StatusType.SUCCESS:\n        additional_info[\"error\"] = error\n\n        if cost != self._crash_cost:\n            cost = self._crash_cost\n            logger.info(\n                \"The target function crashed but returned a cost. The cost is ignored and replaced by crash cost.\"\n            )\n\n    return status, cost, runtime, cpu_time, additional_info\n</code></pre>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.run_wrapper","title":"run_wrapper","text":"<pre><code>run_wrapper(\n    trial_info: TrialInfo,\n    **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]\n</code></pre> <p>Wrapper around run() to execute and check the execution of a given config. This function encapsulates common handling/processing, so that run() implementation is simplified.</p>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.run_wrapper--parameters","title":"Parameters","text":"<p>trial_info : RunInfo     Object that contains enough information to execute a configuration run in isolation. dask_data_to_scatter: dict[str, Any]     When a user scatters data from their local process to the distributed network,     this data is distributed in a round-robin fashion grouping by number of cores.     Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data     every time we would like to execute a target function with a big dataset.     For example, when your target function has a big dataset shared across all the target function,     this argument is very useful.</p>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.run_wrapper--returns","title":"Returns","text":"<p>info : TrialInfo     An object containing the configuration launched. value : TrialValue     Contains information about the status/performance of config.</p> Source code in <code>smac/runner/abstract_runner.py</code> <pre><code>def run_wrapper(\n    self, trial_info: TrialInfo, **dask_data_to_scatter: dict[str, Any]\n) -&gt; tuple[TrialInfo, TrialValue]:\n    \"\"\"Wrapper around run() to execute and check the execution of a given config.\n    This function encapsulates common\n    handling/processing, so that run() implementation is simplified.\n\n    Parameters\n    ----------\n    trial_info : RunInfo\n        Object that contains enough information to execute a configuration run in isolation.\n    dask_data_to_scatter: dict[str, Any]\n        When a user scatters data from their local process to the distributed network,\n        this data is distributed in a round-robin fashion grouping by number of cores.\n        Roughly speaking, we can keep this data in memory and then we do not have to (de-)serialize the data\n        every time we would like to execute a target function with a big dataset.\n        For example, when your target function has a big dataset shared across all the target function,\n        this argument is very useful.\n\n    Returns\n    -------\n    info : TrialInfo\n        An object containing the configuration launched.\n    value : TrialValue\n        Contains information about the status/performance of config.\n    \"\"\"\n    start = time.time()\n    cpu_time = time.process_time()\n    try:\n        status, cost, runtime, cpu_time, additional_info = self.run(\n            config=trial_info.config,\n            instance=trial_info.instance,\n            budget=trial_info.budget,\n            seed=trial_info.seed,\n            **dask_data_to_scatter,\n        )\n    except Exception as e:\n        status = StatusType.CRASHED\n        cost = self._crash_cost\n        cpu_time = time.process_time() - cpu_time\n        runtime = time.time() - start\n\n        # Add context information to the error message\n        exception_traceback = traceback.format_exc()\n        error_message = repr(e)\n        additional_info = {\n            \"traceback\": exception_traceback,\n            \"error\": error_message,\n        }\n\n    end = time.time()\n\n    # Catch NaN or inf\n    if not np.all(np.isfinite(cost)):\n        logger.warning(\n            \"Target function returned infinity or nothing at all. Result is treated as CRASHED\"\n            f\" and cost is set to {self._crash_cost}.\"\n        )\n\n        if \"traceback\" in additional_info:\n            logger.warning(f\"Traceback: {additional_info['traceback']}\\n\")\n\n        status = StatusType.CRASHED\n\n    if status == StatusType.CRASHED:\n        cost = self._crash_cost\n\n    trial_value = TrialValue(\n        status=status,\n        cost=cost,\n        time=runtime,\n        cpu_time=cpu_time,\n        additional_info=additional_info,\n        starttime=start,\n        endtime=end,\n    )\n\n    return trial_info, trial_value\n</code></pre>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.submit_trial","title":"submit_trial","text":"<pre><code>submit_trial(trial_info: TrialInfo) -&gt; None\n</code></pre> <p>This function submits a trial_info object in a serial fashion. As there is a single  worker for this task, this interface can be considered a wrapper over the <code>run</code> method.</p> <p>Both result/exceptions can be completely determined in this step so both lists are properly filled.</p>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.submit_trial--parameters","title":"Parameters","text":"<p>trial_info : TrialInfo     An object containing the configuration launched.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def submit_trial(self, trial_info: TrialInfo) -&gt; None:\n    \"\"\"This function submits a trial_info object in a serial fashion. As there is a single\n     worker for this task, this interface can be considered a wrapper over the `run` method.\n\n    Both result/exceptions can be completely determined in this step so both lists\n    are properly filled.\n\n    Parameters\n    ----------\n    trial_info : TrialInfo\n        An object containing the configuration launched.\n    \"\"\"\n    self._results_queue.append(self.run_wrapper(trial_info))\n</code></pre>"},{"location":"api/smac/runner/target_function_script_runner/#smac.runner.target_function_script_runner.TargetFunctionScriptRunner.wait","title":"wait","text":"<pre><code>wait() -&gt; None\n</code></pre> <p>The SMBO/intensifier might need to wait for trials to finish before making a decision. For serial runners, no wait is needed as the result is immediately available.</p> Source code in <code>smac/runner/abstract_serial_runner.py</code> <pre><code>def wait(self) -&gt; None:\n    \"\"\"The SMBO/intensifier might need to wait for trials to finish before making a decision.\n    For serial runners, no wait is needed as the result is immediately available.\n    \"\"\"\n    # There is no need to wait in serial runners. When launching a trial via submit, as\n    # the serial trial uses the same process to run, the result is always available\n    # immediately after. This method implements is just an implementation of the\n    # abstract method via a simple return, again, because there is no need to wait\n    return\n</code></pre>"},{"location":"api/smac/utils/configspace/","title":"Configspace","text":""},{"location":"api/smac/utils/configspace/#smac.utils.configspace","title":"smac.utils.configspace","text":""},{"location":"api/smac/utils/configspace/#smac.utils.configspace.convert_configurations_to_array","title":"convert_configurations_to_array","text":"<pre><code>convert_configurations_to_array(\n    configs: list[Configuration],\n) -&gt; ndarray\n</code></pre> <p>Impute inactive hyperparameters in configurations with their default.</p>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.convert_configurations_to_array--parameters","title":"Parameters","text":"<p>configs : List[Configuration]     List of configuration objects.</p>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.convert_configurations_to_array--returns","title":"Returns","text":"<p>np.ndarray</p> Source code in <code>smac/utils/configspace.py</code> <pre><code>def convert_configurations_to_array(configs: list[Configuration]) -&gt; np.ndarray:\n    \"\"\"Impute inactive hyperparameters in configurations with their default.\n\n    Parameters\n    ----------\n    configs : List[Configuration]\n        List of configuration objects.\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    return np.array([config.get_array() for config in configs], dtype=np.float64)\n</code></pre>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.get_conditional_hyperparameters","title":"get_conditional_hyperparameters","text":"<pre><code>get_conditional_hyperparameters(\n    X: ndarray, Y: ndarray | None = None\n) -&gt; ndarray\n</code></pre> <p>Returns conditional hyperparameters if values with -1 or smaller are observed. X is used if Y is not specified.</p> Source code in <code>smac/utils/configspace.py</code> <pre><code>def get_conditional_hyperparameters(X: np.ndarray, Y: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"Returns conditional hyperparameters if values with -1 or smaller are observed. X is used\n    if Y is not specified.\n    \"\"\"\n    # Taking care of conditional hyperparameters according to Levesque et al.\n    X_cond = X &lt;= -1\n\n    if Y is not None:\n        Y_cond = Y &lt;= -1\n    else:\n        Y_cond = X &lt;= -1\n\n    active = ~((np.expand_dims(X_cond, axis=1) != Y_cond).any(axis=2))\n    return active\n</code></pre>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.get_config_hash","title":"get_config_hash","text":"<pre><code>get_config_hash(\n    config: Configuration, chars: int = 6\n) -&gt; str\n</code></pre> <p>Returns a hash of the configuration.</p> Source code in <code>smac/utils/configspace.py</code> <pre><code>def get_config_hash(config: Configuration, chars: int = 6) -&gt; str:\n    \"\"\"Returns a hash of the configuration.\"\"\"\n    return hashlib.sha1(str(config).encode(\"utf-8\")).hexdigest()[:chars]\n</code></pre>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.get_types","title":"get_types","text":"<pre><code>get_types(\n    configspace: ConfigurationSpace,\n    instance_features: dict[str, list[float]] | None = None,\n) -&gt; tuple[list[int], list[tuple[float, float]]]\n</code></pre> <p>Return the types of the hyperparameters and the bounds of the hyperparameters and instance features.</p>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.get_types--warning","title":"Warning","text":"<p>The bounds for the instance features are not added in this function.</p> Source code in <code>smac/utils/configspace.py</code> <pre><code>def get_types(\n    configspace: ConfigurationSpace,\n    instance_features: dict[str, list[float]] | None = None,\n) -&gt; tuple[list[int], list[tuple[float, float]]]:\n    \"\"\"Return the types of the hyperparameters and the bounds of the\n    hyperparameters and instance features.\n\n    Warning\n    -------\n    The bounds for the instance features are *not* added in this function.\n    \"\"\"\n    # Extract types vector for rf from config space and the bounds\n    types = [0] * len(list(configspace.values()))\n    bounds = [(np.nan, np.nan)] * len(types)\n\n    for i, param in enumerate(list(configspace.values())):\n        parents = configspace.parents_of[param.name]\n        if len(parents) == 0:\n            can_be_inactive = False\n        else:\n            can_be_inactive = True\n\n        if isinstance(param, (CategoricalHyperparameter)):\n            n_cats = len(param.choices)\n            if can_be_inactive:\n                n_cats = len(param.choices) + 1\n            types[i] = n_cats\n            bounds[i] = (int(n_cats), np.nan)\n        elif isinstance(param, (OrdinalHyperparameter)):\n            n_cats = len(param.sequence)\n            types[i] = 0\n            if can_be_inactive:\n                bounds[i] = (0, int(n_cats))\n            else:\n                bounds[i] = (0, int(n_cats) - 1)\n        elif isinstance(param, Constant):\n            # For constants we simply set types to 0 which makes it a numerical parameter\n            if can_be_inactive:\n                bounds[i] = (2, np.nan)\n                types[i] = 2\n            else:\n                bounds[i] = (0, np.nan)\n                types[i] = 0\n            # and we leave the bounds to be 0 for now\n        elif isinstance(param, UniformFloatHyperparameter):\n            # Are sampled on the unit hypercube thus the bounds\n            # are always 0.0, 1.0\n            if can_be_inactive:\n                bounds[i] = (-1.0, 1.0)\n            else:\n                bounds[i] = (0, 1.0)\n        elif isinstance(param, UniformIntegerHyperparameter):\n            if can_be_inactive:\n                bounds[i] = (-1.0, 1.0)\n            else:\n                bounds[i] = (0, 1.0)\n        elif isinstance(param, NormalFloatHyperparameter):\n            if can_be_inactive:\n                raise ValueError(\"Inactive parameters not supported for Beta and Normal Hyperparameters\")\n\n            bounds[i] = (param.lower_vectorized, param.upper_vectorized)\n        elif isinstance(param, NormalIntegerHyperparameter):\n            if can_be_inactive:\n                raise ValueError(\"Inactive parameters not supported for Beta and Normal Hyperparameters\")\n\n            bounds[i] = (param.lower_vectorized, param.upper_vectorized)\n        elif isinstance(param, BetaFloatHyperparameter):\n            if can_be_inactive:\n                raise ValueError(\"Inactive parameters not supported for Beta and Normal Hyperparameters\")\n\n            bounds[i] = (param.lower_vectorized, param.upper_vectorized)\n        elif isinstance(param, BetaIntegerHyperparameter):\n            if can_be_inactive:\n                raise ValueError(\"Inactive parameters not supported for Beta and Normal Hyperparameters\")\n\n            bounds[i] = (param.lower_vectorized, param.upper_vectorized)\n        elif not isinstance(\n            param,\n            (\n                UniformFloatHyperparameter,\n                UniformIntegerHyperparameter,\n                OrdinalHyperparameter,\n                CategoricalHyperparameter,\n                NormalFloatHyperparameter,\n                NormalIntegerHyperparameter,\n                BetaFloatHyperparameter,\n                BetaIntegerHyperparameter,\n            ),\n        ):\n            raise TypeError(\"Unknown hyperparameter type %s\" % type(param))\n\n    if instance_features is not None:\n        n_features = len(list(instance_features.values())[0])\n        types = types + [0] * n_features\n\n    return types, bounds\n</code></pre>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.print_config_changes","title":"print_config_changes","text":"<pre><code>print_config_changes(\n    incumbent: Configuration | None,\n    challenger: Configuration | None,\n    logger: Logger,\n) -&gt; None\n</code></pre> <p>Compares two configurations and prints the differences.</p> Source code in <code>smac/utils/configspace.py</code> <pre><code>def print_config_changes(\n    incumbent: Configuration | None,\n    challenger: Configuration | None,\n    logger: logging.Logger,\n) -&gt; None:\n    \"\"\"Compares two configurations and prints the differences.\"\"\"\n    if incumbent is None or challenger is None:\n        return\n\n    inc_keys = set(incumbent.keys())\n    all_keys = inc_keys.union(challenger.keys())\n\n    lines = []\n    for k in sorted(all_keys):\n        inc_k = incumbent.get(k, \"-inactive-\")\n        cha_k = challenger.get(k, \"-inactive-\")\n        lines.append(f\"--- {k}: {inc_k} -&gt; {cha_k}\" + \" (unchanged)\" if inc_k == cha_k else \"\")\n\n    msg = \"\\n\".join(lines)\n    logger.debug(msg)\n</code></pre>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.transform_continuous_designs","title":"transform_continuous_designs","text":"<pre><code>transform_continuous_designs(\n    design: ndarray,\n    origin: str,\n    configspace: ConfigurationSpace,\n) -&gt; list[Configuration]\n</code></pre> <p>Transforms the continuous designs into a discrete list of configurations.</p>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.transform_continuous_designs--parameters","title":"Parameters","text":"<p>design : np.ndarray     Array of hyperparameters originating from the initial design strategy. origin : str | None, defaults to None     Label for a configuration where it originated from. configspace : ConfigurationSpace</p>"},{"location":"api/smac/utils/configspace/#smac.utils.configspace.transform_continuous_designs--returns","title":"Returns","text":"<p>configs : list[Configuration]     Continuous transformed configs.</p> Source code in <code>smac/utils/configspace.py</code> <pre><code>def transform_continuous_designs(\n    design: np.ndarray, origin: str, configspace: ConfigurationSpace\n) -&gt; list[Configuration]:\n    \"\"\"Transforms the continuous designs into a discrete list of configurations.\n\n    Parameters\n    ----------\n    design : np.ndarray\n        Array of hyperparameters originating from the initial design strategy.\n    origin : str | None, defaults to None\n        Label for a configuration where it originated from.\n    configspace : ConfigurationSpace\n\n    Returns\n    -------\n    configs : list[Configuration]\n        Continuous transformed configs.\n    \"\"\"\n    params = configspace.get_hyperparameters()\n    for idx, param in enumerate(params):\n        if isinstance(param, IntegerHyperparameter):\n            design[:, idx] = param._inverse_transform(param._transform(design[:, idx]))\n        elif isinstance(param, NumericalHyperparameter):\n            continue\n        elif isinstance(param, Constant):\n            design_ = np.zeros(np.array(design.shape) + np.array((0, 1)))\n            design_[:, :idx] = design[:, :idx]\n            design_[:, idx + 1 :] = design[:, idx:]\n            design = design_\n        elif isinstance(param, CategoricalHyperparameter):\n            v_design = design[:, idx]\n            v_design[v_design == 1] = 1 - 10**-10\n            design[:, idx] = np.array(v_design * len(param.choices), dtype=int)\n        elif isinstance(param, OrdinalHyperparameter):\n            v_design = design[:, idx]\n            v_design[v_design == 1] = 1 - 10**-10\n            design[:, idx] = np.array(v_design * len(param.sequence), dtype=int)\n        else:\n            raise ValueError(\"Hyperparameter not supported when transforming a continuous design.\")\n\n    configs = []\n    for vector in design:\n        try:\n            conf = deactivate_inactive_hyperparameters(\n                configuration=None, configuration_space=configspace, vector=vector\n            )\n        except ForbiddenValueError:\n            continue\n\n        conf.origin = origin\n        configs.append(conf)\n\n    return configs\n</code></pre>"},{"location":"api/smac/utils/data_structures/","title":"Data structures","text":""},{"location":"api/smac/utils/data_structures/#smac.utils.data_structures","title":"smac.utils.data_structures","text":""},{"location":"api/smac/utils/data_structures/#smac.utils.data_structures.batch","title":"batch","text":"<pre><code>batch(iterable: list, n: int = 1) -&gt; Iterable[list]\n</code></pre> <p>Batches an iterable into chunks of size n.</p> Source code in <code>smac/utils/data_structures.py</code> <pre><code>def batch(iterable: list, n: int = 1) -&gt; Iterable[list]:\n    \"\"\"Batches an iterable into chunks of size n.\"\"\"\n    length = len(iterable)\n    for ndx in range(0, length, n):\n        yield iterable[ndx : min(ndx + n, length)]\n</code></pre>"},{"location":"api/smac/utils/data_structures/#smac.utils.data_structures.recursively_compare_dicts","title":"recursively_compare_dicts","text":"<pre><code>recursively_compare_dicts(\n    d1: dict,\n    d2: dict,\n    *,\n    level: str = \"root\",\n    diff: list[str] | None = None\n) -&gt; list[str]\n</code></pre> <p>Compares dictionaries recursively. Returns a list of differences in string format.</p>"},{"location":"api/smac/utils/data_structures/#smac.utils.data_structures.recursively_compare_dicts--parameters","title":"Parameters","text":"<p>d1 : dict     First dictionary. d2 : dict     Second dictionary. level : str, defaults to \"root\"     How the first level is called. diff : list[str] | None, defaults to None     Used for recursion.</p>"},{"location":"api/smac/utils/data_structures/#smac.utils.data_structures.recursively_compare_dicts--returns","title":"Returns","text":"<p>list[str]     List of differences in string format.</p> Source code in <code>smac/utils/data_structures.py</code> <pre><code>def recursively_compare_dicts(\n    d1: dict,\n    d2: dict,\n    *,\n    level: str = \"root\",\n    diff: list[str] | None = None,\n) -&gt; list[str]:\n    \"\"\"Compares dictionaries recursively. Returns a list of differences in string format.\n\n    Parameters\n    ----------\n    d1 : dict\n        First dictionary.\n    d2 : dict\n        Second dictionary.\n    level : str, defaults to \"root\"\n        How the first level is called.\n    diff : list[str] | None, defaults to None\n        Used for recursion.\n\n    Returns\n    -------\n    list[str]\n        List of differences in string format.\n    \"\"\"\n    if diff is None:\n        diff = []\n\n    if isinstance(d1, dict) and isinstance(d2, dict):\n        if d1.keys() != d2.keys():\n            s1 = set(d1.keys())\n            s2 = set(d2.keys())\n            # logger.info(\"{:&lt;20} + {} - {}\".format(level, s1 - s2, s2 - s1))\n            # logger.info(\"{} - {}\".format(s1 - s2, s2 - s1))\n            diff += [f\"{level} + {s1 - s2} - {s2 - s1}\"]\n            common_keys = s1 &amp; s2\n        else:\n            common_keys = set(d1.keys())\n\n        for k in common_keys:\n            recursively_compare_dicts(d1[k], d2[k], level=\"{}.{}\".format(level, k), diff=diff)\n\n    elif isinstance(d1, list) and isinstance(d2, list):\n        if len(d1) != len(d2):\n            diff += [f\"{level}: len1={len(d1)}; len2={len(d2)}\"]\n            # logger.info(\"{:&lt;20} len1={}; len2={}\".format(level, len(d1), len(d2)))\n            # logger.info(\"len1={}; len2={}\".format(len(d1), len(d2)))\n        common_len = min(len(d1), len(d2))\n\n        for i in range(common_len):\n            recursively_compare_dicts(d1[i], d2[i], level=\"{}[{}]\".format(level, i), diff=diff)\n\n    else:\n        if d1 != d2:\n            diff += [f\"{level}: {d1} != {d2}\"]\n            # logger.info(\"{:&lt;20} {} != {}\".format(level, d1, d2))\n            # logger.info(\"len1={}; len2={}\".format(len(d1), len(d2)))\n\n    return diff\n</code></pre>"},{"location":"api/smac/utils/logging/","title":"Logging","text":""},{"location":"api/smac/utils/logging/#smac.utils.logging","title":"smac.utils.logging","text":""},{"location":"api/smac/utils/logging/#smac.utils.logging.get_logger","title":"get_logger","text":"<pre><code>get_logger(logger_name: str) -&gt; Logger\n</code></pre> <p>Get the logger by name.</p> Source code in <code>smac/utils/logging.py</code> <pre><code>def get_logger(logger_name: str) -&gt; logging.Logger:\n    \"\"\"Get the logger by name.\"\"\"\n    logger = logging.getLogger(logger_name)\n    return logger\n</code></pre>"},{"location":"api/smac/utils/logging/#smac.utils.logging.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(\n    level: int | Path | Literal[False] | None = False,\n) -&gt; None\n</code></pre> <p>Sets up the logging configuration for all modules.</p>"},{"location":"api/smac/utils/logging/#smac.utils.logging.setup_logging--parameters","title":"Parameters","text":"<p>level : int | Path | Literal[False] | None, defaults to None     An integer representing the logging level. An custom logging configuration can be used when passing a path.     If False, no logging setup is performed.</p> Source code in <code>smac/utils/logging.py</code> <pre><code>def setup_logging(\n    level: int | Path | Literal[False] | None = False,\n) -&gt; None:\n    \"\"\"Sets up the logging configuration for all modules.\n\n    Parameters\n    ----------\n    level : int | Path | Literal[False] | None, defaults to None\n        An integer representing the logging level. An custom logging configuration can be used when passing a path.\n        If False, no logging setup is performed.\n    \"\"\"\n    if level is False:\n        return\n\n    if isinstance(level, Path):\n        log_filename = level\n    else:\n        path = Path() / smac.__file__\n        log_filename = path.parent / \"logging.yml\"\n\n    with (log_filename).open(\"r\") as stream:\n        config = yaml.safe_load(stream)\n\n    if isinstance(level, int):\n        config[\"root\"][\"level\"] = level\n        config[\"handlers\"][\"console\"][\"level\"] = level\n\n    logging.config.dictConfig(config)\n</code></pre>"},{"location":"api/smac/utils/multi_objective/","title":"Multi objective","text":""},{"location":"api/smac/utils/multi_objective/#smac.utils.multi_objective","title":"smac.utils.multi_objective","text":""},{"location":"api/smac/utils/multi_objective/#smac.utils.multi_objective.normalize_costs","title":"normalize_costs","text":"<pre><code>normalize_costs(\n    values: list[float],\n    bounds: list[tuple[float, float]] | None = None,\n) -&gt; list[float]\n</code></pre> <p>Normalizes a list of floats with corresponding bounds.</p>"},{"location":"api/smac/utils/multi_objective/#smac.utils.multi_objective.normalize_costs--parameters","title":"Parameters","text":"<p>values : list[float]     List of costs to be normalized. bounds : list[tuple[float, float]] | None, optional, defaults to None     List of tuple of bounds. If no bounds are passed, the input is returned.</p>"},{"location":"api/smac/utils/multi_objective/#smac.utils.multi_objective.normalize_costs--returns","title":"Returns","text":"<p>normalized_costs : list[float]     Normalized costs based on the bounds. If no bounds are given, the original values are returned.     Also, if min and max bounds are the same, the value of the corresponding objective is set to 1.</p> Source code in <code>smac/utils/multi_objective.py</code> <pre><code>def normalize_costs(\n    values: list[float],\n    bounds: list[tuple[float, float]] | None = None,\n) -&gt; list[float]:\n    \"\"\"\n    Normalizes a list of floats with corresponding bounds.\n\n    Parameters\n    ----------\n    values : list[float]\n        List of costs to be normalized.\n    bounds : list[tuple[float, float]] | None, optional, defaults to None\n        List of tuple of bounds. If no bounds are passed, the input is returned.\n\n    Returns\n    -------\n    normalized_costs : list[float]\n        Normalized costs based on the bounds. If no bounds are given, the original values are returned.\n        Also, if min and max bounds are the same, the value of the corresponding objective is set to 1.\n    \"\"\"\n    if bounds is None:\n        return values\n\n    if len(values) != len(bounds):\n        raise ValueError(\"Number of values and bounds must be equal.\")\n\n    costs = []\n    for v, b in zip(values, bounds):\n        assert not isinstance(v, list)\n        p = v - b[0]\n        q = b[1] - b[0]\n\n        if q &lt; 1e-10:\n            cost = 1.0\n        else:\n            cost = p / q\n        costs.append(cost)\n\n    return costs\n</code></pre>"},{"location":"api/smac/utils/numpyencoder/","title":"Numpyencoder","text":""},{"location":"api/smac/utils/numpyencoder/#smac.utils.numpyencoder","title":"smac.utils.numpyencoder","text":""},{"location":"api/smac/utils/numpyencoder/#smac.utils.numpyencoder.NumpyEncoder","title":"NumpyEncoder","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>Custom encoder for numpy data types</p> <p>From stackoverflow.com/a/61903895</p>"},{"location":"api/smac/utils/numpyencoder/#smac.utils.numpyencoder.NumpyEncoder.default","title":"default","text":"<pre><code>default(obj: Any) -&gt; Any\n</code></pre> <p>Handle numpy datatypes if present by converting to native python</p>"},{"location":"api/smac/utils/numpyencoder/#smac.utils.numpyencoder.NumpyEncoder.default--parameters","title":"Parameters","text":"<p>obj : Any     Object to serialize</p>"},{"location":"api/smac/utils/numpyencoder/#smac.utils.numpyencoder.NumpyEncoder.default--returns","title":"Returns","text":"<p>Any     Object in native python</p> Source code in <code>smac/utils/numpyencoder.py</code> <pre><code>def default(self, obj: Any) -&gt; Any:\n    \"\"\"Handle numpy datatypes if present by converting to native python\n\n    Parameters\n    ----------\n    obj : Any\n        Object to serialize\n\n    Returns\n    -------\n    Any\n        Object in native python\n    \"\"\"\n    if isinstance(\n        obj,\n        (\n            np.int_,\n            np.intc,\n            np.intp,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.int64,\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n        ),\n    ):\n        return int(obj)\n\n    elif isinstance(obj, (np.float16, np.float32, np.float64)):\n        return float(obj)\n\n    elif isinstance(obj, (np.complex64, np.complex128)):\n        return {\"real\": obj.real, \"imag\": obj.imag}\n\n    elif isinstance(obj, (np.ndarray,)):\n        return obj.tolist()\n\n    elif isinstance(obj, (np.bool_)):\n        return bool(obj)\n\n    elif isinstance(obj, (np.void)):\n        return None\n\n    return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"api/smac/utils/pareto_front/","title":"Pareto front","text":""},{"location":"api/smac/utils/pareto_front/#smac.utils.pareto_front","title":"smac.utils.pareto_front","text":""},{"location":"api/smac/utils/pareto_front/#smac.utils.pareto_front.calculate_pareto_front","title":"calculate_pareto_front","text":"<pre><code>calculate_pareto_front(\n    runhistory: RunHistory,\n    configs: list[Configuration],\n    config_instance_seed_budget_keys: list[\n        list[InstanceSeedBudgetKey]\n    ],\n) -&gt; list[Configuration]\n</code></pre> <p>Compares the passed configurations and returns only the ones on the pareto front.</p>"},{"location":"api/smac/utils/pareto_front/#smac.utils.pareto_front.calculate_pareto_front--parameters","title":"Parameters","text":"<p>runhistory : RunHistory     The runhistory containing the given configurations. configs : list[Configuration]     The configurations from which the Pareto front should be computed. config_instance_seed_budget_keys: list[list[InstanceSeedBudgetKey]]     The instance-seed budget keys for the configurations on the basis of which the Pareto front should be computed.</p>"},{"location":"api/smac/utils/pareto_front/#smac.utils.pareto_front.calculate_pareto_front--returns","title":"Returns","text":"<p>pareto_front : list[Configuration]     The pareto front computed from the given configurations.</p> Source code in <code>smac/utils/pareto_front.py</code> <pre><code>def calculate_pareto_front(\n    runhistory: RunHistory,\n    configs: list[Configuration],\n    config_instance_seed_budget_keys: list[list[InstanceSeedBudgetKey]],\n) -&gt; list[Configuration]:\n    \"\"\"Compares the passed configurations and returns only the ones on the pareto front.\n\n    Parameters\n    ----------\n    runhistory : RunHistory\n        The runhistory containing the given configurations.\n    configs : list[Configuration]\n        The configurations from which the Pareto front should be computed.\n    config_instance_seed_budget_keys: list[list[InstanceSeedBudgetKey]]\n        The instance-seed budget keys for the configurations on the basis of which the Pareto front should be computed.\n\n    Returns\n    -------\n    pareto_front : list[Configuration]\n        The pareto front computed from the given configurations.\n    \"\"\"\n    costs = _get_costs(runhistory, configs, config_instance_seed_budget_keys)\n\n    # The following code is an efficient pareto front implementation\n    is_efficient = np.arange(costs.shape[0])\n    next_point_index = 0  # Next index in the is_efficient array to search for\n    while next_point_index &lt; len(costs):\n        nondominated_point_mask = np.any(costs &lt; costs[next_point_index], axis=1)\n        nondominated_point_mask[next_point_index] = True\n        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n        costs = costs[nondominated_point_mask]\n        next_point_index = np.sum(nondominated_point_mask[:next_point_index]) + 1\n\n    new_incumbents = [configs[i] for i in is_efficient]\n    return new_incumbents\n</code></pre>"},{"location":"api/smac/utils/pareto_front/#smac.utils.pareto_front.sort_by_crowding_distance","title":"sort_by_crowding_distance","text":"<pre><code>sort_by_crowding_distance(\n    runhistory: RunHistory,\n    configs: list[Configuration],\n    config_instance_seed_budget_keys: list[\n        list[InstanceSeedBudgetKey]\n    ],\n) -&gt; list[Configuration]\n</code></pre> <p>Sorts the passed configurations by their crowding distance. Taken from github.com/anyoptimization/pymoo/blob/20abef1ade71915352217400c11ece4c2f35163e/pymoo/algorithms/nsga2.py</p>"},{"location":"api/smac/utils/pareto_front/#smac.utils.pareto_front.sort_by_crowding_distance--parameters","title":"Parameters","text":"<p>runhistory : RunHistory     The runhistory containing the given configurations. configs : list[Configuration]     The configurations which should be sorted. config_instance_seed_budget_keys: list[list[InstanceSeedBudgetKey]]     The instance-seed budget keys for the configurations which should be sorted.</p>"},{"location":"api/smac/utils/pareto_front/#smac.utils.pareto_front.sort_by_crowding_distance--returns","title":"Returns","text":"<p>sorted_list : list[Configuration]     Configurations sorted by crowding distance.</p> Source code in <code>smac/utils/pareto_front.py</code> <pre><code>def sort_by_crowding_distance(\n    runhistory: RunHistory,\n    configs: list[Configuration],\n    config_instance_seed_budget_keys: list[list[InstanceSeedBudgetKey]],\n) -&gt; list[Configuration]:\n    \"\"\"Sorts the passed configurations by their crowding distance. Taken from\n    https://github.com/anyoptimization/pymoo/blob/20abef1ade71915352217400c11ece4c2f35163e/pymoo/algorithms/nsga2.py\n\n\n    Parameters\n    ----------\n    runhistory : RunHistory\n        The runhistory containing the given configurations.\n    configs : list[Configuration]\n        The configurations which should be sorted.\n    config_instance_seed_budget_keys: list[list[InstanceSeedBudgetKey]]\n        The instance-seed budget keys for the configurations which should be sorted.\n\n    Returns\n    -------\n    sorted_list : list[Configuration]\n        Configurations sorted by crowding distance.\n    \"\"\"\n    F = _get_costs(runhistory, configs, config_instance_seed_budget_keys)\n    infinity = 1e14\n\n    n_points = F.shape[0]\n    n_obj = F.shape[1]\n\n    if n_points &lt;= 2:\n        # distances = np.full(n_points, infinity)\n        return configs\n    else:\n        # Sort each column and get index\n        I = np.argsort(F, axis=0, kind=\"mergesort\")  # noqa\n\n        # Now really sort the whole array\n        F = F[I, np.arange(n_obj)]\n\n        # get the distance to the last element in sorted list and replace zeros with actual values\n        dist = np.concatenate([F, np.full((1, n_obj), np.inf)]) - np.concatenate([np.full((1, n_obj), -np.inf), F])\n\n        index_dist_is_zero = np.where(dist == 0)\n\n        dist_to_last = np.copy(dist)\n        for i, j in zip(*index_dist_is_zero):\n            dist_to_last[i, j] = dist_to_last[i - 1, j]\n\n        dist_to_next = np.copy(dist)\n        for i, j in reversed(list(zip(*index_dist_is_zero))):\n            dist_to_next[i, j] = dist_to_next[i + 1, j]\n\n        # Normalize all the distances\n        norm = np.max(F, axis=0) - np.min(F, axis=0)\n        norm[norm == 0] = np.nan\n        dist_to_last, dist_to_next = dist_to_last[:-1] / norm, dist_to_next[1:] / norm\n\n        # If we divided by zero because all values in one columns are equal replace by none\n        dist_to_last[np.isnan(dist_to_last)] = 0.0\n        dist_to_next[np.isnan(dist_to_next)] = 0.0\n\n        # Sum up the distance to next and last and norm by objectives - also reorder from sorted list\n        J = np.argsort(I, axis=0)\n        crowding = np.sum(dist_to_last[J, np.arange(n_obj)] + dist_to_next[J, np.arange(n_obj)], axis=1) / n_obj\n\n    # Replace infinity with a large number\n    crowding[np.isinf(crowding)] = infinity\n    config_with_crowding = [(config, v) for config, v in zip(configs, crowding)]\n    config_with_crowding = sorted(config_with_crowding, key=lambda x: x[1], reverse=True)\n\n    return [c for c, _ in config_with_crowding]\n</code></pre>"},{"location":"api/smac/utils/subspaces/boing_subspace/","title":"Boing subspace","text":""},{"location":"api/smac/utils/subspaces/boing_subspace/#smac.utils.subspaces.boing_subspace","title":"smac.utils.subspaces.boing_subspace","text":""},{"location":"api/smac/utils/subspaces/turbo_subspace/","title":"Turbo subspace","text":""},{"location":"api/smac/utils/subspaces/turbo_subspace/#smac.utils.subspaces.turbo_subspace","title":"smac.utils.subspaces.turbo_subspace","text":""},{"location":"examples/1%20Basics/1_quadratic_function/","title":"Quadratic Function","text":"Expand to copy <code>examples/1_basics/1_quadratic_function.py</code>  (top right) <pre><code>import numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom matplotlib import pyplot as plt\n\nfrom smac.facade.hyperparameter_optimization_facade import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import RunHistory, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass QuadraticFunction:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x = Float(\"x\", (-5, 5), default=-5)\n        cs.add([x])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Returns the y value of a quadratic function with a minimum we know to be at x=0.\"\"\"\n        x = config[\"x\"]\n        return x**2\n\n\ndef plot(runhistory: RunHistory, incumbent: Configuration) -&gt; None:\n    plt.figure()\n\n    # Plot ground truth\n    x = list(np.linspace(-5, 5, 100))\n    y = [xi * xi for xi in x]\n    plt.plot(x, y)\n\n    # Plot all trials\n    for k, v in runhistory.items():\n        config = runhistory.get_config(k.config_id)\n        x = config[\"x\"]\n        y = v.cost  # type: ignore\n        plt.scatter(x, y, c=\"blue\", alpha=0.1, zorder=9999, marker=\"o\")\n\n    # Plot incumbent\n    plt.scatter(incumbent[\"x\"], incumbent[\"x\"] * incumbent[\"x\"], c=\"red\", zorder=10000, marker=\"x\")\n\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    model = QuadraticFunction()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, deterministic=True, n_trials=100)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HPOFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n\n    # Let's plot it too\n    plot(smac.runhistory, incumbent)\n</code></pre>"},{"location":"examples/1%20Basics/1_quadratic_function/#description","title":"Description","text":"<p>An example of applying SMAC to optimize a quadratic function.</p> <p>We use the black-box facade because it is designed for black-box function optimization. The black-box facade uses a Gaussian Process as its surrogate model. The facade works best on a numerical hyperparameter configuration space and should not be applied to problems with large evaluation budgets (up to 1000 evaluations).</p> CodeRun <pre><code>import numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom matplotlib import pyplot as plt\n\nfrom smac.facade.hyperparameter_optimization_facade import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import RunHistory, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass QuadraticFunction:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x = Float(\"x\", (-5, 5), default=-5)\n        cs.add([x])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Returns the y value of a quadratic function with a minimum we know to be at x=0.\"\"\"\n        x = config[\"x\"]\n        return x**2\n\n\ndef plot(runhistory: RunHistory, incumbent: Configuration) -&gt; None:\n    plt.figure()\n\n    # Plot ground truth\n    x = list(np.linspace(-5, 5, 100))\n    y = [xi * xi for xi in x]\n    plt.plot(x, y)\n\n    # Plot all trials\n    for k, v in runhistory.items():\n        config = runhistory.get_config(k.config_id)\n        x = config[\"x\"]\n        y = v.cost  # type: ignore\n        plt.scatter(x, y, c=\"blue\", alpha=0.1, zorder=9999, marker=\"o\")\n\n    # Plot incumbent\n    plt.scatter(incumbent[\"x\"], incumbent[\"x\"] * incumbent[\"x\"], c=\"red\", zorder=10000, marker=\"x\")\n\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    model = QuadraticFunction()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, deterministic=True, n_trials=100)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HPOFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n\n    # Let's plot it too\n    plot(smac.runhistory, incumbent)\n</code></pre> <p></p>"},{"location":"examples/1%20Basics/2_svm_cv/","title":"Support Vector Machine with Cross-Validation","text":"Expand to copy <code>examples/1_basics/2_svm_cv.py</code>  (top right) <pre><code>import numpy as np\nfrom ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float, Integer\nfrom ConfigSpace.conditions import InCondition\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import cross_val_score\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\n# We load the iris-dataset (a widely used benchmark)\niris = datasets.load_iris()\n\n\nclass SVM:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges\n        cs = ConfigurationSpace(seed=0)\n\n        # First we create our hyperparameters\n        kernel = Categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], default=\"poly\")\n        C = Float(\"C\", (0.001, 1000.0), default=1.0, log=True)\n        shrinking = Categorical(\"shrinking\", [True, False], default=True)\n        degree = Integer(\"degree\", (1, 5), default=3)\n        coef = Float(\"coef0\", (0.0, 10.0), default=0.0)\n        gamma = Categorical(\"gamma\", [\"auto\", \"value\"], default=\"auto\")\n        gamma_value = Float(\"gamma_value\", (0.0001, 8.0), default=1.0, log=True)\n\n        # Then we create dependencies\n        use_degree = InCondition(child=degree, parent=kernel, values=[\"poly\"])\n        use_coef = InCondition(child=coef, parent=kernel, values=[\"poly\", \"sigmoid\"])\n        use_gamma = InCondition(child=gamma, parent=kernel, values=[\"rbf\", \"poly\", \"sigmoid\"])\n        use_gamma_value = InCondition(child=gamma_value, parent=gamma, values=[\"value\"])\n\n        # Add hyperparameters and conditions to our configspace\n        cs.add([kernel, C, shrinking, degree, coef, gamma, gamma_value])\n        cs.add([use_degree, use_coef, use_gamma, use_gamma_value])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Creates a SVM based on a configuration and evaluates it on the\n        iris-dataset using cross-validation.\"\"\"\n        config_dict = config.get_dictionary()\n        if \"gamma\" in config:\n            config_dict[\"gamma\"] = config_dict[\"gamma_value\"] if config_dict[\"gamma\"] == \"value\" else \"auto\"\n            config_dict.pop(\"gamma_value\", None)\n\n        classifier = svm.SVC(**config_dict, random_state=seed)\n        scores = cross_val_score(classifier, iris.data, iris.target, cv=5)\n        cost = 1 - np.mean(scores)\n\n        return cost\n\n\nif __name__ == \"__main__\":\n    classifier = SVM()\n\n    # Next, we create an object, holding general information about the run\n    scenario = Scenario(\n        classifier.configspace,\n        n_trials=50,  # We want to run max 50 trials (combination of config and seed)\n    )\n\n    # We want to run the facade's default initial design, but we want to change the number\n    # of initial configs to 5.\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(scenario, n_configs=5)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        classifier.train,\n        initial_design=initial_design,\n        overwrite=True,  # If the run exists, we overwrite it; alternatively, we can continue from last state\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(classifier.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/1%20Basics/2_svm_cv/#description","title":"Description","text":"<p>An example of optimizing a simple support vector machine on the IRIS dataset. We use the hyperparameter optimization facade, which uses a random forest as its surrogate model. It is able to scale to higher evaluation budgets and a higher number of dimensions. Also, you can use mixed data types as well as conditional hyperparameters.</p> CodeRun <pre><code>import numpy as np\nfrom ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float, Integer\nfrom ConfigSpace.conditions import InCondition\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import cross_val_score\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\n# We load the iris-dataset (a widely used benchmark)\niris = datasets.load_iris()\n\n\nclass SVM:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges\n        cs = ConfigurationSpace(seed=0)\n\n        # First we create our hyperparameters\n        kernel = Categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], default=\"poly\")\n        C = Float(\"C\", (0.001, 1000.0), default=1.0, log=True)\n        shrinking = Categorical(\"shrinking\", [True, False], default=True)\n        degree = Integer(\"degree\", (1, 5), default=3)\n        coef = Float(\"coef0\", (0.0, 10.0), default=0.0)\n        gamma = Categorical(\"gamma\", [\"auto\", \"value\"], default=\"auto\")\n        gamma_value = Float(\"gamma_value\", (0.0001, 8.0), default=1.0, log=True)\n\n        # Then we create dependencies\n        use_degree = InCondition(child=degree, parent=kernel, values=[\"poly\"])\n        use_coef = InCondition(child=coef, parent=kernel, values=[\"poly\", \"sigmoid\"])\n        use_gamma = InCondition(child=gamma, parent=kernel, values=[\"rbf\", \"poly\", \"sigmoid\"])\n        use_gamma_value = InCondition(child=gamma_value, parent=gamma, values=[\"value\"])\n\n        # Add hyperparameters and conditions to our configspace\n        cs.add([kernel, C, shrinking, degree, coef, gamma, gamma_value])\n        cs.add([use_degree, use_coef, use_gamma, use_gamma_value])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Creates a SVM based on a configuration and evaluates it on the\n        iris-dataset using cross-validation.\"\"\"\n        config_dict = config.get_dictionary()\n        if \"gamma\" in config:\n            config_dict[\"gamma\"] = config_dict[\"gamma_value\"] if config_dict[\"gamma\"] == \"value\" else \"auto\"\n            config_dict.pop(\"gamma_value\", None)\n\n        classifier = svm.SVC(**config_dict, random_state=seed)\n        scores = cross_val_score(classifier, iris.data, iris.target, cv=5)\n        cost = 1 - np.mean(scores)\n\n        return cost\n\n\nif __name__ == \"__main__\":\n    classifier = SVM()\n\n    # Next, we create an object, holding general information about the run\n    scenario = Scenario(\n        classifier.configspace,\n        n_trials=50,  # We want to run max 50 trials (combination of config and seed)\n    )\n\n    # We want to run the facade's default initial design, but we want to change the number\n    # of initial configs to 5.\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(scenario, n_configs=5)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        classifier.train,\n        initial_design=initial_design,\n        overwrite=True,  # If the run exists, we overwrite it; alternatively, we can continue from last state\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(classifier.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre> <p></p>"},{"location":"examples/1%20Basics/3_ask_and_tell/","title":"Ask-and-Tell","text":"Expand to copy <code>examples/1_basics/3_ask_and_tell.py</code>  (top right) <pre><code>from ConfigSpace import Configuration, ConfigurationSpace, Float\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom smac.runhistory.dataclasses import TrialValue\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"The 2-dimensional Rosenbrock function as a toy model.\n        The Rosenbrock function is well know in the optimization community and\n        often serves as a toy problem. It can be defined for arbitrary\n        dimensions. The minimium is always at x_i = 1 with a function value of\n        zero. All input parameters are continuous. The search domain for\n        all x's is the interval [-5, 10].\n        \"\"\"\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nif __name__ == \"__main__\":\n    model = Rosenbrock2D()\n\n    # Scenario object\n    scenario = Scenario(model.configspace, deterministic=False, n_trials=100)\n\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(\n        scenario,\n        max_config_calls=1,  # We basically use one seed per config only\n    )\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        model.train,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    # We can ask SMAC which trials should be evaluated next\n    for _ in range(10):\n        info = smac.ask()\n        assert info.seed is not None\n\n        cost = model.train(info.config, seed=info.seed)\n        value = TrialValue(cost=cost, time=0.5)\n\n        smac.tell(info, value)\n\n    # After calling ask+tell, we can still optimize\n    # Note: SMAC will optimize the next 90 trials because 10 trials already have been evaluated\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/1%20Basics/3_ask_and_tell/#description","title":"Description","text":"<p>This examples show how to use the Ask-and-Tell interface.</p> CodeRun <pre><code>from ConfigSpace import Configuration, ConfigurationSpace, Float\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom smac.runhistory.dataclasses import TrialValue\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"The 2-dimensional Rosenbrock function as a toy model.\n        The Rosenbrock function is well know in the optimization community and\n        often serves as a toy problem. It can be defined for arbitrary\n        dimensions. The minimium is always at x_i = 1 with a function value of\n        zero. All input parameters are continuous. The search domain for\n        all x's is the interval [-5, 10].\n        \"\"\"\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nif __name__ == \"__main__\":\n    model = Rosenbrock2D()\n\n    # Scenario object\n    scenario = Scenario(model.configspace, deterministic=False, n_trials=100)\n\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(\n        scenario,\n        max_config_calls=1,  # We basically use one seed per config only\n    )\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        model.train,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    # We can ask SMAC which trials should be evaluated next\n    for _ in range(10):\n        info = smac.ask()\n        assert info.seed is not None\n\n        cost = model.train(info.config, seed=info.seed)\n        value = TrialValue(cost=cost, time=0.5)\n\n        smac.tell(info, value)\n\n    # After calling ask+tell, we can still optimize\n    # Note: SMAC will optimize the next 90 trials because 10 trials already have been evaluated\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre> <p></p>"},{"location":"examples/1%20Basics/4_callback/","title":"Custom Callback","text":"Expand to copy <code>examples/1_basics/4_callback.py</code>  (top right) <pre><code>from __future__ import annotations\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\n\nimport smac\nfrom smac import Callback\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.runhistory import TrialInfo, TrialValue\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nclass CustomCallback(Callback):\n    def __init__(self) -&gt; None:\n        self.trials_counter = 0\n\n    def on_start(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n        print(\"Let's start!\")\n        print(\"\")\n\n    def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n        self.trials_counter += 1\n        if self.trials_counter % 10 == 0:\n            print(f\"Evaluated {self.trials_counter} trials so far.\")\n\n            incumbent = smbo.intensifier.get_incumbent()\n            assert incumbent is not None\n            print(f\"Current incumbent: {incumbent.get_dictionary()}\")\n            print(f\"Current incumbent value: {smbo.runhistory.get_cost(incumbent)}\")\n            print(\"\")\n\n        if self.trials_counter == 50:\n            print(f\"We just triggered to stop the optimization after {smbo.runhistory.finished} finished trials.\")\n            return False\n\n        return None\n\n\nif __name__ == \"__main__\":\n    model = Rosenbrock2D()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, n_trials=200)\n\n    # Now we use SMAC to find the best hyperparameters\n    HPOFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n        callbacks=[CustomCallback()],\n        logging_level=999999,\n    ).optimize()\n</code></pre>"},{"location":"examples/1%20Basics/4_callback/#description","title":"Description","text":"<p>Using callbacks is the easieast way to integrate custom code inside the Bayesian optimization loop. In this example, we disable SMAC's default logging option and use the custom callback to log the evaluated trials. Furthermore, we print some stages of the optimization process.</p> CodeRun <pre><code>from __future__ import annotations\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\n\nimport smac\nfrom smac import Callback\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.runhistory import TrialInfo, TrialValue\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nclass CustomCallback(Callback):\n    def __init__(self) -&gt; None:\n        self.trials_counter = 0\n\n    def on_start(self, smbo: smac.main.smbo.SMBO) -&gt; None:\n        print(\"Let's start!\")\n        print(\"\")\n\n    def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n        self.trials_counter += 1\n        if self.trials_counter % 10 == 0:\n            print(f\"Evaluated {self.trials_counter} trials so far.\")\n\n            incumbent = smbo.intensifier.get_incumbent()\n            assert incumbent is not None\n            print(f\"Current incumbent: {incumbent.get_dictionary()}\")\n            print(f\"Current incumbent value: {smbo.runhistory.get_cost(incumbent)}\")\n            print(\"\")\n\n        if self.trials_counter == 50:\n            print(f\"We just triggered to stop the optimization after {smbo.runhistory.finished} finished trials.\")\n            return False\n\n        return None\n\n\nif __name__ == \"__main__\":\n    model = Rosenbrock2D()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, n_trials=200)\n\n    # Now we use SMAC to find the best hyperparameters\n    HPOFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n        callbacks=[CustomCallback()],\n        logging_level=999999,\n    ).optimize()\n</code></pre> <p></p>"},{"location":"examples/1%20Basics/5_continue/","title":"Continue an Optimization","text":"Expand to copy <code>examples/1_basics/5_continue.py</code>  (top right) <pre><code>from __future__ import annotations\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\n\nfrom smac import Callback\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.main.smbo import SMBO\nfrom smac.runhistory import TrialInfo, TrialValue\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass StopCallback(Callback):\n    def __init__(self, stop_after: int):\n        self._stop_after = stop_after\n\n    def on_tell_end(self, smbo: SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n        \"\"\"Called after the stats are updated and the trial is added to the runhistory. Optionally, returns false\n        to gracefully stop the optimization.\n        \"\"\"\n        if smbo.runhistory.finished == self._stop_after:\n            return False\n\n        return None\n\n\nclass QuadraticFunction:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x = Float(\"x\", (-5, 5), default=-5)\n        cs.add([x])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Returns the y value of a quadratic function with a minimum at x=0.\"\"\"\n        x = config[\"x\"]\n        return x * x\n\n\nif __name__ == \"__main__\":\n    model = QuadraticFunction()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, deterministic=True, n_trials=50)\n    stop_after = 10\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HPOFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        callbacks=[StopCallback(stop_after=stop_after)],\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n    )\n\n    incumbent = smac.optimize()\n    assert smac.runhistory.finished == stop_after\n\n    # Now, we want to continue the optimization\n    # Make sure, we don't overwrite the last run\n    smac2 = HPOFacade(\n        scenario,\n        model.train,\n        overwrite=False,\n    )\n\n    # Check whether we get the same incumbent\n    assert smac.intensifier.get_incumbent() == smac2.intensifier.get_incumbent()\n    assert smac2.runhistory.finished == stop_after\n\n    # And now we finish the optimization\n    incumbent2 = smac2.optimize()\n\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost of first run: {incumbent_cost}\")\n\n    incumbent_cost = smac2.validate(incumbent2)\n    print(f\"Incumbent cost of continued run: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/1%20Basics/5_continue/#description","title":"Description","text":"<p>SMAC can also be continued from a previous run. To do so, it reads in old files (derived from scenario's name, output_directory and seed) and sets the corresponding components. In this example, an optimization of a simple quadratic function is continued.</p> <p>First, after creating a scenario with 50 trials, we run SMAC with <code>overwrite=True</code>. This will overwrite any previous runs (in case the example was called before). We use a custom callback to artificially stop this first optimization after 10 trials.</p> <p>Second, we again run the SMAC optimization using the same scenario, but this time with <code>overwrite=False</code>. As there already is a previous run with the same meta data, this run will be continued until the 50 trials are reached.</p> CodeRun <pre><code>from __future__ import annotations\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\n\nfrom smac import Callback\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.main.smbo import SMBO\nfrom smac.runhistory import TrialInfo, TrialValue\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass StopCallback(Callback):\n    def __init__(self, stop_after: int):\n        self._stop_after = stop_after\n\n    def on_tell_end(self, smbo: SMBO, info: TrialInfo, value: TrialValue) -&gt; bool | None:\n        \"\"\"Called after the stats are updated and the trial is added to the runhistory. Optionally, returns false\n        to gracefully stop the optimization.\n        \"\"\"\n        if smbo.runhistory.finished == self._stop_after:\n            return False\n\n        return None\n\n\nclass QuadraticFunction:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x = Float(\"x\", (-5, 5), default=-5)\n        cs.add([x])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Returns the y value of a quadratic function with a minimum at x=0.\"\"\"\n        x = config[\"x\"]\n        return x * x\n\n\nif __name__ == \"__main__\":\n    model = QuadraticFunction()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, deterministic=True, n_trials=50)\n    stop_after = 10\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HPOFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        callbacks=[StopCallback(stop_after=stop_after)],\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n    )\n\n    incumbent = smac.optimize()\n    assert smac.runhistory.finished == stop_after\n\n    # Now, we want to continue the optimization\n    # Make sure, we don't overwrite the last run\n    smac2 = HPOFacade(\n        scenario,\n        model.train,\n        overwrite=False,\n    )\n\n    # Check whether we get the same incumbent\n    assert smac.intensifier.get_incumbent() == smac2.intensifier.get_incumbent()\n    assert smac2.runhistory.finished == stop_after\n\n    # And now we finish the optimization\n    incumbent2 = smac2.optimize()\n\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost of first run: {incumbent_cost}\")\n\n    incumbent_cost = smac2.validate(incumbent2)\n    print(f\"Incumbent cost of continued run: {incumbent_cost}\")\n</code></pre> <p></p>"},{"location":"examples/1%20Basics/6_priors/","title":"User Priors over the Optimum","text":"Expand to copy <code>examples/1_basics/6_priors.py</code>  (top right) <pre><code>import warnings\n\nimport numpy as np\nfrom ConfigSpace import (\n    BetaIntegerHyperparameter,\n    CategoricalHyperparameter,\n    Configuration,\n    ConfigurationSpace,\n    NormalFloatHyperparameter,\n    UniformIntegerHyperparameter,\n    UniformFloatHyperparameter,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom smac.acquisition.function import PriorAcquisitionFunction\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\nclass MLP:\n    @property\n    def prior_configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types,\n        # we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        # We do not have an educated belief on the number of layers beforehand\n        # As such, the prior on the HP is uniform\n        n_layer = UniformIntegerHyperparameter(\n            \"n_layer\",\n            lower=1,\n            upper=5,\n        )\n\n        # We believe the optimal network is likely going to be relatively wide,\n        # And place a Beta Prior skewed towards wider networks in log space\n        n_neurons = BetaIntegerHyperparameter(\n            \"n_neurons\",\n            lower=8,\n            upper=256,\n            alpha=4,\n            beta=2,\n            log=True,\n        )\n\n        # We believe that ReLU is likely going to be the optimal activation function about\n        # 60% of the time, and thus place weight on that accordingly\n        activation = CategoricalHyperparameter(\n            \"activation\",\n            [\"logistic\", \"tanh\", \"relu\"],\n            weights=[1, 1, 3],\n            default_value=\"relu\",\n        )\n\n        # Moreover, we believe ADAM is the most likely optimizer\n        optimizer = CategoricalHyperparameter(\n            \"optimizer\",\n            [\"sgd\", \"adam\"],\n            weights=[1, 2],\n            default_value=\"adam\",\n        )\n\n        # We do not have an educated opinion on the batch size, and thus leave it as-is\n        batch_size = UniformIntegerHyperparameter(\n            \"batch_size\",\n            16,\n            512,\n            default_value=128,\n        )\n\n        # We place a log-normal prior on the learning rate, so that it is centered on 10^-3,\n        # with one unit of standard deviation per multiple of 10 (in log space)\n        learning_rate_init = NormalFloatHyperparameter(\n            \"learning_rate_init\",\n            lower=1e-5,\n            upper=1.0,\n            mu=1e-3,  # will be transformed to log space later\n            sigma=10,  # will be transformed to log space later\n            log=True,\n        )\n\n        # Add all hyperparameters at once:\n        cs.add(\n            [n_layer, n_neurons, activation, optimizer, batch_size, learning_rate_init]\n        )\n\n        return cs\n\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types,\n        # we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        # We do not have an educated belief on the number of layers beforehand\n        n_layer = UniformIntegerHyperparameter(\n            \"n_layer\",\n            lower=1,\n            upper=5,\n        )\n\n        # Define network width without a specific prior\n        n_neurons = UniformIntegerHyperparameter(\n            \"n_neurons\",\n            lower=8,\n            upper=256,\n        )\n\n        # Define activation functions without specific weights\n        activation = CategoricalHyperparameter(\n            \"activation\",\n            [\"logistic\", \"tanh\", \"relu\"],\n            default_value=\"relu\",\n        )\n\n        # Define optimizer without specific weights\n        optimizer = CategoricalHyperparameter(\n            \"optimizer\",\n            [\"sgd\", \"adam\"],\n            default_value=\"adam\",\n        )\n\n        # Define batch size without specific distribution\n        batch_size = UniformIntegerHyperparameter(\n            \"batch_size\",\n            16,\n            512,\n            default_value=128,\n        )\n\n        # Define learning rate range without log-normal prior\n        learning_rate_init = UniformFloatHyperparameter(\n            \"learning_rate_init\",\n            lower=1e-5,\n            upper=1.0,\n            default_value=1e-3,\n        )\n\n        # Add all hyperparameters at once:\n        cs.add(\n            [n_layer, n_neurons, activation, optimizer, batch_size, learning_rate_init]\n        )\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"optimizer\"],\n                batch_size=config[\"batch_size\"],\n                activation=config[\"activation\"],\n                learning_rate_init=config[\"learning_rate_init\"],\n                random_state=seed,\n                max_iter=5,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(\n                n_splits=5, random_state=seed, shuffle=True\n            )  # to make CV splits consistent\n            score = cross_val_score(\n                classifier, digits.data, digits.target, cv=cv, error_score=\"raise\"\n            )\n\n        return 1 - np.mean(score)\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n    default_config = mlp.configspace.get_default_configuration()\n\n    # Define our environment variables\n    scenario = Scenario(mlp.configspace, n_trials=40)\n\n    # We also want to include our default configuration in the initial design\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(\n        scenario,\n        additional_configs=[default_config],\n    )\n\n    # We define the prior acquisition function, which conduct the optimization using priors over the optimum\n    acquisition_function = PriorAcquisitionFunction(\n        acquisition_function=HyperparameterOptimizationFacade.get_acquisition_function(\n            scenario\n        ),\n        decay_beta=scenario.n_trials / 10,  # Proven solid value\n    )\n\n    # We only want one config call (use only one seed in this example)\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(\n        scenario,\n        max_config_calls=1,\n    )\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        mlp.train,\n        initial_design=initial_design,\n        acquisition_function=acquisition_function,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(default_config)\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Default cost: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/1%20Basics/6_priors/#description","title":"Description","text":"<p>Example for optimizing a Multi-Layer Perceptron (MLP) setting priors over the optimum on the hyperparameters. These priors are derived from user knowledge (from previous runs on similar tasks, common knowledge or intuition gained from manual tuning). To create the priors, we make use of the Normal and Beta Hyperparameters, as well as the \"weights\" property of the <code>CategoricalHyperparameter</code>. This can be integrated into the optimiztion for any SMAC facade, but we stick with the hyperparameter optimization facade here. To incorporate user priors into the optimization, you have to change the acquisition function to <code>PriorAcquisitionFunction</code>.</p> CodeRun <pre><code>import warnings\n\nimport numpy as np\nfrom ConfigSpace import (\n    BetaIntegerHyperparameter,\n    CategoricalHyperparameter,\n    Configuration,\n    ConfigurationSpace,\n    NormalFloatHyperparameter,\n    UniformIntegerHyperparameter,\n    UniformFloatHyperparameter,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom smac.acquisition.function import PriorAcquisitionFunction\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\nclass MLP:\n    @property\n    def prior_configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types,\n        # we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        # We do not have an educated belief on the number of layers beforehand\n        # As such, the prior on the HP is uniform\n        n_layer = UniformIntegerHyperparameter(\n            \"n_layer\",\n            lower=1,\n            upper=5,\n        )\n\n        # We believe the optimal network is likely going to be relatively wide,\n        # And place a Beta Prior skewed towards wider networks in log space\n        n_neurons = BetaIntegerHyperparameter(\n            \"n_neurons\",\n            lower=8,\n            upper=256,\n            alpha=4,\n            beta=2,\n            log=True,\n        )\n\n        # We believe that ReLU is likely going to be the optimal activation function about\n        # 60% of the time, and thus place weight on that accordingly\n        activation = CategoricalHyperparameter(\n            \"activation\",\n            [\"logistic\", \"tanh\", \"relu\"],\n            weights=[1, 1, 3],\n            default_value=\"relu\",\n        )\n\n        # Moreover, we believe ADAM is the most likely optimizer\n        optimizer = CategoricalHyperparameter(\n            \"optimizer\",\n            [\"sgd\", \"adam\"],\n            weights=[1, 2],\n            default_value=\"adam\",\n        )\n\n        # We do not have an educated opinion on the batch size, and thus leave it as-is\n        batch_size = UniformIntegerHyperparameter(\n            \"batch_size\",\n            16,\n            512,\n            default_value=128,\n        )\n\n        # We place a log-normal prior on the learning rate, so that it is centered on 10^-3,\n        # with one unit of standard deviation per multiple of 10 (in log space)\n        learning_rate_init = NormalFloatHyperparameter(\n            \"learning_rate_init\",\n            lower=1e-5,\n            upper=1.0,\n            mu=1e-3,  # will be transformed to log space later\n            sigma=10,  # will be transformed to log space later\n            log=True,\n        )\n\n        # Add all hyperparameters at once:\n        cs.add(\n            [n_layer, n_neurons, activation, optimizer, batch_size, learning_rate_init]\n        )\n\n        return cs\n\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types,\n        # we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        # We do not have an educated belief on the number of layers beforehand\n        n_layer = UniformIntegerHyperparameter(\n            \"n_layer\",\n            lower=1,\n            upper=5,\n        )\n\n        # Define network width without a specific prior\n        n_neurons = UniformIntegerHyperparameter(\n            \"n_neurons\",\n            lower=8,\n            upper=256,\n        )\n\n        # Define activation functions without specific weights\n        activation = CategoricalHyperparameter(\n            \"activation\",\n            [\"logistic\", \"tanh\", \"relu\"],\n            default_value=\"relu\",\n        )\n\n        # Define optimizer without specific weights\n        optimizer = CategoricalHyperparameter(\n            \"optimizer\",\n            [\"sgd\", \"adam\"],\n            default_value=\"adam\",\n        )\n\n        # Define batch size without specific distribution\n        batch_size = UniformIntegerHyperparameter(\n            \"batch_size\",\n            16,\n            512,\n            default_value=128,\n        )\n\n        # Define learning rate range without log-normal prior\n        learning_rate_init = UniformFloatHyperparameter(\n            \"learning_rate_init\",\n            lower=1e-5,\n            upper=1.0,\n            default_value=1e-3,\n        )\n\n        # Add all hyperparameters at once:\n        cs.add(\n            [n_layer, n_neurons, activation, optimizer, batch_size, learning_rate_init]\n        )\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"optimizer\"],\n                batch_size=config[\"batch_size\"],\n                activation=config[\"activation\"],\n                learning_rate_init=config[\"learning_rate_init\"],\n                random_state=seed,\n                max_iter=5,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(\n                n_splits=5, random_state=seed, shuffle=True\n            )  # to make CV splits consistent\n            score = cross_val_score(\n                classifier, digits.data, digits.target, cv=cv, error_score=\"raise\"\n            )\n\n        return 1 - np.mean(score)\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n    default_config = mlp.configspace.get_default_configuration()\n\n    # Define our environment variables\n    scenario = Scenario(mlp.configspace, n_trials=40)\n\n    # We also want to include our default configuration in the initial design\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(\n        scenario,\n        additional_configs=[default_config],\n    )\n\n    # We define the prior acquisition function, which conduct the optimization using priors over the optimum\n    acquisition_function = PriorAcquisitionFunction(\n        acquisition_function=HyperparameterOptimizationFacade.get_acquisition_function(\n            scenario\n        ),\n        decay_beta=scenario.n_trials / 10,  # Proven solid value\n    )\n\n    # We only want one config call (use only one seed in this example)\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(\n        scenario,\n        max_config_calls=1,\n    )\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        mlp.train,\n        initial_design=initial_design,\n        acquisition_function=acquisition_function,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(default_config)\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Default cost: {incumbent_cost}\")\n</code></pre> <p></p>"},{"location":"examples/1%20Basics/7_parallelization_cluster/","title":"Parallelization on Cluster","text":"Expand to copy <code>examples/1_basics/7_parallelization_cluster.py</code>  (top right) <pre><code>import numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom dask.distributed import Client\nfrom dask_jobqueue import SLURMCluster\n\nfrom smac import BlackBoxFacade, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Branin(object):\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-5, log=False)\n        x1 = Float(\"x1\", (0, 15), default=2, log=False)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Branin function\n\n        Parameters\n        ----------\n        config : Configuration\n            Contains two continuous hyperparameters, x0 and x1\n        seed : int, optional\n            Not used, by default 0\n\n        Returns\n        -------\n        float\n            Branin function value\n        \"\"\"\n        x0 = config[\"x0\"]\n        x1 = config[\"x1\"]\n        a = 1.0\n        b = 5.1 / (4.0 * np.pi**2)\n        c = 5.0 / np.pi\n        r = 6.0\n        s = 10.0\n        t = 1.0 / (8.0 * np.pi)\n        ret = a * (x1 - b * x0**2 + c * x0 - r) ** 2 + s * (1 - t) * np.cos(x0) + s\n\n        return ret\n\n\nif __name__ == \"__main__\":\n    model = Branin()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, deterministic=True, n_trials=100, trial_walltime_limit=100)\n\n    # Create cluster\n    n_workers = 4  # Use 4 workers on the cluster\n    # Please note that the number of workers is directly set in the\n    # cluster / client. `scenario.n_workers` is ignored in this case.\n\n    cluster = SLURMCluster(\n        # This is the partition of our slurm cluster.\n        queue=\"cpu_short\",\n        # Your account name\n        # account=\"myaccount\",\n        cores=1,\n        memory=\"1 GB\",\n        # Walltime limit for each worker. Ensure that your function evaluations\n        # do not exceed this limit.\n        # More tips on this here: https://jobqueue.dask.org/en/latest/advanced-tips-and-tricks.html#how-to-handle-job-queueing-system-walltime-killing-workers\n        walltime=\"00:10:00\",\n        processes=1,\n        log_directory=\"tmp/smac_dask_slurm\",\n        # if you would like to limit the resources consumption of each function evaluation with pynisher, you need to\n        # set nanny as False\n        # Otherwise, an error `daemonic processes are not allowed to have children` will raise!\n        nanny=False,  # if you do not use pynisher to limit the memory/time usage, feel free to set this one as True\n    )\n    cluster.scale(jobs=n_workers)\n\n    # Dask will create n_workers jobs on the cluster which stay open.\n    # Then, SMAC/Dask will schedule individual runs on the workers like on your local machine.\n    client = Client(\n        address=cluster,\n    )\n    # Instead, you can also do\n    # client = cluster.get_client()\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = BlackBoxFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n        dask_client=client,\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/1%20Basics/7_parallelization_cluster/#description","title":"Description","text":"<p>An example of applying SMAC to optimize Branin using parallelization via Dask client on a  SLURM cluster. If you do not want to use a cluster but your local machine, set dask_client to <code>None</code> and pass <code>n_workers</code> to the <code>Scenario</code>.</p> <p>Sometimes, the submitted jobs by the slurm client might be cancelled once it starts. In that case, you could try to start your job from a computing node</p> <p> On some clusters you cannot spawn new jobs when running a SLURMCluster inside a job instead of on the login node. No obvious errors might be raised but it can hang silently.</p> <p>Sometimes you need to modify your launch command which can be done with <code>SLURMCluster.job_class.submit_command</code>. </p> <pre><code>cluster.job_cls.submit_command = submit_command\ncluster.job_cls.cancel_command = cancel_command\n</code></pre> <p>Here we optimize the synthetic 2d function Branin. We use the black-box facade because it is designed for black-box function optimization. The black-box facade uses a Gaussian Process as its surrogate model. The facade works best on a numerical hyperparameter configuration space and should not be applied to problems with large evaluation budgets (up to 1000 evaluations).</p> <pre><code>import numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom dask.distributed import Client\nfrom dask_jobqueue import SLURMCluster\n\nfrom smac import BlackBoxFacade, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Branin(object):\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-5, log=False)\n        x1 = Float(\"x1\", (0, 15), default=2, log=False)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"Branin function\n\n        Parameters\n        ----------\n        config : Configuration\n            Contains two continuous hyperparameters, x0 and x1\n        seed : int, optional\n            Not used, by default 0\n\n        Returns\n        -------\n        float\n            Branin function value\n        \"\"\"\n        x0 = config[\"x0\"]\n        x1 = config[\"x1\"]\n        a = 1.0\n        b = 5.1 / (4.0 * np.pi**2)\n        c = 5.0 / np.pi\n        r = 6.0\n        s = 10.0\n        t = 1.0 / (8.0 * np.pi)\n        ret = a * (x1 - b * x0**2 + c * x0 - r) ** 2 + s * (1 - t) * np.cos(x0) + s\n\n        return ret\n\n\nif __name__ == \"__main__\":\n    model = Branin()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, deterministic=True, n_trials=100, trial_walltime_limit=100)\n\n    # Create cluster\n    n_workers = 4  # Use 4 workers on the cluster\n    # Please note that the number of workers is directly set in the\n    # cluster / client. `scenario.n_workers` is ignored in this case.\n\n    cluster = SLURMCluster(\n        # This is the partition of our slurm cluster.\n        queue=\"cpu_short\",\n        # Your account name\n        # account=\"myaccount\",\n        cores=1,\n        memory=\"1 GB\",\n        # Walltime limit for each worker. Ensure that your function evaluations\n        # do not exceed this limit.\n        # More tips on this here: https://jobqueue.dask.org/en/latest/advanced-tips-and-tricks.html#how-to-handle-job-queueing-system-walltime-killing-workers\n        walltime=\"00:10:00\",\n        processes=1,\n        log_directory=\"tmp/smac_dask_slurm\",\n        # if you would like to limit the resources consumption of each function evaluation with pynisher, you need to\n        # set nanny as False\n        # Otherwise, an error `daemonic processes are not allowed to have children` will raise!\n        nanny=False,  # if you do not use pynisher to limit the memory/time usage, feel free to set this one as True\n    )\n    cluster.scale(jobs=n_workers)\n\n    # Dask will create n_workers jobs on the cluster which stay open.\n    # Then, SMAC/Dask will schedule individual runs on the workers like on your local machine.\n    client = Client(\n        address=cluster,\n    )\n    # Instead, you can also do\n    # client = cluster.get_client()\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = BlackBoxFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n        dask_client=client,\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/1%20Basics/8_warmstart/","title":"Warmstarting SMAC","text":"Expand to copy <code>examples/1_basics/8_warmstart.py</code>  (top right) <pre><code>from __future__ import annotations\n\nfrom smac.scenario import Scenario\nfrom smac.facade import HyperparameterOptimizationFacade\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom smac.runhistory.dataclasses import TrialValue, TrialInfo\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def evaluate(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"The 2-dimensional Rosenbrock function as a toy model.\n        The Rosenbrock function is well know in the optimization community and\n        often serves as a toy problem. It can be defined for arbitrary\n        dimensions. The minimium is always at x_i = 1 with a function value of\n        zero. All input parameters are continuous. The search domain for\n        all x's is the interval [-5, 10].\n        \"\"\"\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nif __name__ == \"__main__\":\n    SEED = 12345\n    task = Rosenbrock2D()\n\n    # Previous evaluations\n    # X vectors need to be connected to the configuration space\n    configurations = [\n        Configuration(task.configspace, {'x0':1, 'x1':2}),\n        Configuration(task.configspace, {'x0':-1, 'x1':3}),\n        Configuration(task.configspace, {'x0':5, 'x1':5}),\n    ]\n    costs = [task.evaluate(c, seed=SEED) for c in configurations]\n\n    # Define optimization problem and budget\n    scenario = Scenario(task.configspace, deterministic=False, n_trials=30)\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(scenario, max_config_calls=1)\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        task.evaluate,\n        intensifier=intensifier,\n        overwrite=True,\n\n        # Modify the initial design to use our custom initial design\n        initial_design=HyperparameterOptimizationFacade.get_initial_design(\n            scenario,\n            n_configs=0,  # Do not use the default initial design\n            additional_configs=configurations  # Use the configurations previously evaluated as initial design\n                                               # This only passes the configurations but not the cost!\n                                               # So in order to actually use the custom, pre-evaluated initial design\n                                               # we need to tell those trials, like below.\n        )\n    )\n\n    # Convert previously evaluated configurations into TrialInfo and TrialValue instances to pass to SMAC\n    trial_infos = [TrialInfo(config=c, seed=SEED) for c in configurations]\n    trial_values = [TrialValue(cost=c) for c in costs]\n\n    # Warmstart SMAC with the trial information and values\n    for info, value in zip(trial_infos, trial_values):\n        smac.tell(info, value)\n\n    # Optimize as usual\n    smac.optimize()\n</code></pre>"},{"location":"examples/1%20Basics/8_warmstart/#description","title":"Description","text":"<p>With the ask and tell interface, we can support warmstarting SMAC. We can communicate rich information about the previous trials to SMAC using <code>TrialInfo</code> and <code>TrialValue</code> instances. For more details on ask and tell consult the info page ask-and-tell.</p> CodeRun <pre><code>from __future__ import annotations\n\nfrom smac.scenario import Scenario\nfrom smac.facade import HyperparameterOptimizationFacade\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom smac.runhistory.dataclasses import TrialValue, TrialInfo\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def evaluate(self, config: Configuration, seed: int = 0) -&gt; float:\n        \"\"\"The 2-dimensional Rosenbrock function as a toy model.\n        The Rosenbrock function is well know in the optimization community and\n        often serves as a toy problem. It can be defined for arbitrary\n        dimensions. The minimium is always at x_i = 1 with a function value of\n        zero. All input parameters are continuous. The search domain for\n        all x's is the interval [-5, 10].\n        \"\"\"\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nif __name__ == \"__main__\":\n    SEED = 12345\n    task = Rosenbrock2D()\n\n    # Previous evaluations\n    # X vectors need to be connected to the configuration space\n    configurations = [\n        Configuration(task.configspace, {'x0':1, 'x1':2}),\n        Configuration(task.configspace, {'x0':-1, 'x1':3}),\n        Configuration(task.configspace, {'x0':5, 'x1':5}),\n    ]\n    costs = [task.evaluate(c, seed=SEED) for c in configurations]\n\n    # Define optimization problem and budget\n    scenario = Scenario(task.configspace, deterministic=False, n_trials=30)\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(scenario, max_config_calls=1)\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        task.evaluate,\n        intensifier=intensifier,\n        overwrite=True,\n\n        # Modify the initial design to use our custom initial design\n        initial_design=HyperparameterOptimizationFacade.get_initial_design(\n            scenario,\n            n_configs=0,  # Do not use the default initial design\n            additional_configs=configurations  # Use the configurations previously evaluated as initial design\n                                               # This only passes the configurations but not the cost!\n                                               # So in order to actually use the custom, pre-evaluated initial design\n                                               # we need to tell those trials, like below.\n        )\n    )\n\n    # Convert previously evaluated configurations into TrialInfo and TrialValue instances to pass to SMAC\n    trial_infos = [TrialInfo(config=c, seed=SEED) for c in configurations]\n    trial_values = [TrialValue(cost=c) for c in costs]\n\n    # Warmstart SMAC with the trial information and values\n    for info, value in zip(trial_infos, trial_values):\n        smac.tell(info, value)\n\n    # Optimize as usual\n    smac.optimize()\n</code></pre> <p></p>"},{"location":"examples/2%20Multi-Fidelity%20and%20Multi-Instances/1_mlp_epochs/","title":"Multi-Layer Perceptron Using Multiple Epochs","text":"Expand to copy <code>examples/2_multi_fidelity/1_mlp_epochs.py</code>  (top right) <pre><code>import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ConfigSpace import (\n    Categorical,\n    Configuration,\n    ConfigurationSpace,\n    EqualsCondition,\n    Float,\n    InCondition,\n    Integer,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import MultiFidelityFacade as MFFacade\nfrom smac import Scenario\nfrom smac.facade import AbstractFacade\nfrom smac.intensifier.hyperband import Hyperband\nfrom smac.intensifier.successive_halving import SuccessiveHalving\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndataset = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types, we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        n_layer = Integer(\"n_layer\", (1, 5), default=1)\n        n_neurons = Integer(\"n_neurons\", (8, 256), log=True, default=10)\n        activation = Categorical(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default=\"tanh\")\n        solver = Categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\")\n        batch_size = Integer(\"batch_size\", (30, 300), default=200)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        learning_rate_init = Float(\"learning_rate_init\", (0.0001, 1.0), default=0.001, log=True)\n\n        # Add all hyperparameters at once:\n        cs.add([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])\n\n        # Adding conditions to restrict the hyperparameter space...\n        # ... since learning rate is only used when solver is 'sgd'.\n        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n        # ... since learning rate initialization will only be accounted for when using 'sgd' or 'adam'.\n        use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n        # ... since batch size will not be considered when optimizer is 'lbfgs'.\n        use_batch_size = InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n        # We can also add multiple conditions on hyperparameters at once:\n        cs.add([use_lr, use_batch_size, use_lr_init])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: int = 25) -&gt; float:\n        # For deactivated parameters (by virtue of the conditions),\n        # the configuration stores None-values.\n        # This is not accepted by the MLP, so we replace them with placeholder values.\n        lr = config.get(\"learning_rate\", \"constant\")\n        lr_init = config.get(\"learning_rate_init\", 0.001)\n        batch_size = config.get(\"batch_size\", 200)\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"solver\"],\n                batch_size=batch_size,\n                activation=config[\"activation\"],\n                learning_rate=lr,\n                learning_rate_init=lr_init,\n                max_iter=int(np.ceil(budget)),\n                random_state=seed,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, dataset.data, dataset.target, cv=cv, error_score=\"raise\")\n\n        return 1 - np.mean(score)\n\n\ndef plot_trajectory(facades: list[AbstractFacade]) -&gt; None:\n    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n    plt.figure()\n    plt.title(\"Trajectory\")\n    plt.xlabel(\"Wallclock time [s]\")\n    plt.ylabel(facades[0].scenario.objectives)\n    plt.ylim(0, 0.4)\n\n    for facade in facades:\n        X, Y = [], []\n        for item in facade.intensifier.trajectory:\n            # Single-objective optimization\n            assert len(item.config_ids) == 1\n            assert len(item.costs) == 1\n\n            y = item.costs[0]\n            x = item.walltime\n\n            X.append(x)\n            Y.append(y)\n\n        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n        plt.scatter(X, Y, marker=\"x\")\n\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n\n    facades: list[AbstractFacade] = []\n    for intensifier_object in [SuccessiveHalving, Hyperband]:\n        # Define our environment variables\n        scenario = Scenario(\n            mlp.configspace,\n            walltime_limit=60,  # After 60 seconds, we stop the hyperparameter optimization\n            n_trials=500,  # Evaluate max 500 different trials\n            min_budget=1,  # Train the MLP using a hyperparameter configuration for at least 5 epochs\n            max_budget=25,  # Train the MLP using a hyperparameter configuration for at most 25 epochs\n            n_workers=8,\n        )\n\n        # We want to run five random configurations before starting the optimization.\n        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n\n        # Create our intensifier\n        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n\n        # Create our SMAC object and pass the scenario and the train method\n        smac = MFFacade(\n            scenario,\n            mlp.train,\n            initial_design=initial_design,\n            intensifier=intensifier,\n            overwrite=True,\n        )\n\n        # Let's optimize\n        incumbent = smac.optimize()\n\n        # Get cost of default configuration\n        default_cost = smac.validate(mlp.configspace.get_default_configuration())\n        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n\n        # Let's calculate the cost of the incumbent\n        incumbent_cost = smac.validate(incumbent)\n        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n\n        facades.append(smac)\n\n    # Let's plot it\n    plot_trajectory(facades)\n</code></pre>"},{"location":"examples/2%20Multi-Fidelity%20and%20Multi-Instances/1_mlp_epochs/#description","title":"Description","text":"<p>Example for optimizing a Multi-Layer Perceptron (MLP) using multiple budgets. Since we want to take advantage of multi-fidelity, the <code>MultiFidelityFacade</code> is a good choice. By default, <code>MultiFidelityFacade</code> internally runs with <code>hyperband &lt;https://arxiv.org/abs/1603.06560&gt;</code>_ as intensification, which is a combination of an aggressive racing mechanism and Successive Halving. Crucially, the target  function must accept a budget variable, detailing how much fidelity smac wants to allocate to this configuration. In this example, we use both <code>SuccessiveHalving</code> and <code>Hyperband</code> to compare the results.</p> <p>MLP is a deep neural network, and therefore, we choose epochs as fidelity type. This implies, that <code>budget</code> specifies the number of epochs smac wants to allocate. The digits dataset is chosen to optimize the average accuracy on 5-fold cross validation.</p> <p>Note</p> <p>This example uses the <code>MultiFidelityFacade</code> facade, which is the closest implementation to <code>BOHB &lt;https://github.com/automl/HpBandSter&gt;</code>_.</p> CodeRun <pre><code>import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ConfigSpace import (\n    Categorical,\n    Configuration,\n    ConfigurationSpace,\n    EqualsCondition,\n    Float,\n    InCondition,\n    Integer,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import MultiFidelityFacade as MFFacade\nfrom smac import Scenario\nfrom smac.facade import AbstractFacade\nfrom smac.intensifier.hyperband import Hyperband\nfrom smac.intensifier.successive_halving import SuccessiveHalving\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndataset = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types, we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        n_layer = Integer(\"n_layer\", (1, 5), default=1)\n        n_neurons = Integer(\"n_neurons\", (8, 256), log=True, default=10)\n        activation = Categorical(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default=\"tanh\")\n        solver = Categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\")\n        batch_size = Integer(\"batch_size\", (30, 300), default=200)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        learning_rate_init = Float(\"learning_rate_init\", (0.0001, 1.0), default=0.001, log=True)\n\n        # Add all hyperparameters at once:\n        cs.add([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])\n\n        # Adding conditions to restrict the hyperparameter space...\n        # ... since learning rate is only used when solver is 'sgd'.\n        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n        # ... since learning rate initialization will only be accounted for when using 'sgd' or 'adam'.\n        use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n        # ... since batch size will not be considered when optimizer is 'lbfgs'.\n        use_batch_size = InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n        # We can also add multiple conditions on hyperparameters at once:\n        cs.add([use_lr, use_batch_size, use_lr_init])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: int = 25) -&gt; float:\n        # For deactivated parameters (by virtue of the conditions),\n        # the configuration stores None-values.\n        # This is not accepted by the MLP, so we replace them with placeholder values.\n        lr = config.get(\"learning_rate\", \"constant\")\n        lr_init = config.get(\"learning_rate_init\", 0.001)\n        batch_size = config.get(\"batch_size\", 200)\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"solver\"],\n                batch_size=batch_size,\n                activation=config[\"activation\"],\n                learning_rate=lr,\n                learning_rate_init=lr_init,\n                max_iter=int(np.ceil(budget)),\n                random_state=seed,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, dataset.data, dataset.target, cv=cv, error_score=\"raise\")\n\n        return 1 - np.mean(score)\n\n\ndef plot_trajectory(facades: list[AbstractFacade]) -&gt; None:\n    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n    plt.figure()\n    plt.title(\"Trajectory\")\n    plt.xlabel(\"Wallclock time [s]\")\n    plt.ylabel(facades[0].scenario.objectives)\n    plt.ylim(0, 0.4)\n\n    for facade in facades:\n        X, Y = [], []\n        for item in facade.intensifier.trajectory:\n            # Single-objective optimization\n            assert len(item.config_ids) == 1\n            assert len(item.costs) == 1\n\n            y = item.costs[0]\n            x = item.walltime\n\n            X.append(x)\n            Y.append(y)\n\n        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n        plt.scatter(X, Y, marker=\"x\")\n\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n\n    facades: list[AbstractFacade] = []\n    for intensifier_object in [SuccessiveHalving, Hyperband]:\n        # Define our environment variables\n        scenario = Scenario(\n            mlp.configspace,\n            walltime_limit=60,  # After 60 seconds, we stop the hyperparameter optimization\n            n_trials=500,  # Evaluate max 500 different trials\n            min_budget=1,  # Train the MLP using a hyperparameter configuration for at least 5 epochs\n            max_budget=25,  # Train the MLP using a hyperparameter configuration for at most 25 epochs\n            n_workers=8,\n        )\n\n        # We want to run five random configurations before starting the optimization.\n        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n\n        # Create our intensifier\n        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n\n        # Create our SMAC object and pass the scenario and the train method\n        smac = MFFacade(\n            scenario,\n            mlp.train,\n            initial_design=initial_design,\n            intensifier=intensifier,\n            overwrite=True,\n        )\n\n        # Let's optimize\n        incumbent = smac.optimize()\n\n        # Get cost of default configuration\n        default_cost = smac.validate(mlp.configspace.get_default_configuration())\n        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n\n        # Let's calculate the cost of the incumbent\n        incumbent_cost = smac.validate(incumbent)\n        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n\n        facades.append(smac)\n\n    # Let's plot it\n    plot_trajectory(facades)\n</code></pre> <p></p>"},{"location":"examples/2%20Multi-Fidelity%20and%20Multi-Instances/2_sgd_datasets/","title":"Stochastic Gradient Descent On Multiple Datasets","text":"Expand to copy <code>examples/2_multi_fidelity/2_sgd_datasets.py</code>  (top right) <pre><code>from __future__ import annotations\n\nimport itertools\nimport warnings\n\nimport numpy as np\nfrom ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float\nfrom sklearn import datasets\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\nfrom smac import MultiFidelityFacade as MFFacade\nfrom smac import Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass DigitsDataset:\n    def __init__(self) -&gt; None:\n        self._data = datasets.load_digits()\n\n    def get_instances(self) -&gt; list[str]:\n        \"\"\"Create instances from the dataset which include two classes only.\"\"\"\n        return [f\"{classA}-{classB}\" for classA, classB in itertools.combinations(self._data.target_names, 2)]\n\n    def get_instance_features(self) -&gt; dict[str, list[int | float]]:\n        \"\"\"Returns the mean and variance of all instances as features.\"\"\"\n        features = {}\n        for instance in self.get_instances():\n            data, _ = self.get_instance_data(instance)\n            features[instance] = [np.mean(data), np.var(data)]\n\n        return features\n\n    def get_instance_data(self, instance: str) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Retrieve data from the passed instance.\"\"\"\n        # We split the dataset into two classes\n        classA, classB = instance.split(\"-\")\n        indices = np.where(np.logical_or(int(classA) == self._data.target, int(classB) == self._data.target))\n\n        data = self._data.data[indices]\n        target = self._data.target[indices]\n\n        return data, target\n\n\nclass SGD:\n    def __init__(self, dataset: DigitsDataset) -&gt; None:\n        self.dataset = dataset\n\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        \"\"\"Build the configuration space which defines all parameters and their ranges for the SGD classifier.\"\"\"\n        cs = ConfigurationSpace()\n\n        # We define a few possible parameters for the SGD classifier\n        alpha = Float(\"alpha\", (0, 1), default=1.0)\n        l1_ratio = Float(\"l1_ratio\", (0, 1), default=0.5)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        eta0 = Float(\"eta0\", (0.00001, 1), default=0.1, log=True)\n        # Add the parameters to configuration space\n        cs.add([alpha, l1_ratio, learning_rate, eta0])\n\n        return cs\n\n    def train(self, config: Configuration, instance: str, seed: int = 0) -&gt; float:\n        \"\"\"Creates a SGD classifier based on a configuration and evaluates it on the\n        digits dataset using cross-validation.\"\"\"\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            # SGD classifier using given configuration\n            clf = SGDClassifier(\n                loss=\"log_loss\",\n                penalty=\"elasticnet\",\n                alpha=config[\"alpha\"],\n                l1_ratio=config[\"l1_ratio\"],\n                learning_rate=config[\"learning_rate\"],\n                eta0=config[\"eta0\"],\n                max_iter=30,\n                early_stopping=True,\n                random_state=seed,\n            )\n\n            # get instance\n            data, target = self.dataset.get_instance_data(instance)\n\n            cv = StratifiedKFold(n_splits=4, random_state=seed, shuffle=True)  # to make CV splits consistent\n            scores = cross_val_score(clf, data, target, cv=cv)\n\n        return 1 - np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    dataset = DigitsDataset()\n    model = SGD(dataset)\n\n    scenario = Scenario(\n        model.configspace,\n        walltime_limit=30,  # We want to optimize for 30 seconds\n        n_trials=5000,  # We want to try max 5000 different trials\n        min_budget=1,  # Use min one instance\n        max_budget=45,  # Use max 45 instances (if we have a lot of instances we could constraint it here)\n        instances=dataset.get_instances(),\n        instance_features=dataset.get_instance_features(),\n    )\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = MFFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n    )\n\n    # Now we start the optimization process\n    incumbent = smac.optimize()\n\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/2%20Multi-Fidelity%20and%20Multi-Instances/2_sgd_datasets/#description","title":"Description","text":"<p>Example for optimizing a Multi-Layer Perceptron (MLP) across multiple (dataset) instances.</p> <p>Alternative to budgets, here wlog. we consider instances as a fidelity type. An instance represents a specific scenario/condition (e.g. different datasets, subsets, transformations) for the algorithm to run. SMAC then returns the algorithm that had the best performance across all the instances. In this case, an instance is a binary dataset i.e., digit-2 vs digit-3.</p> <p>If we use instance as our fidelity, we need to initialize scenario with argument instance. In this case the argument budget is no longer required by the target function. But due to the scenario instance argument, the target function now is required to have an instance argument.</p> CodeRun <pre><code>from __future__ import annotations\n\nimport itertools\nimport warnings\n\nimport numpy as np\nfrom ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float\nfrom sklearn import datasets\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\nfrom smac import MultiFidelityFacade as MFFacade\nfrom smac import Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass DigitsDataset:\n    def __init__(self) -&gt; None:\n        self._data = datasets.load_digits()\n\n    def get_instances(self) -&gt; list[str]:\n        \"\"\"Create instances from the dataset which include two classes only.\"\"\"\n        return [f\"{classA}-{classB}\" for classA, classB in itertools.combinations(self._data.target_names, 2)]\n\n    def get_instance_features(self) -&gt; dict[str, list[int | float]]:\n        \"\"\"Returns the mean and variance of all instances as features.\"\"\"\n        features = {}\n        for instance in self.get_instances():\n            data, _ = self.get_instance_data(instance)\n            features[instance] = [np.mean(data), np.var(data)]\n\n        return features\n\n    def get_instance_data(self, instance: str) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Retrieve data from the passed instance.\"\"\"\n        # We split the dataset into two classes\n        classA, classB = instance.split(\"-\")\n        indices = np.where(np.logical_or(int(classA) == self._data.target, int(classB) == self._data.target))\n\n        data = self._data.data[indices]\n        target = self._data.target[indices]\n\n        return data, target\n\n\nclass SGD:\n    def __init__(self, dataset: DigitsDataset) -&gt; None:\n        self.dataset = dataset\n\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        \"\"\"Build the configuration space which defines all parameters and their ranges for the SGD classifier.\"\"\"\n        cs = ConfigurationSpace()\n\n        # We define a few possible parameters for the SGD classifier\n        alpha = Float(\"alpha\", (0, 1), default=1.0)\n        l1_ratio = Float(\"l1_ratio\", (0, 1), default=0.5)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        eta0 = Float(\"eta0\", (0.00001, 1), default=0.1, log=True)\n        # Add the parameters to configuration space\n        cs.add([alpha, l1_ratio, learning_rate, eta0])\n\n        return cs\n\n    def train(self, config: Configuration, instance: str, seed: int = 0) -&gt; float:\n        \"\"\"Creates a SGD classifier based on a configuration and evaluates it on the\n        digits dataset using cross-validation.\"\"\"\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            # SGD classifier using given configuration\n            clf = SGDClassifier(\n                loss=\"log_loss\",\n                penalty=\"elasticnet\",\n                alpha=config[\"alpha\"],\n                l1_ratio=config[\"l1_ratio\"],\n                learning_rate=config[\"learning_rate\"],\n                eta0=config[\"eta0\"],\n                max_iter=30,\n                early_stopping=True,\n                random_state=seed,\n            )\n\n            # get instance\n            data, target = self.dataset.get_instance_data(instance)\n\n            cv = StratifiedKFold(n_splits=4, random_state=seed, shuffle=True)  # to make CV splits consistent\n            scores = cross_val_score(clf, data, target, cv=cv)\n\n        return 1 - np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    dataset = DigitsDataset()\n    model = SGD(dataset)\n\n    scenario = Scenario(\n        model.configspace,\n        walltime_limit=30,  # We want to optimize for 30 seconds\n        n_trials=5000,  # We want to try max 5000 different trials\n        min_budget=1,  # Use min one instance\n        max_budget=45,  # Use max 45 instances (if we have a lot of instances we could constraint it here)\n        instances=dataset.get_instances(),\n        instance_features=dataset.get_instance_features(),\n    )\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = MFFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n    )\n\n    # Now we start the optimization process\n    incumbent = smac.optimize()\n\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre> <p></p>"},{"location":"examples/2%20Multi-Fidelity%20and%20Multi-Instances/3_specify_HB_via_total_budget/","title":"Specify Number of Trials via a Total Budget in Hyperband","text":"Expand to copy <code>examples/2_multi_fidelity/3_specify_HB_via_total_budget.py</code>  (top right) <pre><code>from __future__ import annotations\n\nimport numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom matplotlib import pyplot as plt\n\nfrom smac import MultiFidelityFacade, RunHistory, Scenario\nfrom smac.intensifier.hyperband_utils import get_n_trials_for_hyperband_multifidelity\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass QuadraticFunction:\n    max_budget = 500\n\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x = Float(\"x\", (-5, 5), default=-5)\n        cs.add([x])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: float | None = None) -&gt; float:\n        \"\"\"Returns the y value of a quadratic function with a minimum we know to be at x=0.\"\"\"\n        x = config[\"x\"]\n\n        if budget is None:\n            multiplier = 1\n        else:\n            multiplier = 1 + budget / self.max_budget\n\n        return x**2 * multiplier\n\n\ndef plot(runhistory: RunHistory, incumbent: Configuration) -&gt; None:\n    plt.figure()\n\n    # Plot ground truth\n    x = list(np.linspace(-5, 5, 100))\n    y = [xi * xi for xi in x]\n    plt.plot(x, y)\n\n    # Plot all trials\n    for k, v in runhistory.items():\n        config = runhistory.get_config(k.config_id)\n        x = config[\"x\"]\n        y = v.cost  # type: ignore\n        plt.scatter(x, y, c=\"blue\", alpha=0.1, zorder=9999, marker=\"o\")\n\n    # Plot incumbent\n    plt.scatter(incumbent[\"x\"], incumbent[\"x\"] * incumbent[\"x\"], c=\"red\", zorder=10000, marker=\"x\")\n\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    model = QuadraticFunction()\n\n    min_budget = 10  # minimum budget per trial\n    max_budget = 500  # maximum budget per trial\n    eta = 3  # standard HB parameter influencing the number of stages\n\n    # Let's calculate how many trials we need to exhaust the total optimization budget (in terms of\n    # fidelity units)\n    n_trials = get_n_trials_for_hyperband_multifidelity(\n        total_budget=10000,  # this is the total optimization budget we specify in terms of fidelity units\n        min_budget=min_budget,  # This influences the Hyperband rounds, minimum budget per trial\n        max_budget=max_budget,  # This influences the Hyperband rounds, maximum budget per trial\n        eta=eta,  # This influences the Hyperband rounds\n        print_summary=True,\n    )\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(\n        model.configspace, deterministic=True, n_trials=n_trials, min_budget=min_budget, max_budget=max_budget\n    )\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = MultiFidelityFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n        intensifier=MultiFidelityFacade.get_intensifier(scenario=scenario, eta=eta),\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n\n    # Let's plot it too\n    plot(smac.runhistory, incumbent)\n</code></pre>"},{"location":"examples/2%20Multi-Fidelity%20and%20Multi-Instances/3_specify_HB_via_total_budget/#description","title":"Description","text":"<p>This example uses a dummy function but illustrates how to setup Hyperband if you  want to specify a total optimization budget in terms of fidelity units.</p> <p>In Hyperband, normally SMAC calculates a typical Hyperband round. If the number of trials is not used up by one single round, the next round is started. Instead of specifying the number of trial beforehand, specify the total budget in terms of the fidelity units and let SMAC calculate how many trials that would be.</p> CodeRun <pre><code>from __future__ import annotations\n\nimport numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom matplotlib import pyplot as plt\n\nfrom smac import MultiFidelityFacade, RunHistory, Scenario\nfrom smac.intensifier.hyperband_utils import get_n_trials_for_hyperband_multifidelity\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass QuadraticFunction:\n    max_budget = 500\n\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x = Float(\"x\", (-5, 5), default=-5)\n        cs.add([x])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: float | None = None) -&gt; float:\n        \"\"\"Returns the y value of a quadratic function with a minimum we know to be at x=0.\"\"\"\n        x = config[\"x\"]\n\n        if budget is None:\n            multiplier = 1\n        else:\n            multiplier = 1 + budget / self.max_budget\n\n        return x**2 * multiplier\n\n\ndef plot(runhistory: RunHistory, incumbent: Configuration) -&gt; None:\n    plt.figure()\n\n    # Plot ground truth\n    x = list(np.linspace(-5, 5, 100))\n    y = [xi * xi for xi in x]\n    plt.plot(x, y)\n\n    # Plot all trials\n    for k, v in runhistory.items():\n        config = runhistory.get_config(k.config_id)\n        x = config[\"x\"]\n        y = v.cost  # type: ignore\n        plt.scatter(x, y, c=\"blue\", alpha=0.1, zorder=9999, marker=\"o\")\n\n    # Plot incumbent\n    plt.scatter(incumbent[\"x\"], incumbent[\"x\"] * incumbent[\"x\"], c=\"red\", zorder=10000, marker=\"x\")\n\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    model = QuadraticFunction()\n\n    min_budget = 10  # minimum budget per trial\n    max_budget = 500  # maximum budget per trial\n    eta = 3  # standard HB parameter influencing the number of stages\n\n    # Let's calculate how many trials we need to exhaust the total optimization budget (in terms of\n    # fidelity units)\n    n_trials = get_n_trials_for_hyperband_multifidelity(\n        total_budget=10000,  # this is the total optimization budget we specify in terms of fidelity units\n        min_budget=min_budget,  # This influences the Hyperband rounds, minimum budget per trial\n        max_budget=max_budget,  # This influences the Hyperband rounds, maximum budget per trial\n        eta=eta,  # This influences the Hyperband rounds\n        print_summary=True,\n    )\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(\n        model.configspace, deterministic=True, n_trials=n_trials, min_budget=min_budget, max_budget=max_budget\n    )\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = MultiFidelityFacade(\n        scenario,\n        model.train,  # We pass the target function here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n        intensifier=MultiFidelityFacade.get_intensifier(scenario=scenario, eta=eta),\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n\n    # Let's plot it too\n    plot(smac.runhistory, incumbent)\n</code></pre> <p></p>"},{"location":"examples/3%20Multi-Objective/1_schaffer/","title":"2D Schaffer Function with Objective Weights","text":"Expand to copy <code>examples/3_multi_objective/1_schaffer.py</code>  (top right) <pre><code>from __future__ import annotations\n\nfrom typing import Dict, Tuple\n\nimport numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace\nfrom matplotlib import pyplot as plt\n\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.facade import AbstractFacade\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndef schaffer(x: float) -&gt; Tuple[float, float]:\n    f1 = np.square(x)\n    f2 = np.square(np.sqrt(f1) - 2)\n\n    return f1, f2\n\n\ndef target_function(config: Configuration, seed: int = 0) -&gt; Dict[str, float]:\n    f1, f2 = schaffer(config[\"x\"])\n    return {\"metric1\": f1, \"metric2\": f2}\n\n\ndef plot_from_smac(smac: AbstractFacade) -&gt; None:\n    plt.figure()\n    configs = smac.runhistory.get_configs()\n    incumbents = smac.intensifier.get_incumbents()\n\n    for i, config in enumerate(configs):\n        if config in incumbents:\n            continue\n\n        label = None\n        if i == 0:\n            label = \"Configuration\"\n\n        x = config[\"x\"]\n        f1, f2 = schaffer(x)\n        plt.scatter(f1, f2, c=\"blue\", alpha=0.1, marker=\"o\", zorder=3000, label=label)\n\n    for i, config in enumerate(incumbents):\n        label = None\n        if i == 0:\n            label = \"Incumbent\"\n\n        x = config[\"x\"]\n        f1, f2 = schaffer(x)\n        plt.scatter(f1, f2, c=\"red\", alpha=1, marker=\"x\", zorder=3000, label=label)\n\n    plt.xlabel(\"f1\")\n    plt.ylabel(\"f2\")\n    plt.title(\"Schaffer 2D\")\n    plt.legend()\n\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # Simple configspace\n    cs = ConfigurationSpace({\"x\": (-2.0, 2.0)})\n\n    # Scenario object\n    scenario = Scenario(\n        configspace=cs,\n        deterministic=True,  # Only one seed\n        n_trials=150,\n        objectives=[\"metric1\", \"metric2\"],\n    )\n\n    smac = HPOFacade(\n        scenario=scenario,\n        target_function=target_function,\n        multi_objective_algorithm=HPOFacade.get_multi_objective_algorithm(\n            scenario,\n            objective_weights=[1, 2],  # Weight metric2 twice as much as metric1\n        ),\n        overwrite=True,\n    )\n    incumbents = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(cs.get_default_configuration())\n    print(f\"Validated costs from default config: \\n--- {default_cost}\\n\")\n\n    print(\"Validated costs from the Pareto front (incumbents):\")\n    for incumbent in incumbents:\n        cost = smac.validate(incumbent)\n        print(\"---\", cost)\n\n    # Plot the evaluated points\n    plot_from_smac(smac)\n</code></pre>"},{"location":"examples/3%20Multi-Objective/1_schaffer/#description","title":"Description","text":"<p>A simple example on how to use multi-objective optimization is shown. The 2D Schaffer function is used. In the plot you can see that all points are on the Pareto front. However, since we set the objective weights, you can notice that SMAC prioritizes the second objective over the first one.</p> CodeRun <pre><code>from __future__ import annotations\n\nfrom typing import Dict, Tuple\n\nimport numpy as np\nfrom ConfigSpace import Configuration, ConfigurationSpace\nfrom matplotlib import pyplot as plt\n\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.facade import AbstractFacade\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndef schaffer(x: float) -&gt; Tuple[float, float]:\n    f1 = np.square(x)\n    f2 = np.square(np.sqrt(f1) - 2)\n\n    return f1, f2\n\n\ndef target_function(config: Configuration, seed: int = 0) -&gt; Dict[str, float]:\n    f1, f2 = schaffer(config[\"x\"])\n    return {\"metric1\": f1, \"metric2\": f2}\n\n\ndef plot_from_smac(smac: AbstractFacade) -&gt; None:\n    plt.figure()\n    configs = smac.runhistory.get_configs()\n    incumbents = smac.intensifier.get_incumbents()\n\n    for i, config in enumerate(configs):\n        if config in incumbents:\n            continue\n\n        label = None\n        if i == 0:\n            label = \"Configuration\"\n\n        x = config[\"x\"]\n        f1, f2 = schaffer(x)\n        plt.scatter(f1, f2, c=\"blue\", alpha=0.1, marker=\"o\", zorder=3000, label=label)\n\n    for i, config in enumerate(incumbents):\n        label = None\n        if i == 0:\n            label = \"Incumbent\"\n\n        x = config[\"x\"]\n        f1, f2 = schaffer(x)\n        plt.scatter(f1, f2, c=\"red\", alpha=1, marker=\"x\", zorder=3000, label=label)\n\n    plt.xlabel(\"f1\")\n    plt.ylabel(\"f2\")\n    plt.title(\"Schaffer 2D\")\n    plt.legend()\n\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # Simple configspace\n    cs = ConfigurationSpace({\"x\": (-2.0, 2.0)})\n\n    # Scenario object\n    scenario = Scenario(\n        configspace=cs,\n        deterministic=True,  # Only one seed\n        n_trials=150,\n        objectives=[\"metric1\", \"metric2\"],\n    )\n\n    smac = HPOFacade(\n        scenario=scenario,\n        target_function=target_function,\n        multi_objective_algorithm=HPOFacade.get_multi_objective_algorithm(\n            scenario,\n            objective_weights=[1, 2],  # Weight metric2 twice as much as metric1\n        ),\n        overwrite=True,\n    )\n    incumbents = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(cs.get_default_configuration())\n    print(f\"Validated costs from default config: \\n--- {default_cost}\\n\")\n\n    print(\"Validated costs from the Pareto front (incumbents):\")\n    for incumbent in incumbents:\n        cost = smac.validate(incumbent)\n        print(\"---\", cost)\n\n    # Plot the evaluated points\n    plot_from_smac(smac)\n</code></pre> <p></p>"},{"location":"examples/3%20Multi-Objective/2_parego/","title":"ParEGO","text":"Expand to copy <code>examples/3_multi_objective/2_parego.py</code>  (top right) <pre><code>from __future__ import annotations\n\nimport time\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ConfigSpace import (\n    Categorical,\n    Configuration,\n    ConfigurationSpace,\n    EqualsCondition,\n    Float,\n    InCondition,\n    Integer,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.facade.abstract_facade import AbstractFacade\nfrom smac.multi_objective.parego import ParEGO\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace()\n\n        n_layer = Integer(\"n_layer\", (1, 5), default=1)\n        n_neurons = Integer(\"n_neurons\", (8, 256), log=True, default=10)\n        activation = Categorical(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default=\"tanh\")\n        solver = Categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\")\n        batch_size = Integer(\"batch_size\", (30, 300), default=200)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        learning_rate_init = Float(\"learning_rate_init\", (0.0001, 1.0), default=0.001, log=True)\n\n        cs.add([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])\n\n        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n        use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n        use_batch_size = InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n        # We can also add multiple conditions on hyperparameters at once:\n        cs.add([use_lr, use_batch_size, use_lr_init])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: int = 10) -&gt; dict[str, float]:\n        lr = config.get(\"learning_rate\", \"constant\")\n        lr_init = config.get(\"learning_rate_init\", 0.001)\n        batch_size = config.get(\"batch_size\", 200)\n\n        start_time = time.time()\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"solver\"],\n                batch_size=batch_size,\n                activation=config[\"activation\"],\n                learning_rate=lr,\n                learning_rate_init=lr_init,\n                max_iter=int(np.ceil(budget)),\n                random_state=seed,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n        return {\n            \"1 - accuracy\": 1 - np.mean(score),\n            \"time\": time.time() - start_time,\n        }\n\n\ndef plot_pareto(smac: AbstractFacade, incumbents: list[Configuration]) -&gt; None:\n    \"\"\"Plots configurations from SMAC and highlights the best configurations in a Pareto front.\"\"\"\n    average_costs = []\n    average_pareto_costs = []\n    for config in smac.runhistory.get_configs():\n        # Since we use multiple seeds, we have to average them to get only one cost value pair for each configuration\n        average_cost = smac.runhistory.average_cost(config)\n\n        if config in incumbents:\n            average_pareto_costs += [average_cost]\n        else:\n            average_costs += [average_cost]\n\n    # Let's work with a numpy array\n    costs = np.vstack(average_costs)\n    pareto_costs = np.vstack(average_pareto_costs)\n    pareto_costs = pareto_costs[pareto_costs[:, 0].argsort()]  # Sort them\n\n    costs_x, costs_y = costs[:, 0], costs[:, 1]\n    pareto_costs_x, pareto_costs_y = pareto_costs[:, 0], pareto_costs[:, 1]\n\n    plt.scatter(costs_x, costs_y, marker=\"x\", label=\"Configuration\")\n    plt.scatter(pareto_costs_x, pareto_costs_y, marker=\"x\", c=\"r\", label=\"Incumbent\")\n    plt.step(\n        [pareto_costs_x[0]] + pareto_costs_x.tolist() + [np.max(costs_x)],  # We add bounds\n        [np.max(costs_y)] + pareto_costs_y.tolist() + [np.min(pareto_costs_y)],  # We add bounds\n        where=\"post\",\n        linestyle=\":\",\n    )\n\n    plt.title(\"Pareto-Front\")\n    plt.xlabel(smac.scenario.objectives[0])\n    plt.ylabel(smac.scenario.objectives[1])\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n    objectives = [\"1 - accuracy\", \"time\"]\n\n    # Define our environment variables\n    scenario = Scenario(\n        mlp.configspace,\n        objectives=objectives,\n        walltime_limit=30,  # After 30 seconds, we stop the hyperparameter optimization\n        n_trials=200,  # Evaluate max 200 different trials\n        n_workers=1,\n    )\n\n    # We want to run five random configurations before starting the optimization.\n    initial_design = HPOFacade.get_initial_design(scenario, n_configs=5)\n    multi_objective_algorithm = ParEGO(scenario)\n    intensifier = HPOFacade.get_intensifier(scenario, max_config_calls=2)\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = HPOFacade(\n        scenario,\n        mlp.train,\n        initial_design=initial_design,\n        multi_objective_algorithm=multi_objective_algorithm,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    # Let's optimize\n    incumbents = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(mlp.configspace.get_default_configuration())\n    print(f\"Validated costs from default config: \\n--- {default_cost}\\n\")\n\n    print(\"Validated costs from the Pareto front (incumbents):\")\n    for incumbent in incumbents:\n        cost = smac.validate(incumbent)\n        print(\"---\", cost)\n\n    # Let's plot a pareto front\n    plot_pareto(smac, incumbents)\n</code></pre>"},{"location":"examples/3%20Multi-Objective/2_parego/#description","title":"Description","text":"<p>An example of how to use multi-objective optimization with ParEGO. Both accuracy and run-time are going to be optimized on the digits dataset using an MLP, and the configurations are shown in a plot, highlighting the best ones in  a Pareto front. The red cross indicates the best configuration selected by SMAC.</p> <p>In the optimization, SMAC evaluates the configurations on two different seeds. Therefore, the plot shows the mean accuracy and run-time of each configuration.</p> CodeRun <pre><code>from __future__ import annotations\n\nimport time\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ConfigSpace import (\n    Categorical,\n    Configuration,\n    ConfigurationSpace,\n    EqualsCondition,\n    Float,\n    InCondition,\n    Integer,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.facade.abstract_facade import AbstractFacade\nfrom smac.multi_objective.parego import ParEGO\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace()\n\n        n_layer = Integer(\"n_layer\", (1, 5), default=1)\n        n_neurons = Integer(\"n_neurons\", (8, 256), log=True, default=10)\n        activation = Categorical(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default=\"tanh\")\n        solver = Categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\")\n        batch_size = Integer(\"batch_size\", (30, 300), default=200)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        learning_rate_init = Float(\"learning_rate_init\", (0.0001, 1.0), default=0.001, log=True)\n\n        cs.add([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])\n\n        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n        use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n        use_batch_size = InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n        # We can also add multiple conditions on hyperparameters at once:\n        cs.add([use_lr, use_batch_size, use_lr_init])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: int = 10) -&gt; dict[str, float]:\n        lr = config.get(\"learning_rate\", \"constant\")\n        lr_init = config.get(\"learning_rate_init\", 0.001)\n        batch_size = config.get(\"batch_size\", 200)\n\n        start_time = time.time()\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"solver\"],\n                batch_size=batch_size,\n                activation=config[\"activation\"],\n                learning_rate=lr,\n                learning_rate_init=lr_init,\n                max_iter=int(np.ceil(budget)),\n                random_state=seed,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n        return {\n            \"1 - accuracy\": 1 - np.mean(score),\n            \"time\": time.time() - start_time,\n        }\n\n\ndef plot_pareto(smac: AbstractFacade, incumbents: list[Configuration]) -&gt; None:\n    \"\"\"Plots configurations from SMAC and highlights the best configurations in a Pareto front.\"\"\"\n    average_costs = []\n    average_pareto_costs = []\n    for config in smac.runhistory.get_configs():\n        # Since we use multiple seeds, we have to average them to get only one cost value pair for each configuration\n        average_cost = smac.runhistory.average_cost(config)\n\n        if config in incumbents:\n            average_pareto_costs += [average_cost]\n        else:\n            average_costs += [average_cost]\n\n    # Let's work with a numpy array\n    costs = np.vstack(average_costs)\n    pareto_costs = np.vstack(average_pareto_costs)\n    pareto_costs = pareto_costs[pareto_costs[:, 0].argsort()]  # Sort them\n\n    costs_x, costs_y = costs[:, 0], costs[:, 1]\n    pareto_costs_x, pareto_costs_y = pareto_costs[:, 0], pareto_costs[:, 1]\n\n    plt.scatter(costs_x, costs_y, marker=\"x\", label=\"Configuration\")\n    plt.scatter(pareto_costs_x, pareto_costs_y, marker=\"x\", c=\"r\", label=\"Incumbent\")\n    plt.step(\n        [pareto_costs_x[0]] + pareto_costs_x.tolist() + [np.max(costs_x)],  # We add bounds\n        [np.max(costs_y)] + pareto_costs_y.tolist() + [np.min(pareto_costs_y)],  # We add bounds\n        where=\"post\",\n        linestyle=\":\",\n    )\n\n    plt.title(\"Pareto-Front\")\n    plt.xlabel(smac.scenario.objectives[0])\n    plt.ylabel(smac.scenario.objectives[1])\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n    objectives = [\"1 - accuracy\", \"time\"]\n\n    # Define our environment variables\n    scenario = Scenario(\n        mlp.configspace,\n        objectives=objectives,\n        walltime_limit=30,  # After 30 seconds, we stop the hyperparameter optimization\n        n_trials=200,  # Evaluate max 200 different trials\n        n_workers=1,\n    )\n\n    # We want to run five random configurations before starting the optimization.\n    initial_design = HPOFacade.get_initial_design(scenario, n_configs=5)\n    multi_objective_algorithm = ParEGO(scenario)\n    intensifier = HPOFacade.get_intensifier(scenario, max_config_calls=2)\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = HPOFacade(\n        scenario,\n        mlp.train,\n        initial_design=initial_design,\n        multi_objective_algorithm=multi_objective_algorithm,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    # Let's optimize\n    incumbents = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(mlp.configspace.get_default_configuration())\n    print(f\"Validated costs from default config: \\n--- {default_cost}\\n\")\n\n    print(\"Validated costs from the Pareto front (incumbents):\")\n    for incumbent in incumbents:\n        cost = smac.validate(incumbent)\n        print(\"---\", cost)\n\n    # Let's plot a pareto front\n    plot_pareto(smac, incumbents)\n</code></pre> <p></p>"},{"location":"examples/4%20Advanced%20Topics/3_metadata_callback/","title":"Callback for logging run metadata","text":"Expand to copy <code>examples/4_advanced_optimizer/3_metadata_callback.py</code>  (top right) <pre><code>import sys\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\n\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.callback.metadata_callback import MetadataCallback\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nif __name__ == \"__main__\":\n    model = Rosenbrock2D()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, n_trials=200)\n\n    # Now we use SMAC to find the best hyperparameters and add the metadata callback defined above\n    HPOFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n        callbacks=[\n            MetadataCallback(\n                project_name=\"My Project Name\",\n                repository=\"My Repository Name\",\n                branch=\"Name of Active Branch\",\n                commit=\"Commit Hash\",\n                command=\" \".join(sys.argv),\n                additional_information=\"Some Additional Information\",\n            )\n        ],\n        logging_level=999999,\n    ).optimize()\n</code></pre>"},{"location":"examples/4%20Advanced%20Topics/3_metadata_callback/#description","title":"Description","text":"<p>An example for using a callback to log run metadata to a file. Any arguments passed to the callback will be logged to a json file at the beginning of the SMAC run (arguments must be json serializable).</p> <p>Instead of editing the Git-related information (repository, branch, commit) by hand each time they change, this information can also be added automatically using GitPython (install via <code>pip install GitPython</code>). There is an example for obtaining the information via GitPython below: <pre><code>from git import Repo\nrepo = Repo(\".\", search_parent_directories=True)\nMetadataCallback(\n    repository=repo.working_tree_dir.split(\"/\")[-1],\n    branch=str(repo.active_branch),\n    commit=str(repo.head.commit),\n    command=\" \".join([sys.argv[0][len(repo.working_tree_dir) + 1:]] + sys.argv[1:]),\n)\n</code></pre></p> <pre><code>import sys\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\n\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.callback.metadata_callback import MetadataCallback\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -&gt; float:\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nif __name__ == \"__main__\":\n    model = Rosenbrock2D()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, n_trials=200)\n\n    # Now we use SMAC to find the best hyperparameters and add the metadata callback defined above\n    HPOFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n        callbacks=[\n            MetadataCallback(\n                project_name=\"My Project Name\",\n                repository=\"My Repository Name\",\n                branch=\"Name of Active Branch\",\n                commit=\"Commit Hash\",\n                command=\" \".join(sys.argv),\n                additional_information=\"Some Additional Information\",\n            )\n        ],\n        logging_level=999999,\n    ).optimize()\n</code></pre>"},{"location":"examples/4%20Advanced%20Topics/4_intensify_crossvalidation/","title":"Speeding up Cross-Validation with Intensification","text":"Expand to copy <code>examples/4_advanced_optimizer/4_intensify_crossvalidation.py</code>  (top right) <pre><code>__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\nN_FOLDS = 10  # Global variable that determines the number of folds\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom smac.intensifier import Intensifier\n\n# We load the digits dataset, a small-scale 10-class digit recognition dataset\nX, y = datasets.load_digits(return_X_y=True)\n\n\nclass SVM:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges\n        cs = ConfigurationSpace(seed=0)\n\n        # First we create our hyperparameters\n        C = Float(\"C\", (2**-5, 2**15), default=1.0, log=True)\n        gamma = Float(\"gamma\", (2**-15, 2**3), default=1.0, log=True)\n\n        # Add hyperparameters to our configspace\n        cs.add([C, gamma])\n\n        return cs\n\n    def train(self, config: Configuration, instance: str, seed: int = 0) -&gt; float:\n        \"\"\"Creates a SVM based on a configuration and evaluate on the given fold of the digits dataset\n\n        Parameters\n        ----------\n        config: Configuration\n            The configuration to train the SVM.\n        instance: str\n            The name of the instance this configuration should be evaluated on. This is always of type\n            string by definition. In our case we cast to int, but this could also be the filename of a\n            problem instance to be loaded.\n        seed: int\n            The seed used for this call.\n        \"\"\"\n        instance = int(instance)\n        classifier = svm.SVC(**config, random_state=seed)\n        splitter = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n        for k, (train_idx, test_idx) in enumerate(splitter.split(X=X, y=y)):\n            if k != instance:\n                continue\n            else:\n                train_X = X[train_idx]\n                train_y = y[train_idx]\n                test_X = X[test_idx]\n                test_y = y[test_idx]\n                classifier.fit(train_X, train_y)\n                cost = 1 - classifier.score(test_X, test_y)\n\n        return cost\n\n\nif __name__ == \"__main__\":\n    classifier = SVM()\n\n    # Next, we create an object, holding general information about the run\n    scenario = Scenario(\n        classifier.configspace,\n        n_trials=50,  # We want to run max 50 trials (combination of config and instances in the case of\n        # deterministic=True. In the case of deterministic=False, this would be the\n        # combination of instances, seeds and configs). The number of distinct configurations\n        # evaluated by SMAC will be lower than this number because some of the configurations\n        # will be executed on more than one instance (CV fold).\n        instances=[f\"{i}\" for i in range(N_FOLDS)],  # Specify all instances by their name (as a string)\n        instance_features={f\"{i}\": [i] for i in range(N_FOLDS)},  # breaks SMAC\n        deterministic=True  # To simplify the problem we make SMAC believe that we have a deterministic\n        # optimization problem.\n    )\n\n    # We want to run the facade's default initial design, but we want to change the number\n    # of initial configs to 5.\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(scenario, n_configs=5)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        classifier.train,\n        initial_design=initial_design,\n        overwrite=True,  # If the run exists, we overwrite it; alternatively, we can continue from last state\n        # The next line defines the intensifier, i.e., the module that governs the selection of\n        # instance-seed pairs. Since we set deterministic to True above, it only governs the instance in\n        # this example. Technically, it is not necessary to create the intensifier as a user, but it is\n        # necessary to do so because we change the argument max_config_calls (the number of instance-seed pairs\n        # per configuration to try) to the number of cross-validation folds, while the default would be 3.\n        intensifier=Intensifier(scenario=scenario, max_config_calls=N_FOLDS, seed=0),\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(classifier.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n\n    # Let's see how many configurations we have evaluated. If this number is higher than 5, we have looked\n    # at more configurations than would have been possible with regular cross-validation, where the number\n    # of configurations would be determined by the number of trials divided by the number of folds (50 / 10).\n    runhistory = smac.runhistory\n    print(f\"Number of evaluated configurations: {len(runhistory.config_ids)}\")\n</code></pre>"},{"location":"examples/4%20Advanced%20Topics/4_intensify_crossvalidation/#description","title":"Description","text":"<p>An example of optimizing a simple support vector machine on the digits dataset. In contrast to the simple example, in which all cross-validation folds are executed at once, we use the intensification mechanism described in the original  SMAC paper as also demonstrated by Auto-WEKA. This mechanism allows us to terminate the evaluation of a configuration if after a certain number of folds, the configuration is found to be worse than the incumbent configuration. This is especially useful if the evaluation of a configuration is expensive, e.g., if we have to train a neural network or if we have to evaluate the configuration on a large dataset.</p> CodeRun <pre><code>__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\nN_FOLDS = 10  # Global variable that determines the number of folds\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom smac.intensifier import Intensifier\n\n# We load the digits dataset, a small-scale 10-class digit recognition dataset\nX, y = datasets.load_digits(return_X_y=True)\n\n\nclass SVM:\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges\n        cs = ConfigurationSpace(seed=0)\n\n        # First we create our hyperparameters\n        C = Float(\"C\", (2**-5, 2**15), default=1.0, log=True)\n        gamma = Float(\"gamma\", (2**-15, 2**3), default=1.0, log=True)\n\n        # Add hyperparameters to our configspace\n        cs.add([C, gamma])\n\n        return cs\n\n    def train(self, config: Configuration, instance: str, seed: int = 0) -&gt; float:\n        \"\"\"Creates a SVM based on a configuration and evaluate on the given fold of the digits dataset\n\n        Parameters\n        ----------\n        config: Configuration\n            The configuration to train the SVM.\n        instance: str\n            The name of the instance this configuration should be evaluated on. This is always of type\n            string by definition. In our case we cast to int, but this could also be the filename of a\n            problem instance to be loaded.\n        seed: int\n            The seed used for this call.\n        \"\"\"\n        instance = int(instance)\n        classifier = svm.SVC(**config, random_state=seed)\n        splitter = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n        for k, (train_idx, test_idx) in enumerate(splitter.split(X=X, y=y)):\n            if k != instance:\n                continue\n            else:\n                train_X = X[train_idx]\n                train_y = y[train_idx]\n                test_X = X[test_idx]\n                test_y = y[test_idx]\n                classifier.fit(train_X, train_y)\n                cost = 1 - classifier.score(test_X, test_y)\n\n        return cost\n\n\nif __name__ == \"__main__\":\n    classifier = SVM()\n\n    # Next, we create an object, holding general information about the run\n    scenario = Scenario(\n        classifier.configspace,\n        n_trials=50,  # We want to run max 50 trials (combination of config and instances in the case of\n        # deterministic=True. In the case of deterministic=False, this would be the\n        # combination of instances, seeds and configs). The number of distinct configurations\n        # evaluated by SMAC will be lower than this number because some of the configurations\n        # will be executed on more than one instance (CV fold).\n        instances=[f\"{i}\" for i in range(N_FOLDS)],  # Specify all instances by their name (as a string)\n        instance_features={f\"{i}\": [i] for i in range(N_FOLDS)},  # breaks SMAC\n        deterministic=True  # To simplify the problem we make SMAC believe that we have a deterministic\n        # optimization problem.\n    )\n\n    # We want to run the facade's default initial design, but we want to change the number\n    # of initial configs to 5.\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(scenario, n_configs=5)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        classifier.train,\n        initial_design=initial_design,\n        overwrite=True,  # If the run exists, we overwrite it; alternatively, we can continue from last state\n        # The next line defines the intensifier, i.e., the module that governs the selection of\n        # instance-seed pairs. Since we set deterministic to True above, it only governs the instance in\n        # this example. Technically, it is not necessary to create the intensifier as a user, but it is\n        # necessary to do so because we change the argument max_config_calls (the number of instance-seed pairs\n        # per configuration to try) to the number of cross-validation folds, while the default would be 3.\n        intensifier=Intensifier(scenario=scenario, max_config_calls=N_FOLDS, seed=0),\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(classifier.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n\n    # Let's see how many configurations we have evaluated. If this number is higher than 5, we have looked\n    # at more configurations than would have been possible with regular cross-validation, where the number\n    # of configurations would be determined by the number of trials divided by the number of folds (50 / 10).\n    runhistory = smac.runhistory\n    print(f\"Number of evaluated configurations: {len(runhistory.config_ids)}\")\n</code></pre> <p></p>"},{"location":"examples/5%20Command-Line%20Interface/1_call_target_function_script/","title":"Call Target Function From Script","text":"Expand to copy <code>examples/5_commandline/1_call_target_function_script.py</code>  (top right) <pre><code>from ConfigSpace import ConfigurationSpace\n\nfrom smac import BlackBoxFacade, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nif __name__ == \"__main__\":\n    cs = ConfigurationSpace({\"x0\": (0, 1000)}, seed=0)\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(cs, deterministic=True, n_trials=30)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = BlackBoxFacade(\n        scenario,\n        \"./script.sh\",  # We pass the filename of our script here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n    )\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(cs.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre>"},{"location":"examples/5%20Command-Line%20Interface/1_call_target_function_script/#description","title":"Description","text":"<p>This simple example shows how to call a script with the following content:</p> <pre><code>#!/bin/bash\n\n# Set arguments first\nfor argument in \"$@\"\ndo\n    key=$(echo $argument | cut -f1 -d=)\n    value=$(echo $argument | cut -f2 -d=)  \n\n    if [[ $key == *\"--\"* ]]; then\n        v=\"${key/--/}\"\n        declare $v=\"${value}\"\n    fi\ndone\n\n# We simply set the cost to our parameter\ncost=$x0\n\n# Return everything\necho \"cost=$cost\"\n</code></pre> CodeRun <pre><code>from ConfigSpace import ConfigurationSpace\n\nfrom smac import BlackBoxFacade, Scenario\n\n__copyright__ = \"Copyright 2025, Leibniz University Hanover, Institute of AI\"\n__license__ = \"3-clause BSD\"\n\n\nif __name__ == \"__main__\":\n    cs = ConfigurationSpace({\"x0\": (0, 1000)}, seed=0)\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(cs, deterministic=True, n_trials=30)\n\n    # Now we use SMAC to find the best hyperparameters\n    smac = BlackBoxFacade(\n        scenario,\n        \"./script.sh\",  # We pass the filename of our script here\n        overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data\n    )\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(cs.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")\n</code></pre> <p></p>"},{"location":"images/","title":"Overview Figure","text":"<p>Figure generated with Miro.</p>"}]}