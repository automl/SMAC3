{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ParEGO\n\nAn example of how to use multi-objective optimization with ParEGO. Both accuracy and run-time are going to be\noptimized on the digits dataset using an MLP, and the configurations are shown in a plot, highlighting the best ones in \na Pareto front. The red cross indicates the best configuration selected by SMAC.\n\nIn the optimization, SMAC evaluates the configurations on two different seeds. Therefore, the plot shows the\nmean accuracy and run-time of each configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport time\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ConfigSpace import (\n    Categorical,\n    Configuration,\n    ConfigurationSpace,\n    EqualsCondition,\n    Float,\n    InCondition,\n    Integer,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.facade.abstract_facade import AbstractFacade\nfrom smac.multi_objective.parego import ParEGO\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self) -> ConfigurationSpace:\n        cs = ConfigurationSpace()\n\n        n_layer = Integer(\"n_layer\", (1, 5), default=1)\n        n_neurons = Integer(\"n_neurons\", (8, 256), log=True, default=10)\n        activation = Categorical(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default=\"tanh\")\n        solver = Categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\")\n        batch_size = Integer(\"batch_size\", (30, 300), default=200)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        learning_rate_init = Float(\"learning_rate_init\", (0.0001, 1.0), default=0.001, log=True)\n\n        cs.add_hyperparameters([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])\n\n        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n        use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n        use_batch_size = InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n        # We can also add multiple conditions on hyperparameters at once:\n        cs.add_conditions([use_lr, use_batch_size, use_lr_init])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0, budget: int = 10) -> dict[str, float]:\n        lr = config.get(\"learning_rate\", \"constant\")\n        lr_init = config.get(\"learning_rate_init\", 0.001)\n        batch_size = config.get(\"batch_size\", 200)\n\n        start_time = time.time()\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"solver\"],\n                batch_size=batch_size,\n                activation=config[\"activation\"],\n                learning_rate=lr,\n                learning_rate_init=lr_init,\n                max_iter=int(np.ceil(budget)),\n                random_state=seed,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n        return {\n            \"1 - accuracy\": 1 - np.mean(score),\n            \"time\": time.time() - start_time,\n        }\n\n\ndef plot_pareto(smac: AbstractFacade, incumbents: list[Configuration]) -> None:\n    \"\"\"Plots configurations from SMAC and highlights the best configurations in a Pareto front.\"\"\"\n    average_costs = []\n    average_pareto_costs = []\n    for config in smac.runhistory.get_configs():\n        # Since we use multiple seeds, we have to average them to get only one cost value pair for each configuration\n        average_cost = smac.runhistory.average_cost(config)\n\n        if config in incumbents:\n            average_pareto_costs += [average_cost]\n        else:\n            average_costs += [average_cost]\n\n    # Let's work with a numpy array\n    costs = np.vstack(average_costs)\n    pareto_costs = np.vstack(average_pareto_costs)\n    pareto_costs = pareto_costs[pareto_costs[:, 0].argsort()]  # Sort them\n\n    costs_x, costs_y = costs[:, 0], costs[:, 1]\n    pareto_costs_x, pareto_costs_y = pareto_costs[:, 0], pareto_costs[:, 1]\n\n    plt.scatter(costs_x, costs_y, marker=\"x\", label=\"Configuration\")\n    plt.scatter(pareto_costs_x, pareto_costs_y, marker=\"x\", c=\"r\", label=\"Incumbent\")\n    plt.step(\n        [pareto_costs_x[0]] + pareto_costs_x.tolist() + [np.max(costs_x)],  # We add bounds\n        [np.max(costs_y)] + pareto_costs_y.tolist() + [np.min(pareto_costs_y)],  # We add bounds\n        where=\"post\",\n        linestyle=\":\",\n    )\n\n    plt.title(\"Pareto-Front\")\n    plt.xlabel(smac.scenario.objectives[0])\n    plt.ylabel(smac.scenario.objectives[1])\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n    objectives = [\"1 - accuracy\", \"time\"]\n\n    # Define our environment variables\n    scenario = Scenario(\n        mlp.configspace,\n        objectives=objectives,\n        walltime_limit=30,  # After 30 seconds, we stop the hyperparameter optimization\n        n_trials=200,  # Evaluate max 200 different trials\n        n_workers=1,\n    )\n\n    # We want to run five random configurations before starting the optimization.\n    initial_design = HPOFacade.get_initial_design(scenario, n_configs=5)\n    multi_objective_algorithm = ParEGO(scenario)\n    intensifier = HPOFacade.get_intensifier(scenario, max_config_calls=2)\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = HPOFacade(\n        scenario,\n        mlp.train,\n        initial_design=initial_design,\n        multi_objective_algorithm=multi_objective_algorithm,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    # Let's optimize\n    incumbents = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(mlp.configspace.get_default_configuration())\n    print(f\"Validated costs from default config: \\n--- {default_cost}\\n\")\n\n    print(\"Validated costs from the Pareto front (incumbents):\")\n    for incumbent in incumbents:\n        cost = smac.validate(incumbent)\n        print(\"---\", cost)\n\n    # Let's plot a pareto front\n    plot_pareto(smac, incumbents)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}