{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Stochastic Gradient Descent On Multiple Datasets\n\nExample for optimizing a Multi-Layer Perceptron (MLP) across multiple (dataset) instances.\n\nAlternative to budgets, here wlog. we consider instances as a fidelity type. An instance represents a specific\nscenario/condition (e.g. different datasets, subsets, transformations) for the algorithm to run. SMAC then returns the\nalgorithm that had the best performance across all the instances. In this case, an instance is a binary dataset i.e.,\ndigit-2 vs digit-3.\n\nIf we use instance as our fidelity, we need to initialize scenario with argument instance. In this case the argument\nbudget is no longer required by the target function. But due to the scenario instance argument,\nthe target function now is required to have an instance argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport itertools\nimport warnings\n\nimport numpy as np\nfrom ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float\nfrom sklearn import datasets\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\nfrom smac import MultiFidelityFacade as MFFacade\nfrom smac import Scenario\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\nclass DigitsDataset:\n    def __init__(self) -> None:\n        self._data = datasets.load_digits()\n\n    def get_instances(self) -> list[str]:\n        \"\"\"Create instances from the dataset which include two classes only.\"\"\"\n        return [f\"{classA}-{classB}\" for classA, classB in itertools.combinations(self._data.target_names, 2)]\n\n    def get_instance_features(self) -> dict[str, list[int | float]]:\n        \"\"\"Returns the mean and variance of all instances as features.\"\"\"\n        features = {}\n        for instance in self.get_instances():\n            data, _ = self.get_instance_data(instance)\n            features[instance] = [np.mean(data), np.var(data)]\n\n        return features\n\n    def get_instance_data(self, instance: str) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\"Retrieve data from the passed instance.\"\"\"\n        # We split the dataset into two classes\n        classA, classB = instance.split(\"-\")\n        indices = np.where(np.logical_or(int(classA) == self._data.target, int(classB) == self._data.target))\n\n        data = self._data.data[indices]\n        target = self._data.target[indices]\n\n        return data, target\n\n\nclass SGD:\n    def __init__(self, dataset: DigitsDataset) -> None:\n        self.dataset = dataset\n\n    @property\n    def configspace(self) -> ConfigurationSpace:\n        \"\"\"Build the configuration space which defines all parameters and their ranges for the SGD classifier.\"\"\"\n        cs = ConfigurationSpace()\n\n        # We define a few possible parameters for the SGD classifier\n        alpha = Float(\"alpha\", (0, 1), default=1.0)\n        l1_ratio = Float(\"l1_ratio\", (0, 1), default=0.5)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        eta0 = Float(\"eta0\", (0.00001, 1), default=0.1, log=True)\n        # Add the parameters to configuration space\n        cs.add_hyperparameters([alpha, l1_ratio, learning_rate, eta0])\n\n        return cs\n\n    def train(self, config: Configuration, instance: str, seed: int = 0) -> float:\n        \"\"\"Creates a SGD classifier based on a configuration and evaluates it on the\n        digits dataset using cross-validation.\"\"\"\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            # SGD classifier using given configuration\n            clf = SGDClassifier(\n                loss=\"log_loss\",\n                penalty=\"elasticnet\",\n                alpha=config[\"alpha\"],\n                l1_ratio=config[\"l1_ratio\"],\n                learning_rate=config[\"learning_rate\"],\n                eta0=config[\"eta0\"],\n                max_iter=30,\n                early_stopping=True,\n                random_state=seed,\n            )\n\n            # get instance\n            data, target = self.dataset.get_instance_data(instance)\n\n            cv = StratifiedKFold(n_splits=4, random_state=seed, shuffle=True)  # to make CV splits consistent\n            scores = cross_val_score(clf, data, target, cv=cv)\n\n        return 1 - np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    dataset = DigitsDataset()\n    model = SGD(dataset)\n\n    scenario = Scenario(\n        model.configspace,\n        walltime_limit=30,  # We want to optimize for 30 seconds\n        n_trials=5000,  # We want to try max 5000 different trials\n        min_budget=1,  # Use min one instance\n        max_budget=45,  # Use max 45 instances (if we have a lot of instances we could constraint it here)\n        instances=dataset.get_instances(),\n        instance_features=dataset.get_instance_features(),\n    )\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = MFFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n    )\n\n    # Now we start the optimization process\n    incumbent = smac.optimize()\n\n    default_cost = smac.validate(model.configspace.get_default_configuration())\n    print(f\"Default cost: {default_cost}\")\n\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Incumbent cost: {incumbent_cost}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}