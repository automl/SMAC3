
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/2_multi_fidelity/1_mlp_epochs.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_2_multi_fidelity_1_mlp_epochs.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_2_multi_fidelity_1_mlp_epochs.py:


Multi-Layer Perceptron Using Multiple Epochs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Example for optimizing a Multi-Layer Perceptron (MLP) using multiple budgets.
Since we want to take advantage of multi-fidelity, the ``MultiFidelityFacade`` is a good choice. By default,
``MultiFidelityFacade`` internally runs with `hyperband <https://arxiv.org/abs/1603.06560>`_ as
intensification, which is a combination of an aggressive racing mechanism and Successive Halving. Crucially, the target 
function must accept a budget variable, detailing how much fidelity smac wants to allocate to this
configuration. In this example, we use both ``SuccessiveHalving`` and ``Hyperband`` to compare the results.

MLP is a deep neural network, and therefore, we choose epochs as fidelity type. This implies,
that ``budget`` specifies the number of epochs smac wants to allocate. The digits dataset
is chosen to optimize the average accuracy on 5-fold cross validation.

.. note::

    This example uses the ``MultiFidelityFacade`` facade, which is the closest implementation to
    `BOHB <https://github.com/automl/HpBandSter>`_.

.. GENERATED FROM PYTHON SOURCE LINES 21-185



.. image-sg:: /examples/2_multi_fidelity/images/sphx_glr_1_mlp_epochs_001.png
   :alt: Trajectory
   :srcset: /examples/2_multi_fidelity/images/sphx_glr_1_mlp_epochs_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [INFO][abstract_facade.py:184] Workers are reduced to 8.
    [INFO][proxy.py:71] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
    [INFO][scheduler.py:1617] State start
    [INFO][scheduler.py:3863]   Scheduler at:     tcp://127.0.0.1:37407
    [INFO][scheduler.py:3870]   dashboard at:  http://127.0.0.1:8787/status
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:44325'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:38487'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:43789'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:44407'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:35901'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:43673'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:33901'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:39243'
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:40675', name: 0, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:40675
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:37948
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:41899', name: 4, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:41899
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:37970
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:36979', name: 1, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:36979
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:38000
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:32969', name: 5, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:32969
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:38014
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:38709', name: 7, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:38709
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:38026
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:36939', name: 6, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:36939
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:37986
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:39903', name: 3, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:39903
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:38032
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:39153', name: 2, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:39153
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:37958
    [INFO][scheduler.py:5235] Receive client connection: Client-a34df8d8-d863-11ed-8796-c5ac5de63e55
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:38034
    [INFO][abstract_initial_design.py:134] Using 5 initial design configurations and 0 additional configurations.
    [INFO][successive_halving.py:163] Successive Halving uses budget type BUDGETS with eta 3, min budget 1, and max budget 25.
    [INFO][successive_halving.py:307] Number of configs in stage:
    [INFO][successive_halving.py:309] --- Bracket 0: [9, 3, 1]
    [INFO][successive_halving.py:311] Budgets in stage:
    [INFO][successive_halving.py:313] --- Bracket 0: [2.7777777777777777, 8.333333333333332, 25.0]
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 50 trials.
    [INFO][abstract_intensifier.py:513] Added config d3f994 as new incumbent because there are no incumbents yet.
    [INFO][abstract_intensifier.py:588] Added config 8ea495 and rejected config d3f994 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- batch_size: None -> 214
    [INFO][configspace.py:175] --- learning_rate_init: None -> 0.005599223654063347
    [INFO][configspace.py:175] --- n_layer: 3 -> 4
    [INFO][configspace.py:175] --- n_neurons: 51 -> 66
    [INFO][configspace.py:175] --- solver: 'lbfgs' -> 'adam'
    [INFO][abstract_intensifier.py:588] Added config 7b908a and rejected config 8ea495 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- batch_size: 214 -> 81
    [INFO][configspace.py:175] --- learning_rate_init: 0.005599223654063347 -> 0.0025790209162800982
    [INFO][configspace.py:175] --- n_layer: 4 -> 5
    [INFO][configspace.py:175] --- n_neurons: 66 -> 67
    [INFO][smbo.py:306] Configuration budget is exhausted:
    [INFO][smbo.py:307] --- Remaining wallclock time: -0.6495020389556885
    [INFO][smbo.py:308] --- Remaining cpu time: inf
    [INFO][smbo.py:309] --- Remaining trials: 395
    [INFO][abstract_intensifier.py:588] Added config d6bd7a and rejected config 7b908a as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- batch_size: 81 -> 122
    [INFO][configspace.py:175] --- learning_rate_init: 0.0025790209162800982 -> 0.0029070693552790757
    [INFO][configspace.py:175] --- n_neurons: 67 -> 112
    Default cost (SuccessiveHalving): 0.36672856700711853
    Incumbent cost (SuccessiveHalving): 0.025601980810894376
    [INFO][abstract_initial_design.py:69] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.
    [INFO][abstract_facade.py:184] Workers are reduced to 8.
    /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
    Perhaps you already have a cluster running?
    Hosting the HTTP server on port 35651 instead
      warnings.warn(
    [INFO][scheduler.py:1617] State start
    [INFO][scheduler.py:3863]   Scheduler at:     tcp://127.0.0.1:44083
    [INFO][scheduler.py:3870]   dashboard at:  http://127.0.0.1:35651/status
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:44637'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:45915'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:39183'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:33575'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:43377'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:32953'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:41245'
    [INFO][nanny.py:367]         Start Nanny at: 'tcp://127.0.0.1:45241'
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:34647', name: 4, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:34647
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34040
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:39823', name: 6, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:39823
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34056
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:41811', name: 1, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:41811
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34026
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:44263', name: 0, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:44263
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34054
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:45093', name: 3, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:45093
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34076
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:33331', name: 5, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:33331
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34086
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:43519', name: 7, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:43519
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34092
    [INFO][scheduler.py:4220] Register worker <WorkerState 'tcp://127.0.0.1:45591', name: 2, status: init, memory: 0, processing: 0>
    [INFO][scheduler.py:5477] Starting worker compute stream, tcp://127.0.0.1:45591
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34066
    [INFO][scheduler.py:5235] Receive client connection: Client-d1214a2c-d863-11ed-8796-c5ac5de63e55
    [INFO][core.py:877] Starting established connection to tcp://127.0.0.1:34106
    [INFO][abstract_initial_design.py:134] Using 5 initial design configurations and 0 additional configurations.
    [INFO][successive_halving.py:163] Successive Halving uses budget type BUDGETS with eta 3, min budget 1, and max budget 25.
    [INFO][successive_halving.py:307] Number of configs in stage:
    [INFO][successive_halving.py:309] --- Bracket 0: [9, 3, 1]
    [INFO][successive_halving.py:309] --- Bracket 1: [3, 1]
    [INFO][successive_halving.py:309] --- Bracket 2: [1]
    [INFO][successive_halving.py:311] Budgets in stage:
    [INFO][successive_halving.py:313] --- Bracket 0: [2.7777777777777777, 8.333333333333332, 25.0]
    [INFO][successive_halving.py:313] --- Bracket 1: [8.333333333333332, 25.0]
    [INFO][successive_halving.py:313] --- Bracket 2: [25.0]
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][smbo.py:298] Finished 0 trials.
    [INFO][abstract_intensifier.py:513] Added config d3f994 as new incumbent because there are no incumbents yet.
    [INFO][abstract_intensifier.py:588] Added config 06bb76 and rejected config d3f994 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- activation: 'tanh' -> 'relu'
    [INFO][configspace.py:175] --- n_layer: 3 -> 2
    [INFO][configspace.py:175] --- n_neurons: 51 -> 48
    [INFO][abstract_intensifier.py:588] Added config 8ea495 and rejected config 06bb76 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- activation: 'relu' -> 'tanh'
    [INFO][configspace.py:175] --- batch_size: None -> 214
    [INFO][configspace.py:175] --- learning_rate_init: None -> 0.005599223654063347
    [INFO][configspace.py:175] --- n_layer: 2 -> 4
    [INFO][configspace.py:175] --- n_neurons: 48 -> 66
    [INFO][configspace.py:175] --- solver: 'lbfgs' -> 'adam'
    [INFO][smbo.py:298] Finished 50 trials.
    [INFO][smbo.py:298] Finished 50 trials.
    [INFO][abstract_intensifier.py:588] Added config 67740e and rejected config 8ea495 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- activation: 'tanh' -> 'logistic'
    [INFO][configspace.py:175] --- batch_size: 214 -> 252
    [INFO][configspace.py:175] --- learning_rate: None -> 'adaptive'
    [INFO][configspace.py:175] --- learning_rate_init: 0.005599223654063347 -> 0.07986885470785313
    [INFO][configspace.py:175] --- n_layer: 4 -> 1
    [INFO][configspace.py:175] --- n_neurons: 66 -> 209
    [INFO][configspace.py:175] --- solver: 'adam' -> 'sgd'
    [INFO][smbo.py:306] Configuration budget is exhausted:
    [INFO][smbo.py:307] --- Remaining wallclock time: -2.9127893447875977
    [INFO][smbo.py:308] --- Remaining cpu time: inf
    [INFO][smbo.py:309] --- Remaining trials: 415
    Default cost (Hyperband): 0.36672856700711853
    Incumbent cost (Hyperband): 0.02170380687093787






|

.. code-block:: default


    import warnings

    import matplotlib.pyplot as plt
    import numpy as np
    from ConfigSpace import (
        Categorical,
        Configuration,
        ConfigurationSpace,
        EqualsCondition,
        Float,
        InCondition,
        Integer,
    )
    from sklearn.datasets import load_digits
    from sklearn.model_selection import StratifiedKFold, cross_val_score
    from sklearn.neural_network import MLPClassifier

    from smac import MultiFidelityFacade as MFFacade
    from smac import Scenario
    from smac.facade import AbstractFacade
    from smac.intensifier.hyperband import Hyperband
    from smac.intensifier.successive_halving import SuccessiveHalving

    __copyright__ = "Copyright 2021, AutoML.org Freiburg-Hannover"
    __license__ = "3-clause BSD"


    dataset = load_digits()


    class MLP:
        @property
        def configspace(self) -> ConfigurationSpace:
            # Build Configuration Space which defines all parameters and their ranges.
            # To illustrate different parameter types, we use continuous, integer and categorical parameters.
            cs = ConfigurationSpace()

            n_layer = Integer("n_layer", (1, 5), default=1)
            n_neurons = Integer("n_neurons", (8, 256), log=True, default=10)
            activation = Categorical("activation", ["logistic", "tanh", "relu"], default="tanh")
            solver = Categorical("solver", ["lbfgs", "sgd", "adam"], default="adam")
            batch_size = Integer("batch_size", (30, 300), default=200)
            learning_rate = Categorical("learning_rate", ["constant", "invscaling", "adaptive"], default="constant")
            learning_rate_init = Float("learning_rate_init", (0.0001, 1.0), default=0.001, log=True)

            # Add all hyperparameters at once:
            cs.add_hyperparameters([n_layer, n_neurons, activation, solver, batch_size, learning_rate, learning_rate_init])

            # Adding conditions to restrict the hyperparameter space...
            # ... since learning rate is only used when solver is 'sgd'.
            use_lr = EqualsCondition(child=learning_rate, parent=solver, value="sgd")
            # ... since learning rate initialization will only be accounted for when using 'sgd' or 'adam'.
            use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=["sgd", "adam"])
            # ... since batch size will not be considered when optimizer is 'lbfgs'.
            use_batch_size = InCondition(child=batch_size, parent=solver, values=["sgd", "adam"])

            # We can also add multiple conditions on hyperparameters at once:
            cs.add_conditions([use_lr, use_batch_size, use_lr_init])

            return cs

        def train(self, config: Configuration, seed: int = 0, budget: int = 25) -> float:
            # For deactivated parameters (by virtue of the conditions),
            # the configuration stores None-values.
            # This is not accepted by the MLP, so we replace them with placeholder values.
            lr = config["learning_rate"] if config["learning_rate"] else "constant"
            lr_init = config["learning_rate_init"] if config["learning_rate_init"] else 0.001
            batch_size = config["batch_size"] if config["batch_size"] else 200

            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")

                classifier = MLPClassifier(
                    hidden_layer_sizes=[config["n_neurons"]] * config["n_layer"],
                    solver=config["solver"],
                    batch_size=batch_size,
                    activation=config["activation"],
                    learning_rate=lr,
                    learning_rate_init=lr_init,
                    max_iter=int(np.ceil(budget)),
                    random_state=seed,
                )

                # Returns the 5-fold cross validation accuracy
                cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent
                score = cross_val_score(classifier, dataset.data, dataset.target, cv=cv, error_score="raise")

            return 1 - np.mean(score)


    def plot_trajectory(facades: list[AbstractFacade]) -> None:
        """Plots the trajectory (incumbents) of the optimization process."""
        plt.figure()
        plt.title("Trajectory")
        plt.xlabel("Wallclock time [s]")
        plt.ylabel(facades[0].scenario.objectives)
        plt.ylim(0, 0.4)

        for facade in facades:
            X, Y = [], []
            for item in facade.intensifier.trajectory:
                # Single-objective optimization
                assert len(item.config_ids) == 1
                assert len(item.costs) == 1

                y = item.costs[0]
                x = item.walltime

                X.append(x)
                Y.append(y)

            plt.plot(X, Y, label=facade.intensifier.__class__.__name__)
            plt.scatter(X, Y, marker="x")

        plt.legend()
        plt.show()


    if __name__ == "__main__":
        mlp = MLP()

        facades: list[AbstractFacade] = []
        for intensifier_object in [SuccessiveHalving, Hyperband]:
            # Define our environment variables
            scenario = Scenario(
                mlp.configspace,
                walltime_limit=60,  # After 60 seconds, we stop the hyperparameter optimization
                n_trials=500,  # Evaluate max 500 different trials
                min_budget=1,  # Train the MLP using a hyperparameter configuration for at least 5 epochs
                max_budget=25,  # Train the MLP using a hyperparameter configuration for at most 25 epochs
                n_workers=8,
            )

            # We want to run five random configurations before starting the optimization.
            initial_design = MFFacade.get_initial_design(scenario, n_configs=5)

            # Create our intensifier
            intensifier = intensifier_object(scenario, incumbent_selection="highest_budget")

            # Create our SMAC object and pass the scenario and the train method
            smac = MFFacade(
                scenario,
                mlp.train,
                initial_design=initial_design,
                intensifier=intensifier,
                overwrite=True,
            )

            # Let's optimize
            incumbent = smac.optimize()

            # Get cost of default configuration
            default_cost = smac.validate(mlp.configspace.get_default_configuration())
            print(f"Default cost ({intensifier.__class__.__name__}): {default_cost}")

            # Let's calculate the cost of the incumbent
            incumbent_cost = smac.validate(incumbent)
            print(f"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}")

            facades.append(smac)

        # Let's plot it
        plot_trajectory(facades)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  37.628 seconds)


.. _sphx_glr_download_examples_2_multi_fidelity_1_mlp_epochs.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 1_mlp_epochs.py <1_mlp_epochs.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 1_mlp_epochs.ipynb <1_mlp_epochs.ipynb>`
