{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Custom Callback\n\nUsing callbacks is the easieast way to integrate custom code inside the Bayesian optimization loop.\nIn this example, we disable SMAC's default logging option and use the custom callback to log the evaluated trials.\nFurthermore, we print some stages of the optimization process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nfrom ConfigSpace import Configuration, ConfigurationSpace, Float\n\nimport smac\nfrom smac import Callback\nfrom smac import HyperparameterOptimizationFacade as HPOFacade\nfrom smac import Scenario\nfrom smac.runhistory import TrialInfo, TrialValue\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\nclass Rosenbrock2D:\n    @property\n    def configspace(self) -> ConfigurationSpace:\n        cs = ConfigurationSpace(seed=0)\n        x0 = Float(\"x0\", (-5, 10), default=-3)\n        x1 = Float(\"x1\", (-5, 10), default=-4)\n        cs.add_hyperparameters([x0, x1])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -> float:\n        x1 = config[\"x0\"]\n        x2 = config[\"x1\"]\n\n        cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0\n        return cost\n\n\nclass CustomCallback(Callback):\n    def __init__(self) -> None:\n        self.trials_counter = 0\n\n    def on_start(self, smbo: smac.main.smbo.SMBO) -> None:\n        print(\"Let's start!\")\n        print(\"\")\n\n    def on_tell_end(self, smbo: smac.main.smbo.SMBO, info: TrialInfo, value: TrialValue) -> bool | None:\n        self.trials_counter += 1\n        if self.trials_counter % 10 == 0:\n            print(f\"Evaluated {self.trials_counter} trials so far.\")\n\n            incumbent = smbo.intensifier.get_incumbent()\n            assert incumbent is not None\n            print(f\"Current incumbent: {incumbent.get_dictionary()}\")\n            print(f\"Current incumbent value: {smbo.runhistory.get_cost(incumbent)}\")\n            print(\"\")\n\n        if self.trials_counter == 50:\n            print(f\"We just triggered to stop the optimization after {smbo.runhistory.finished} finished trials.\")\n            return False\n\n        return None\n\n\nif __name__ == \"__main__\":\n    model = Rosenbrock2D()\n\n    # Scenario object specifying the optimization \"environment\"\n    scenario = Scenario(model.configspace, n_trials=200)\n\n    # Now we use SMAC to find the best hyperparameters\n    HPOFacade(\n        scenario,\n        model.train,\n        overwrite=True,\n        callbacks=[CustomCallback()],\n        logging_level=999999,\n    ).optimize()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}