{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# User Priors over the Optimum\n\nExample for optimizing a Multi-Layer Perceptron (MLP) setting priors over the optimum on the\nhyperparameters. These priors are derived from user knowledge (from previous runs on similar\ntasks, common knowledge or intuition gained from manual tuning). To create the priors, we make\nuse of the Normal and Beta Hyperparameters, as well as the \"weights\" property of the\n``CategoricalHyperparameter``. This can be integrated into the optimiztion for any SMAC facade,\nbut we stick with the hyperparameter optimization facade here. To incorporate user priors into the\noptimization, you have to change the acquisition function to ``PriorAcquisitionFunction``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport numpy as np\nfrom ConfigSpace import (\n    BetaIntegerHyperparameter,\n    CategoricalHyperparameter,\n    Configuration,\n    ConfigurationSpace,\n    NormalFloatHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom smac.acquisition.function import PriorAcquisitionFunction\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self) -> ConfigurationSpace:\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types,\n        # we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        # We do not have an educated belief on the number of layers beforehand\n        # As such, the prior on the HP is uniform\n        n_layer = UniformIntegerHyperparameter(\n            \"n_layer\",\n            lower=1,\n            upper=5,\n        )\n\n        # We believe the optimal network is likely going to be relatively wide,\n        # And place a Beta Prior skewed towards wider networks in log space\n        n_neurons = BetaIntegerHyperparameter(\n            \"n_neurons\",\n            lower=8,\n            upper=256,\n            alpha=4,\n            beta=2,\n            log=True,\n        )\n\n        # We believe that ReLU is likely going to be the optimal activation function about\n        # 60% of the time, and thus place weight on that accordingly\n        activation = CategoricalHyperparameter(\n            \"activation\",\n            [\"logistic\", \"tanh\", \"relu\"],\n            weights=[1, 1, 3],\n            default_value=\"relu\",\n        )\n\n        # Moreover, we believe ADAM is the most likely optimizer\n        optimizer = CategoricalHyperparameter(\n            \"optimizer\",\n            [\"sgd\", \"adam\"],\n            weights=[1, 2],\n            default_value=\"adam\",\n        )\n\n        # We do not have an educated opinion on the batch size, and thus leave it as-is\n        batch_size = UniformIntegerHyperparameter(\n            \"batch_size\",\n            16,\n            512,\n            default_value=128,\n        )\n\n        # We place a log-normal prior on the learning rate, so that it is centered on 10^-3,\n        # with one unit of standard deviation per multiple of 10 (in log space)\n        learning_rate_init = NormalFloatHyperparameter(\n            \"learning_rate_init\",\n            lower=1e-5,\n            upper=1.0,\n            mu=np.log(1e-3),\n            sigma=np.log(10),\n            log=True,\n        )\n\n        # Add all hyperparameters at once:\n        cs.add_hyperparameters([n_layer, n_neurons, activation, optimizer, batch_size, learning_rate_init])\n\n        return cs\n\n    def train(self, config: Configuration, seed: int = 0) -> float:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"optimizer\"],\n                batch_size=config[\"batch_size\"],\n                activation=config[\"activation\"],\n                learning_rate_init=config[\"learning_rate_init\"],\n                random_state=seed,\n                max_iter=5,\n            )\n\n            # Returns the 5-fold cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n        return 1 - np.mean(score)\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n    default_config = mlp.configspace.get_default_configuration()\n\n    # Define our environment variables\n    scenario = Scenario(mlp.configspace, n_trials=40)\n\n    # We also want to include our default configuration in the initial design\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(\n        scenario,\n        additional_configs=[default_config],\n    )\n\n    # We define the prior acquisition function, which conduct the optimization using priors over the optimum\n    acquisition_function = PriorAcquisitionFunction(\n        acquisition_function=HyperparameterOptimizationFacade.get_acquisition_function(scenario),\n        decay_beta=scenario.n_trials / 10,  # Proven solid value\n    )\n\n    # We only want one config call (use only one seed in this example)\n    intensifier = HyperparameterOptimizationFacade.get_intensifier(\n        scenario,\n        max_config_calls=1,\n    )\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        mlp.train,\n        initial_design=initial_design,\n        acquisition_function=acquisition_function,\n        intensifier=intensifier,\n        overwrite=True,\n    )\n\n    incumbent = smac.optimize()\n\n    # Get cost of default configuration\n    default_cost = smac.validate(default_config)\n    print(f\"Default cost: {default_cost}\")\n\n    # Let's calculate the cost of the incumbent\n    incumbent_cost = smac.validate(incumbent)\n    print(f\"Default cost: {incumbent_cost}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}