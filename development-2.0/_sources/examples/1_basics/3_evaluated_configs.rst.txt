
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/1_basics/3_evaluated_configs.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_1_basics_3_evaluated_configs.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_1_basics_3_evaluated_configs.py:


Start with Pre-Evaluated Configurations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This example shows how to incorporate evaluated configurations into SMAC.

.. GENERATED FROM PYTHON SOURCE LINES 7-111




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Default value: 0.03
    [INFO][initial_design.py:91] Ignoring `configs` and `n_configs` since `n_configs_per_hyperparameter` is given.
    [INFO][base_smbo.py:151] Initial design is skipped since 10 entries are found in the runhistory.
    [INFO][base_smbo.py:158] Added 10 configs from the runhistory as initial design and determined the incumbent.
    [INFO][intensification.py:603] Updated estimated cost of incumbent on 2 runs: 0.02
    [INFO][abstract_intensifier.py:345] Challenger (0.0133) is better than incumbent (0.02) on 2 runs.
    [INFO][abstract_intensifier.py:365] Changes in incumbent:
    [INFO][abstract_intensifier.py:368] --- C: 4.135997393839888 -> 0.2871109629679784
    [INFO][abstract_intensifier.py:368] --- coef0: None -> 7.7016631182698605
    [INFO][abstract_intensifier.py:368] --- degree: None -> 2
    [INFO][abstract_intensifier.py:368] --- gamma_value: 0.16521255145781252 -> 0.13397758631857307
    [INFO][abstract_intensifier.py:368] --- kernel: 'rbf' -> 'poly'
    [INFO][abstract_intensifier.py:368] --- shrinking: True -> False
    [INFO][facade.py:330] Final Incumbent: {'C': 0.2871109629679784, 'kernel': 'poly', 'shrinking': False, 'coef0': 7.7016631182698605, 'degree': 2, 'gamma': 'value', 'gamma_value': 0.13397758631857307}
    [INFO][facade.py:331] Estimated cost: 0.013333333333333308
    Incumbent value: 0.01






|

.. code-block:: default


    import copy
    import time
    import numpy as np
    from ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float, Integer
    from ConfigSpace.conditions import InCondition
    from sklearn import datasets, svm
    from sklearn.model_selection import cross_val_score

    from smac import HyperparameterFacade, Scenario
    from smac.runhistory import RunHistory, StatusType

    __copyright__ = "Copyright 2021, AutoML.org Freiburg-Hannover"
    __license__ = "3-clause BSD"


    # We load the iris-dataset (a widely used benchmark)
    iris = datasets.load_iris()


    class SVM:
        @property
        def configspace(self) -> ConfigurationSpace:
            # Build Configuration Space which defines all parameters and their ranges
            cs = ConfigurationSpace(seed=0)

            # First we create our hyperparameters
            kernel = Categorical("kernel", ["linear", "poly", "rbf", "sigmoid"], default="poly")
            C = Float("C", (0.001, 1000.0), default=1.0, log=True)
            shrinking = Categorical("shrinking", [True, False], default=True)
            degree = Integer("degree", (1, 5), default=3)
            coef = Float("coef0", (0.0, 10.0), default=0.0)
            gamma = Categorical("gamma", ["auto", "value"], default="auto")
            gamma_value = Float("gamma_value", (0.0001, 8.0), default=1.0, log=True)

            # Then we create dependencies
            use_degree = InCondition(child=degree, parent=kernel, values=["poly"])
            use_coef = InCondition(child=coef, parent=kernel, values=["poly", "sigmoid"])
            use_gamma = InCondition(child=gamma, parent=kernel, values=["rbf", "poly", "sigmoid"])
            use_gamma_value = InCondition(child=gamma_value, parent=gamma, values=["value"])

            # Add hyperparameters and conditions to our configspace
            cs.add_hyperparameters([kernel, C, shrinking, degree, coef, gamma, gamma_value])
            cs.add_conditions([use_degree, use_coef, use_gamma, use_gamma_value])

            return cs

        def train(self, config: Configuration, seed: int = 0) -> float:
            """Creates a SVM based on a configuration and evaluates it on the
            iris-dataset using cross-validation."""
            config_dict = config.get_dictionary()
            if "gamma" in config:
                config_dict["gamma"] = config_dict["gamma_value"] if config_dict["gamma"] == "value" else "auto"
                config_dict.pop("gamma_value", None)

            classifier = svm.SVC(**config_dict, random_state=seed)
            scores = cross_val_score(classifier, iris.data, iris.target, cv=5)
            cost = 1 - np.mean(scores)

            return cost


    if __name__ == "__main__":
        classifier = SVM()
        configspace = classifier.configspace
        default_config = configspace.get_default_configuration()
        seed = 99

        # Example call of the target algorithm
        default_value = classifier.train(default_config)
        print(f"Default value: {round(default_value, 2)}")

        # Now we create an empty runhistory to add random configurations with our values to start with
        runhistory = RunHistory()
        for config in configspace.sample_configuration(10):
            start_time = time.time()
            cost = classifier.train(copy.deepcopy(config), seed)
            train_time = time.time() - start_time
            runhistory.add(config, cost=cost, time=train_time, seed=seed, status=StatusType.SUCCESS)

        # Next, we create an object, holding general information about the run
        scenario = Scenario(
            configspace,
            # We want to run max 100 trials (combination of config and seed) on top of what's already in the runhistory
            n_trials=100,
        )

        intensifier = HyperparameterFacade.get_intensifier(
            scenario,
            max_config_calls=2,  # We only want to use two seeds per config
        )

        # We use the hyperparameter facade to run SMAC
        smac = HyperparameterFacade(
            scenario,
            classifier.train,
            intensifier=intensifier,
            runhistory=runhistory,
            overwrite=True,  # Disables to continue the run
        )
        incumbent = smac.optimize()

        incumbent_value = classifier.train(incumbent)
        print(f"Incumbent value: {round(incumbent_value, 2)}")


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  2.668 seconds)


.. _sphx_glr_download_examples_1_basics_3_evaluated_configs.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 3_evaluated_configs.py <3_evaluated_configs.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 3_evaluated_configs.ipynb <3_evaluated_configs.ipynb>`
